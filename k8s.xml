<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Kubernetes Planet</title>
    <link>https://whitekid.github.io/goplanet/</link>
    <description>Kubernetes Planet</description>
    <managingEditor>whitekid@gmail.com (Charlie.Choe)</managingEditor>
    <pubDate>Mon, 19 Feb 2024 14:25:35 +0900</pubDate>
    <lastBuildDate>Mon, 19 Feb 2024 14:25:35 +0900</lastBuildDate>
    <item>
      <title>Improving the multi-team Kubernetes ingress experience with Heptio Contour 0.6</title>
      <link>https://blog.heptio.com/improving-the-multi-team-kubernetes-ingress-experience-with-heptio-contour-0-6-55ae0c0cadef?source=rss----7d24bed16a19---4</link>
      <description></description>
      <content:encoded><![CDATA[<p>Kubernetes has a variety of primitives that make it a great platform for running workloads submitted by multiple teams. Features like Role Based Access Control (RBAC) and Namespaces make it possible to divide clusters across multiple teams in a safe way. There are some challenges however, and one of the most important ones our enterprise customers have encountered lies in the Ingress API. In this post, we will explore how a bad Ingress resource can break your ingress layer, and walk through our novel approach to multi-team ingress using <a href="https://blog.heptio.com/introducing-heptio-contour-0-6-ecaa5ee6a67d">Heptio Contour’s new IngressRoute resource.</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1022/1*2a-WtoffgZg3W59qhqrOmg.png" /></figure><p><strong>Multi-team Ingress on Kubernetes</strong></p><p>Most organizations typically have more than one team interacting with a given cluster. Cluster operators assign one or more namespaces to each team and use RBAC to ensure that no team can mess with another team’s resources.</p><p>Even though Ingress is a namespaced resource that can be locked down with RBAC, it poses a challenge in multi-team clusters because it controls cluster-level configuration: the hosts and paths on which to serve application traffic.</p><p>Let us imagine a scenario where the marketing team owns <em>www.example.com/blog</em>. They are responsible for the organization’s blog and they have configured an Ingress resource that looks like this:</p><pre>apiVersion: extensions/v1beta1<br>kind: Ingress<br>metadata:<br>  name: blog<br>  namespace: marketing<br>spec:<br>  rules:<br>  - host: www.example.com<br>    http:<br>      paths:<br>      - path: /blog<br>        backend:<br>          serviceName: blog<br>          servicePort: 80</pre><p>Now, the engineering team is looking to run their own engineering-focused blog, and they mistakenly apply the following Ingress resource into the engineering namespace:</p><pre>apiVersion: extensions/v1beta1<br>kind: Ingress<br>metadata:<br>  name: blog<br>  namespace: engineering<br>spec:<br>  rules:<br>  - host: www.example.com<br>    http:<br>      paths:<br>      - path: /blog<br>        backend:<br>          serviceName: engineering-blog<br>          servicePort: 80</pre><p>We now have two conflicting Ingress configurations that point <em>www.example.com/blog</em> to different services. The Ingress API does not define how to handle this conflict and the behavior of Ingress Controllers frequently differs — this results in a negative user experience affecting multiple parties. The engineering team is completely unaware that they have taken down the company blog, while the avid blog readers are unable to access their favorite blog.</p><p>As you can see in this example, the Ingress resource can become the Achilles’ heel of a multi-team cluster. We have heard from multiple customers that have been bitten by this in production, and thus we decided to address this issue in Contour.</p><p><strong>IngressRoute delegation to the rescue</strong></p><p>One of the most exciting features introduced in the latest version of Heptio Contour is the IngressRoute Custom Resource Definition (CRD). Among the many improvements available in this new custom resource is delegation support, which allow you to delegate the configuration of a specific host or path to another IngressRoute.</p><p>The crux of the problem with the Ingress resource in a multi-team cluster is that operators do not have a way to prevent teams from claiming hosts and paths at will. The ability to create root IngressRoutes in a specific namespace, as well as the ability to do cross-namespace delegation is our answer to this problem.</p><p>Using the delegation feature of the IngressRoute, cluster operators get full control of the roots of their ingress layer by limiting which namespaces are <a href="https://github.com/heptio/contour/blob/master/docs/ingressroute.md#restricted-root-namespaces">authorized to create root IngressRoutes</a>. This eliminates the possibility for two teams to create configurations that collide. The IngressRoute roots specify the top level domains and TLS configuration, while delegating the configuration of specific subdomains or paths to other IngressRoutes in other namespaces. In this way, each team gets the ability to use and configure the slice of the ingress space that has been delegated to their team’s namespace.</p><p>Let us revisit the problematic scenario we outlined above. The cluster operator creates a “roots” namespace, and configures Contour to only accept root IngressRoutes from this namespace. Then, the cluster operator creates a root IngressRoute for <em>www.example.com</em> and delegates the /blog path to the marketing team:</p><pre>apiVersion: contour.heptio.com/v1beta1<br>kind: IngressRoute<br>metadata:<br>  name: example-com-root<br>  namespace: roots<br>spec:<br>  virtualhost:<br>    fqdn: www.example.com<br>  routes:<br>  - match: /blog<br>    delegate:<br>      name: blog<br>      namespace: marketing</pre><p>The marketing team creates an IngressRoute that sets up the company blog. Note that the <em>virtualhost </em>is missing, as this is not a root IngressRoute.</p><pre>apiVersion: contour.heptio.com/v1beta1<br>kind: IngressRoute<br>metadata:<br>  name: blog<br>  namespace: marketing<br>spec:<br>  routes:<br>  - match: /blog<br>    services:<br>    - name: blog<br>      port: 80</pre><p>As you might imagine, if the engineering team were to create a conflicting IngressRoute, the company’s blog would remain accessible as there is no delegation path that points to the engineering team IngressRoute. Instead of producing an outage, Contour ignores the <em>orphaned route</em> and sets its status field accordingly:</p><pre>apiVersion: contour.heptio.com/v1beta1<br>kind: IngressRoute<br>metadata:<br>  name: blog<br>  namespace: engineering<br>spec:<br>  routes:<br>  - match: /blog<br>    services:<br>    - name: engineering-blog<br>      port: 80<br>status:<br>  currentStatus: orphaned<br>  description: this IngressRoute is not part of a delegation chain from a root IngressRoute</pre><p><strong>What’s next?</strong></p><p>We have explored the new IngressRoute and more specifically, the delegation model that enables you to run multi-team Kubernetes clusters in a safe way; this is one of the exciting features available in the latest version of <a href="https://github.com/heptio/contour/releases/tag/v0.6.0">Heptio Contour</a>. But, there’s more.</p><p>In future posts, we will explore other patterns enabled by the IngressRoute, including blue/green deployments, canary deployments and load balancing strategies. If you have any questions, or are interested in learning more, feel to reach us via the #contour channel on the <a href="http://slack.k8s.io/">Kubernetes community Slack</a>, or follow us on <a href="https://twitter.com/heptio">Twitter</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=55ae0c0cadef" width="1" height="1" alt=""><hr><p><a href="https://blog.heptio.com/improving-the-multi-team-kubernetes-ingress-experience-with-heptio-contour-0-6-55ae0c0cadef">Improving the multi-team Kubernetes ingress experience with Heptio Contour 0.6</a> was originally published in <a href="https://blog.heptio.com">Heptio</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
      <guid>https://blog.heptio.com/improving-the-multi-team-kubernetes-ingress-experience-with-heptio-contour-0-6-55ae0c0cadef?source=rss----7d24bed16a19---4</guid>
      <pubDate>Wed, 10 Oct 2018 15:46:03 +0000</pubDate>
    </item>
    <item>
      <title>Flexible software deployment patterns with IngressRoute</title>
      <link>https://blog.heptio.com/flexible-software-deployment-patterns-with-ingressroute-a49a43253992?source=rss----7d24bed16a19---4</link>
      <description></description>
      <content:encoded><![CDATA[<p>This is the third post in a series highlighting some of the exciting new features released in <a href="https://blog.heptio.com/introducing-heptio-contour-0-6-ecaa5ee6a67d">Heptio Contour version 0.6</a>. If you missed out on those, start with <a href="https://blog.heptio.com/introducing-heptio-contour-0-6-ecaa5ee6a67d">Introducing Heptio Contour 0.6</a> and <a href="https://blog.heptio.com/improving-the-multi-team-kubernetes-ingress-experience-with-heptio-contour-0-6-55ae0c0cadef">Improving the multi-team Kubernetes ingress experience with Heptio Contour 0.6</a>.</p><p>One of the improvements that we added to IngressRoute is the ability to route traffic to multiple Services for a given path as well as apply weights to those upstream Services. This seemingly small addition allows users to implement some simple, yet very powerful deployment patterns.</p><h3>Canary Deployments</h3><p>One way to roll out a new version of an application is to utilize a canary deployment. In this model, first deploy the change to a small subset of users to gather information on how the new version is responding. Since only a small set of traffic is targeted, the impact overall will not be as apparent in the event of a failure of the new version. The amount of traffic sent to the canary version is determined by the weight configured, a higher proportion of weight means more traffic will be sent.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/982/1*Tfa1v2uJNmxOi4aMeWP57g.png" /></figure><p>Without IngressRoute, the only way to implement this would be to have a Service select pods from two different deployments, however, traffic would be limited by the number of replicas of each deployment and it would be difficult to manage. Additionally, the standard Kubernetes Ingress object does not allow for multiple Services per virtual host and does not support configurable weighting.</p><p>We took these requirements into account as we designed the IngressRoute specification and added the ability to define multiple Services per Route as well as configurable weighting. By manipulating weights across the Services, the entire rollout can be managed easily until the new version of the application is receiving 100% of the traffic.</p><p>Following is a diagram which visualizes how a canary deployment is rolled out:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/498/0*Gw8318G-sB661Sqg" /></figure><pre>apiVersion: contour.heptio.com/v1beta1<br>kind: IngressRoute<br>metadata: <br>  name: production-webapp<br>spec: <br>  virtualhost:<br>    fqdn: foo.com<br>  routes: <br>  - match: /<br>      services: <br>      - name: webapp-v1.0.0<br>        port: 80<br>        weight: 90<br>      - name: webapp-v1.1.0<br>        port: 80<br>        weight: 10</pre><p>In this example, 90% of the requests to <em>foo.com</em> are routed to the Service <em>webapp-v1.0.0</em> and 10% are routed to <em>webapp-v1.1.0</em>. It’s important to note that modifying the weights triggers an <em>immediate</em> shift of traffic pattern in Envoy (via Contour).</p><h3>Other Use-Cases</h3><p><a href="https://github.com/heptio/gimbal">Heptio Gimbal</a> is an open source initiative that builds on Heptio Contour with the goal of unifying and managing internet traffic on hybrid environments consisting of multiple Kubernetes clusters running on cloud providers and on traditional data centers.</p><p>Gimbal allows users to utilize multi-service IngressRoutes to route traffic across clusters. You can read more about Gimbal from our <a href="https://blog.heptio.com/introducing-heptio-gimbal-bridging-cloud-native-and-traditional-infrastructure-9d6224bece5a">launch blog post</a>.</p><h3>What’s next?</h3><p>In this post, we have explored how traffic can be routed to multiple weighted Services within a Kubernetes cluster utilizing IngressRoute. This is one of the many exciting features available in the latest version of <a href="https://github.com/heptio/contour/releases/tag/v0.6.0">Heptio Contour</a>.</p><p>In future posts, we will explore other patterns enabled by the IngressRoute, including blue/green deployments and load balancing strategies. If you have any questions or are interested in learning more, reach us via the #contour channel on the <a href="http://slack.k8s.io/">Kubernetes community Slack</a> or follow us on <a href="https://twitter.com/heptio">Twitter</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a49a43253992" width="1" height="1" alt=""><hr><p><a href="https://blog.heptio.com/flexible-software-deployment-patterns-with-ingressroute-a49a43253992">Flexible software deployment patterns with IngressRoute</a> was originally published in <a href="https://blog.heptio.com">Heptio</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
      <guid>https://blog.heptio.com/flexible-software-deployment-patterns-with-ingressroute-a49a43253992?source=rss----7d24bed16a19---4</guid>
      <pubDate>Tue, 16 Oct 2018 15:56:03 +0000</pubDate>
    </item>
    <item>
      <title>A Week in the Life of a Field Engineer</title>
      <link>https://blog.heptio.com/a-week-in-the-life-of-a-field-engineer-bd3a0e051be9?source=rss----7d24bed16a19---4</link>
      <description></description>
      <content:encoded><![CDATA[<h3>A week in the life of a Field Engineer</h3><p>It’s no secret that technology workers with Kubernetes skills are in high demand. Large companies like Box, Capital One, and Comcast are transforming how they build and deploy production services with Kubernetes. Most Heptio customers engage with our Field Engineering team to research new patterns and technology, learn best practices, solve problems, and optimize Kubernetes for their needs. While no two days or weeks are exactly alike, I’d like to offer a glimpse into what could be a typical week in the life of a Field Engineer working onsite with a customer.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*QzkShF8C4PJXfBwAZMNRjg.jpeg" /></figure><h3>Wake up!</h3><p>The alarm on your phone goes off. You rub the sleep out of your eyes and snooze the alarm and start checking your networks: email, Slack, and Twitter. The CNCF ecosystem moves quickly and you’re looking for updates: is a technology you’ve been following ready for production? What’s the latest on a pull request you submitted upstream? While you’re onsite, some other members of the Field Engineering team are also onsite at other customers all over the world. The team is making a huge impact on how customers leverage these technologies. You follow up with a London-based Field Engineer over breakfast as they close out their day with questions, answers, and discussion on customer solutions. Last thing before you head out: review the customer’s statement of work and think about what to tackle first. You put your phone down, enjoy your last cup of coffee, and head to the customer’s office.</p><h3>Monday: Showtime</h3><p>You check in at the front desk and get your guest security badge. You send a message to the customer via email or text to let them know you’ve arrived and get the tour. Where is the coffee? Where are the bathrooms? Where are we working? You get put into the conference room with six platform IT engineers and get to work. Over the next few days you’ll work with the customer to find out what problems they are trying to solve, where they are in their Kubernetes and container journey, and how you can help.</p><p>Every customer you engage with is committed to adopting open source Kubernetes. This is when you find out how far along they are in deployment and what kind of move the company is making. Is this an incremental move from cloud instances to containers, or a part of a gigantic digital transformation project? You’re going to get asked about optimized instance types for Kubernetes, CI/CD tools, logging and security. What’s more: you’ve seen this all before. You’re making real recommendations that are going to make this company successful. You’re documenting everything discussed to hand back to the customer at the end of the engagement for reference, and sending a copy to Heptio’s Customer Reliability Engineering team. After debugging a few things, <a href="https://github.com/heptio/sonobuoy">Heptio Sonobuoy</a> is run on the cluster and it’s all passing. What a day!</p><p>You check in with your networks throughout the day: email, Slack, and Twitter. You might confirm something you worked on with Staff Field Engineers like Craig Tracey or Scott Lowe. You have another coffee (or two).</p><h3>Tuesday: Thought-Leader-Driven-Development</h3><p>You get back to the customer’s conference room after lunch and the team lead throws you a curve ball: they saw a demo from a thought leader at a conference and want to put that piece of technology into production. You think you remember seeing that project on the <a href="https://landscape.cncf.io/">CNCF Landscape</a>… somewhere. Time to dig in and learn something new. You break off with a few engineers from the customer side to see if it’s even feasible (much less recommended). You’re looking at code, documentation, GitHub issues and blog posts, and interacting with other members of Field Engineering and Customer Reliability Engineering to get their perspectives. A product manager saw that same demo, but also remembers that it wasn’t deployed in a highly available configuration. You test and confirm: it CAN’T be deployed with HA, but there’s already an open issue where you can contribute a solution. You present your findings to the team lead and they decide to move forward after the change has been implemented.</p><p>After walking through the deployment steps and potential gotchas with the overall team, you note your concerns in the project documentation and file an internal issue with the local team to watch the project. If enough customers are interested in the open source project, it might make sense for Heptio to try to get more involved. In any case, your pull request is open.</p><h3>Thursday: Wrapping up</h3><p>You have a final meeting with the customer team and summarize the recommendations you made that morning, what was accomplished, and what the plan is for the following day. Once again you check in with Slack to see how your teammates are doing and give your own updates. You add to the project documentation and then head for the airport, where you’ve planned to catch dinner with a community pal who’s also on the road. As you board the plane, you get an email that your pull request was accepted to the upstream project. It’ll be in the next release. You make a mental note to touch on that with the customer first thing Monday morning, and finally zone out with some Netflix offline. What a week!</p><p>If this seems like the kind of work that interests you, Heptio is looking for great Field Engineers. You’ll help modernize the software industry and drive adoption of Kubernetes and Cloud Native technologies inside customer environments. <a href="https://jobs.lever.co/heptio?lever-via=WCNPZP94GZ">Apply today</a> and be a part of it!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bd3a0e051be9" width="1" height="1" alt=""><hr><p><a href="https://blog.heptio.com/a-week-in-the-life-of-a-field-engineer-bd3a0e051be9">A Week in the Life of a Field Engineer</a> was originally published in <a href="https://blog.heptio.com">Heptio</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
      <guid>https://blog.heptio.com/a-week-in-the-life-of-a-field-engineer-bd3a0e051be9?source=rss----7d24bed16a19---4</guid>
      <pubDate>Tue, 23 Oct 2018 12:36:02 +0000</pubDate>
    </item>
    <item>
      <title>Heptio Contour 0.7 Release Brings Improved Ingress Control and Request-Prefix Rewriting Support</title>
      <link>https://blog.heptio.com/heptio-contour-0-7-release-brings-improved-ingress-control-and-request-prefix-rewriting-support-bce325ba3c4b?source=rss----7d24bed16a19---4</link>
      <description></description>
      <content:encoded><![CDATA[<p>Heptio Contour is an open source Kubernetes ingress controller that uses Envoy, Lyft’s open source edge and service proxy, to provide a modern way to direct internet traffic into a cluster. Last Friday, we released <a href="https://github.com/heptio/contour/releases/tag/v0.7.0">Contour version 0.7</a>, which includes some helpful new features that you should know about if you’re evaluating options for incoming load balancing in Kubernetes.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6bPaJZMeBJ1kOx_FFaB1fg.png" /></figure><p>Contour 0.7 enables:</p><p><strong>Better traffic control within a cluster:</strong> With support for the <em>‘ingress.class’</em> annotation, you’ll now be able to specify where incoming traffic should go within a cluster. One key use case here is to be able to separate production traffic from staging and development; for example, if the <em>‘contour.heptio.com/ingress.class: production’</em> annotation is on an IngressRoute object, it will only be processed by Contour containers running with the flag <em>‘— ingress-class-name=production’</em>.</p><p><strong>Rewriting a request prefix: </strong>Need to route a legacy or enterprise application to a different path from your specified ingress route? You can now use Contour to rewrite a path prefix and ensure that incoming traffic goes to the right place without issue. <em>(See </em><a href="https://github.com/heptio/contour/blob/master/docs/ingressroute.md#prefix-rewrite-support"><em>Github</em></a><em> for more detail on this.)</em></p><p><strong>Cost savings through GZIP compression:</strong> Contour 0.7 features GZIP compression by default, so that you can see cost savings through reduced bandwidth, while speeding up load times for your customers.</p><p><strong>Envoy health checking and 1.7 compatibility:</strong> Envoy’s now-exposed <em>/healthz</em> endpoint can be used with Kubernetes readiness probes, and Contour is also now compatible with Envoy 1.7, making it easier for you to get Prometheus metrics for HTTP/HTTPS traffic.</p><p>To learn more about the benefits of Heptio Contour 0.7 and how it can help with Kubernetes ingress control, tune in to <a href="https://www.youtube.com/playlist?list=PLvmPtYZtoXOENHJiAQc6HmV2jmuexKfrJ">“TGI Kubernetes”</a> this Friday, November 2, at 1 PM PT — <a href="https://medium.com/u/db5bb4d1b8a1">Joe Beda</a> will be covering all things Contour and IngressRoute CRD!</p><p>As always, we’d love to hear what you think about Contour’s new features, as well as your ideas for new ones: join and contribute to the Contour community on <a href="https://kubernetes.slack.com/?redir=%2Fmessages%2Fcontour">Slack</a> and <a href="https://github.com/heptio/contour">Github</a>, or tweet us <a href="https://twitter.com/heptio">@heptio</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bce325ba3c4b" width="1" height="1" alt=""><hr><p><a href="https://blog.heptio.com/heptio-contour-0-7-release-brings-improved-ingress-control-and-request-prefix-rewriting-support-bce325ba3c4b">Heptio Contour 0.7 Release Brings Improved Ingress Control and Request-Prefix Rewriting Support</a> was originally published in <a href="https://blog.heptio.com">Heptio</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
      <guid>https://blog.heptio.com/heptio-contour-0-7-release-brings-improved-ingress-control-and-request-prefix-rewriting-support-bce325ba3c4b?source=rss----7d24bed16a19---4</guid>
      <pubDate>Tue, 30 Oct 2018 19:51:42 +0000</pubDate>
    </item>
    <item>
      <title>Heptio will be joining forces with VMware on a shared cloud native mission</title>
      <link>https://blog.heptio.com/heptio-will-be-joining-forces-with-vmware-on-a-shared-cloud-native-mission-b01225b1bc9e?source=rss----7d24bed16a19---4</link>
      <description></description>
      <content:encoded><![CDATA[<p>Today we are incredibly excited to announce that Heptio will be acquired by VMware. It is a watershed day for our company, and we hope for the industry as a whole. The inevitable question is … why have we decided to join forces (now)?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*3I1MxzHDWEuRioexvrKhlQ.png" /></figure><p>Life at Heptio has been pretty exceptional since we founded the company two years ago. In a short period, we have made strong contributions in the Kubernetes and cloud native ecosystem, assembled a remarkable team and onboarded some of the most prestigious enterprises as customers. We were incredibly well capitalized and supported by our investors. So what gives?</p><p><strong>Shared vision.</strong></p><p>Heptio’s mission is to build a platform that accelerates IT in a multi-cloud world. We are on the precipice of a major transformation—the de-coupling of applications from the environments where they are run. And we feel a responsibility to help organizations navigate this transformation to true cloud native architecture. To realize the greatest possible impact, Heptio would need access to an entirely different level of resources and execution capabilities than we have today.</p><p>Who is best positioned to lead this transformation? The company that led a parallel transformation—the software defined data center. VMware. They have experience, execution muscle, customer trust and full leadership commitment.</p><p>When we first started conversations with VMware, the alignment of our respective visions was uncanny. With virtualization, VMware helped enterprises change the way their infrastructure operates. VMware values our products and services—together we can apply these technologies to change the way business operates, and where they run their applications.</p><p><strong>Customer Value.</strong></p><p>We live in a really interesting time. Enterprise companies are dealing with waves of disruption in the software space, and increasingly fragmented and complicated hosting environments. Kubernetes has an important role to play as a ubiquitous, uniform framework—to be as available and invisible as a utility, like electricity. We believe that an enterprise should pick their infrastructure hosting environment based solely on pragmatic attributes: cost economics, data locality and sovereignty, and connectivity to the consumers and workloads they support.</p><p>The value for enterprises is not the electricity, nor the vehicle through which it is delivered; value is created when applications are plugged in. The missing piece is a control plane that shapes the experience in deploying and accessing cloud native technologies. It must address day 2 challenges, integrating technologies into a practical enterprise environment, and instituting policies and management capabilities. It is going to take a hard push from an engaged, enterprise-friendly company to make it real. We are convinced that VMware possesses the ability and commitment to create a platform that works everywhere and meets the unique needs of enterprises. Together we can change the game.</p><p><strong>Community Connection.</strong></p><p>From the start, Heptio has maintained a healthy relationship with the open source community. We’re tied into the Kubernetes steering committee and a number of SIGs, plus our team has shepherded five open source projects (Sonobuoy, Contour, Gimbal, Ark and ksonnet). We feel like the community trusts us. That trust continues to be well placed. The team at VMware have a parallel appreciation for the community; they fully understand the importance of being closely connected to foster more innovation. They have so much energy and resources already focused on this area; the time is right to join forces and accelerate the value delivered to the open source community.</p><p><strong>Culture First.</strong></p><p>I’ve left culture to the final topic for this post, but the fact that VMware puts its culture first is central to our decision to join their fold. We think a lot about the culture of a company not only as an expression of its values, but as a blueprint for how it creates value in the world. Even before we started conversations with VMware, we were aware of similarities in our culture and core values. We have some great people working at Heptio that ‘grew up’ at VMware—they enjoyed their work and had tremendous respect for their colleagues. This made us feel good about joining them, and instilled confidence that our teams would gel and we could focus our energy on our shared mission.</p><p><strong>In Closing.</strong></p><p>At Heptio, we’ve often (internally) lamented that we’re not great at celebrating our achievements. But today, we can’t avoid a proper celebration. I’m so proud of our team and what they’ve built in such a compressed time frame, and so grateful to our community for their incredible support. I’m immensely excited to join forces with an organization that shares our mission and that has proved they know how to deliver transformative technology. We’re fired up to have an even bigger impact.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b01225b1bc9e" width="1" height="1" alt=""><hr><p><a href="https://blog.heptio.com/heptio-will-be-joining-forces-with-vmware-on-a-shared-cloud-native-mission-b01225b1bc9e">Heptio will be joining forces with VMware on a shared cloud native mission</a> was originally published in <a href="https://blog.heptio.com">Heptio</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
      <guid>https://blog.heptio.com/heptio-will-be-joining-forces-with-vmware-on-a-shared-cloud-native-mission-b01225b1bc9e?source=rss----7d24bed16a19---4</guid>
      <pubDate>Tue, 06 Nov 2018 09:24:00 +0000</pubDate>
    </item>
    <item>
      <title>Setting up the Kubernetes AWS Cloud Provider</title>
      <link>https://blog.heptio.com/setting-up-the-kubernetes-aws-cloud-provider-6f0349b512bd?source=rss----7d24bed16a19---4</link>
      <description></description>
      <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*cXr7spiwJuwY9hZ9dRCaEQ.jpeg" /></figure><p>The AWS cloud provider for <a href="https://kubernetes.io">Kubernetes</a> enables a couple of key integration points for Kubernetes running on AWS; namely, dynamic provisioning of Elastic Block Store (EBS) volumes, and dynamic provisioning/configuration of Elastic Load Balancers (ELBs) for exposing Kubernetes Service objects. Unfortunately, the documentation surrounding how to set up the AWS cloud provider with Kubernetes is woefully inadequate. This article is an attempt to help address that shortcoming.</p><p>More details are provided below, but at a high-level here’s what you’ll need to make the AWS cloud provider in Kubernetes work:</p><ul><li>You’ll need the hostname of each node to match EC2’s private DNS entry for that node</li><li>You’ll need an IAM role and policy that EC2 instances can assume as an instance profile</li><li>You’ll need some Kubernetes-specific tags applied to the AWS resources used by the cluster</li><li>You’ll add some particular command-line flags to the Kubernetes API server, Kubernetes controller manager, and the Kubelet</li></ul><p>Let’s dig into these requirements in a bit more detail.</p><p><strong>Node Hostname</strong></p><p>It’s important that the name of the Node object in Kubernetes matches the private DNS entry for the instance in EC2. You can use hostnamectl or a configuration management tool (take your pick) to set the instance’s hostname to the FQDN that matches the EC2 private DNS entry. This typically looks something like ip-10–15–30–45.us-west-1.compute.internal, where 10–15–30–45 is the private IP address and us-west-1 is the region where the instance was launched.</p><p>If you’re unsure what it is, or if you’re looking for a programmatic way to retrieve the FQDN, just curl the AWS metadata server:</p><pre>curl <a href="http://169.254.169.254/latest/meta-data/local-hostname">http://169.254.169.254/latest/meta-data/local-hostname</a></pre><p>Make sure you set the hostname before attempting to bootstrap the Kubernetes cluster, or you’ll end up with nodes whose name in Kubernetes doesn’t match up, and you’ll see various “permission denied”/”unable to enumerate” errors in the logs. For what it’s worth, preliminary testing indicates that this step — setting the hostname to the FQDN — is necessary for Ubuntu but may not be needed for CentOS/RHEL.</p><p><strong>IAM Role and Policy</strong></p><p>Because the AWS cloud provider performs some tasks on behalf of the operator — like creating an ELB or an EBS volume — the instances need IAM permissions to perform these tasks. Thus, you need to have an IAM instance profile assigned to the instances that give them permissions.</p><p>The <em>exact</em> permissions that are needed are best documented in <a href="https://github.com/kubernetes/cloud-provider-aws">this GitHub repository for the future out-of-tree AWS cloud provider</a>. Separate permissions are needed for the control plane nodes versus the worker nodes; the control plane nodes need more permissions than the worker nodes. This means you’ll end up with two IAM instance profiles: one for the control plane nodes with a broader set of permissions, and one for the worker nodes with a more restrictive set of permissions.</p><p><strong>AWS Tags</strong></p><p>The AWS cloud provider needs a specific tag to be present on almost all the AWS resources that a Kubernetes cluster needs. The tag key is kubernetes.io/cluster/cluster-name where cluster-name is an arbitrary name for the cluster; the value of the tag is immaterial (this tag replaces an older KubernetesCluster tag you may see referenced). Note that Kubernetes itself will also use this tag on things that it creates, and it will use a value of “owned”. This value does <em>not</em> need to be used on resources that Kubernetes itself did not create. Most of the documentation I’ve seen indicates that the tag is needed on all instances and on exactly one security group (this is the security group that will be modified to allow ELBs to access the nodes, so the worker nodes should be a part of this security group). However, I’ve also found it necessary to make sure the kubernetes.io/cluster/cluster-name tag is present on subnets and route tables in order for the integration to work as expected.</p><p><strong>Kubernetes Configuration</strong></p><p>On the Kubernetes side of the house, you’ll need to make sure that the --cloud-provider=aws command-line flag is present for the API server, controller manager, and every Kubelet in the cluster.</p><p>If you’re using kubeadm to set up your cluster, you can have kubeadm add the flags to the API server and controller manager by using the apiServerExtraArgs and controllerManagerExtraArgs sections in a configuration file, like this:</p><pre>apiServerExtraArgs:<br>  cloud-provider: aws<br>controllerManagerExtraArgs:<br>  cloud-provider: aws</pre><p>Likewise, you can use the nodeRegistration section of a kubeadm configuration file to pass extra arguments to the Kubelet, like this:</p><pre>nodeRegistration:<br>  kubeletExtraArgs:<br>    cloud-provider: aws</pre><p>I’d probably also recommend setting the name of the Kubelet to the node’s private DNS entry in EC2 (this ensures it matches the hostname, as described earlier in this article). Thus, the full nodeRegistration section might look like this:</p><pre>nodeRegistration:<br>  name: ip-10–15–30–45.us-west-1.compute.internal<br>  kubeletExtraArgs:<br>    cloud-provider: aws</pre><p>You would need to substitute the correct fully-qualified domain name for each instance, of course.</p><p>Finally, for dynamic provisioning of Persistent Volumes you’ll need to create a default Storage Class (read about Storage Classes <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">here</a>). The AWS cloud provider has one, but it doesn’t get created automatically. Use this command to define the default Storage Class:</p><pre>kubectl apply -f <a href="https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/storage-class/aws/default.yaml">https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/storage-class/aws/default.yaml</a></pre><p>This will create a Storage Class named “gp2” that has the necessary annotation to make it the default Storage Class (see <a href="https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/">here</a>). Once this Storage Class is defined, dynamic provisioning of Persistent Volumes should work as expected.</p><p><strong>Troubleshooting</strong></p><p>Troubleshooting is notoriously difficult, as most errors seem to be “transparently swallowed” instead of exposed to the user/operator. Here are a few notes that may be helpful:</p><ul><li>You <em>_must_</em> have the --cloud-provider=aws flag added to the Kubelet <strong>before</strong> adding the node to the cluster. Key to the AWS integration is a particular field on the Node object — the .spec.providerID field — and that field will <em>only</em> get populated if the flag is present when the node is first added to the cluster. If you add a node to the cluster and then add the command-line flag afterward, this field/value won’t get populated and the integration won’t work as expected. No error is surfaced in this situation (at least, not that I’ve been able to find).</li><li>If you do find yourself with a missing .spec.providerID field on the Node object, you can add it with a kubectl edit node command. The format of the value for this field is aws:///&lt;az-of-instance&gt;/&lt;instance-id&gt;.</li><li>Missing AWS tags on resources will cause odd behaviors, like failing to create an ELB for a LoadBalancer-type Service. I haven’t had time to test all the different failure scenarios, but if the cloud provider integration isn’t working as expected I’d double-check that the Kubernetes-specific tags are present on all the AWS resources.</li></ul><p>Hopefully, the information in this article helps remove some of the confusion and lack of clarity around getting the AWS cloud provider working with your Kubernetes cluster. I’ll do my best to keep this document updated as I discover additional failure scenarios or find more detailed documentation. If you have questions, feel free to <a href="https://twitter.com/scott_lowe">hit me on Twitter</a> or find me in <a href="https://kubernetes.slack.com">the Kubernetes Slack community</a>. (If you’re an expert in the AWS cloud provider code and can help flesh out the details of this post, please contact me as well!) Have fun out there, fellow Kubernauts!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6f0349b512bd" width="1" height="1" alt=""><hr><p><a href="https://blog.heptio.com/setting-up-the-kubernetes-aws-cloud-provider-6f0349b512bd">Setting up the Kubernetes AWS Cloud Provider</a> was originally published in <a href="https://blog.heptio.com">Heptio</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
      <guid>https://blog.heptio.com/setting-up-the-kubernetes-aws-cloud-provider-6f0349b512bd?source=rss----7d24bed16a19---4</guid>
      <pubDate>Tue, 13 Nov 2018 17:40:06 +0000</pubDate>
    </item>
    <item>
      <title>Announcing Ark v0.10, with greater support for hybrid and multi-cloud deployments</title>
      <link>https://blog.heptio.com/announcing-ark-v0-10-with-greater-support-for-hybrid-and-multi-cloud-deployments-b52f6ec6148a?source=rss----7d24bed16a19---4</link>
      <description></description>
      <content:encoded><![CDATA[<h3>Announcing Heptio Ark v0.10, with greater support for hybrid and multi-cloud deployments</h3><p>By: <a href="https://medium.com/u/1920b9dbdb67">Carlisia</a>, <a href="https://medium.com/u/445699607da2">Nolan Brubaker</a>, Wayne Witzell III, <a href="https://medium.com/u/215f1d238282">Jennifer Rondeau</a>, <a href="https://medium.com/u/b82e21f7efd6">Steve Kriss</a>, <a href="https://medium.com/u/a0c587d8f149">Andy Goldstein</a>, and <a href="https://medium.com/u/fef13d775a44">Ross Kukulinski</a></p><p>We’re excited to announce the release of <a href="https://github.com/heptio/ark/releases/tag/v0.10.0">Heptio Ark v0.10</a>! This release includes features that give you greater flexibility in migrating applications and organizing cluster backups, along with some usability improvements. Most critically, Ark v0.10 introduces the ability to specify multiple volume snapshot locations, so that if you’re using more than one provider for volume storage within a cluster, you can now snapshot and fully back up every volume.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/744/1*Ekfq_dzHTw9Ke8QW1Hdg3g.png" /></figure><p>We know that today, most Ark users tend to have one volume provider within a cluster, like Portworx or Amazon EBS. However, this can pose challenges for application portability, or if you need faster access speeds for certain workloads. Being able to specify more than one location for backing up data volumes gives you more flexibility within a cluster and, in particular, makes it easier to migrate more complex applications from one Kubernetes environment to another.</p><p>Down the road, this feature will also become critical for supporting full backup replication. Imagine a world where you could define a replication policy that specifies the additional locations for where you can replicate a backup or a volume snapshot, easily solving for redundancy and cluster restoration across regions.</p><p>Read on for more details about this feature and other benefits of Ark v0.10.</p><h3>Support for multiple volume snapshot locations from multiple providers</h3><p>In Ark versions prior to v0.10, you can snapshot volumes only from a single provider. For example, if you are running on AWS and using EBS and Rook, you could snapshot volumes from only one of those two persistent volume providers. With Ark v0.10 you can now specify multiple volume snapshot locations from multiple providers..</p><p>Let’s say you have an application that you have deployed all in one pod. It has a database, which is kept on an Amazon EBS volume. It also holds user uploaded photos on a Portworx volume. There’s a third volume for generated reports, also stored in Portworx. You can now snapshot all three.</p><p>Every persistent volume to be backed up needs to be created with one associated volume snapshot location. This is also a two-step process: first, you create the VolumeSnapshotLocation CRDs for the locations you want (this only needs to be done once). Then, when creating a backup with a persistent volume, you select the location where you want the volume to be stored, by using the --snapshot-location flag and the name of one of the locations you created with the CRD.</p><p>Note that even though multiple volume snapshot locations can be created for each provider, when you create the backup, only one volume snapshot location per provider per backup can be used.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/614/0*MzvOfIukPsgvwdmm" /><figcaption>Multiple Volume Snapshots</figcaption></figure><p>As with regular backup storage locations, the volume snapshot locations can have a default associated with each of them so at backup creation time you don’t have to specify it. Unlike regular backups, however, the names of those locations must be specified as flags to the Ark server. They are not set up front.</p><p>Also as with the new BackupStorageLocation, the new VolumeSnapshotLocation CRD takes the place of the persistent volume setting in the previous Config CRD.</p><h3>Ability to specify multiple backup locations</h3><p>Backups now can be stored in different locations. You might want some backups to go for example to a bucket named full-cluster-backups in us-east-1, and other backups to be stored in a bucket named namespace-backups in us-east-2. As you can see, backup locations can now be in different regions.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/745/1*cRonOT3G9W0rXl_uHqdakw.png" /><figcaption>Multiple backup locations</figcaption></figure><p>Every backup now needs to be created with one associated backup storage location. This is a two-step process: first, you create the BackupStorageLocation CRDs for the locations you want. Then, when creating a backup, you select the location where you want the backup to be stored by using the --backup-location flag and the name of one of the locations you created with the CRD.</p><p>The exception to having to specify the name of a backup storage location is if you want to use the default location feature. In this case, you create the BackupStorageLocation CRD as expected, with the name default. Then, when you create a backup and don’t specify a location, the backup is stored in the default location. You can also rename the default location when you create the CRD, but you must then be sure to specify the --default-backup-storage-location flag when you create the Ark server deployment.</p><p>The BackupStorageLocation CRD replaces the previous Config CRD (now deprecated), which was where you defined the name of your backup, bucket and region</p><h3>Streamlined backup storage</h3><p>This version also introduces the ability to store backups under prefixes in an object storage bucket. Prior to v0.10, Ark stored all backups from a cluster at the root of the bucket. This meant if you wanted to organize the backup of each of your clusters separately, you’d have to create a bucket for each. As of version 0.10, you can organize backups from each cluster in the same bucket, using different prefixes. The new storage layout and instructions for migrating can be found in <a href="http://heptio.github.io/ark/">our documentation</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/424/0*TzP-3OcgFOdeA9T1" /><figcaption>New backup storage organization</figcaption></figure><h3>Stronger plugin system</h3><p>Ark’s plugin system has been significantly refactored to improve robustness and ease of development:</p><ul><li>Plugin processes are now automatically restarted if they unexpectedly terminate.</li><li>Plugin binaries can now contain more than one plugin implementation (for example, an object store and a block store, or many backup item actions).</li><li>Prefixes in object storage are now supported.</li></ul><p>Plugin authors must update their code to be compatible with v0.10. Plugin users will need to update the plugin image tags and/or image pull policy to ensure they have the latest plugins.</p><p>For details, see <a href="https://github.com/heptio/ark-plugin-example/">the GitHub repository for plugins</a>. We’ve updated it with new examples for v0.10, and we continue to provide a v0.9.x branch that refers to the older APIs.</p><p>The Ark team would like to thank plugin authors who have been collaborating with us leading up to the v0.10 launch. The following community Ark Plugins have already been updated to use the new plugin system:</p><ul><li><a href="https://github.com/StackPointCloud/ark-plugin-digitalocean">DigitalOcean Plugin</a> — NetApp, Inc.</li><li><a href="https://docs.portworx.com/scheduler/kubernetes/ark.html">Portworx Ark Plugin</a> — Portworx, Inc.</li></ul><h3>Additional usability improvements</h3><ul><li>The sync process, which ensures that Backup custom resources exist for each backup in object storage, has been revamped to run much more frequently (once per minute rather than once per hour), to use significantly fewer cloud provider API calls, and to not generate spurious Kubernetes API errors.</li><li>Restic backup data is now automatically stored in the same bucket/prefix as the rest of the Ark data. A separate bucket is no longer required (or allowed).</li><li>Ark resources (backups, restores, schedules) can now be bulk-deleted with the Ark CLI, using the --all or --selector flags, or by specifying multiple resource names as arguments to the delete commands.</li><li>The Ark CLI now supports waiting for backups and restores to complete, with the--wait flag for --ark backup create and --ark restore create.</li><li>Restores can be created directly from the most recent backup for a schedule, using --ark restore create --from-schedule SCHEDULE_NAME.</li></ul><h3>Get involved!</h3><p>We are in a phase of evaluating the implementation for replication and would love to have input from the community, especially about how to handle provider-specific issues.</p><p>With this in mind, we have started holding Heptio Ark design sessions. These are public meetings (open to all!) focused on a technical design discussion around whatever Ark feature the team is working on at that moment.</p><p>The next design session will be live streamed here: <a href="https://www.youtube.com/watch?v=Ml5lN4cV1Yk">https://www.youtube.com/watch?v=Ml5lN4cV1Yk</a></p><p>If you’d like to request that we cover a particular feature feel free to<a href="https://github.com/heptio/ark-community/issues"> make that request</a> in our Ark Community repo. Video recording of all our sessions can be found under our Heptio Ark<a href="https://www.youtube.com/playlist?list=PLvmPtYZtoXOFxnW32NRcS8857A4novNVs"> YouTube playlist</a>.</p><p>Other than that, you can also reach us through these channels:</p><p><a href="https://kubernetes.slack.com/messages/C6VCGP4MT">Ark on Kubernetes Slack</a><br><a href="https://groups.google.com/forum/#!forum/heptio-ark">Google Groups</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b52f6ec6148a" width="1" height="1" alt=""><hr><p><a href="https://blog.heptio.com/announcing-ark-v0-10-with-greater-support-for-hybrid-and-multi-cloud-deployments-b52f6ec6148a">Announcing Ark v0.10, with greater support for hybrid and multi-cloud deployments</a> was originally published in <a href="https://blog.heptio.com">Heptio</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
      <guid>https://blog.heptio.com/announcing-ark-v0-10-with-greater-support-for-hybrid-and-multi-cloud-deployments-b52f6ec6148a?source=rss----7d24bed16a19---4</guid>
      <pubDate>Thu, 15 Nov 2018 21:11:29 +0000</pubDate>
    </item>
    <item>
      <title>The Kubernetes Cluster API</title>
      <link>https://blog.heptio.com/the-kubernetes-cluster-api-de5a1ff870a5?source=rss----7d24bed16a19---4</link>
      <description></description>
      <content:encoded><![CDATA[<p>I’ve been working with Kubernetes since filing my first commit in October 2016. I’ve had the chance to collaborate with the community on Kops, Kubicorn, and Kubeadm, but there’s one gap that has been nagging me for years: how to to create the right abstraction for bringing up a Kubernetes cluster and managing it once it’s online. As it turned out, I wasn’t alone. So begins the story of Cluster API.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MtJFkhIIMUU2PhLd656XoQ.png" /></figure><p>In 2017 I spent an afternoon enjoying lunch at the Google office in Seattle’s Fremont neighborhood meeting with Robert Bailey and Weston Hutchins. We had connected via open source and shared a few similar ideas about <strong>declarative infrastructure</strong> built on new primitives in Kubernetes. Robert Bailey and Jacob Beacham began to spearhead the charge from Google. We slowly began to start formalizing an effort to create a system for bootstrapping and managing a Kubernetes cluster in a declarative way. I remember the grass roots nature of the project. Google began work on evangelizing these ideas within the Kubernetes community.</p><p>Following an<a href="https://groups.google.com/forum/#!topic/kubernetes-sig-cluster-lifecycle/ibVxgdLkeiM"> email to the Kubernetes mailing list sig cluster lifecycle</a>, the Cluster API working group was born. The group rapidly discovered other projects such as <a href="https://github.com/kubeup/archon">archon</a> and work from Loodse that also had similar ideas. It was clear we were all thinking of a brighter future with <strong>declarative infrastructure</strong>.</p><p>We started brainstorming what a declarative Kubernetes cluster might look like. We each consulted Kubernetes “elders” at our respective companies. The engineers at Google consulted Tim Hockin while I talked this over with Joe Beda, co-founder of Kubernetes and my long time colleague. Tim suggested we start building tooling and playing with abstractions to get a feel for what would work or not. At the time of this “sandbox” stage, we started prototyping in the <a href="http://github.com/kubernetes/kube-deploy">kube-deploy</a> repository. We needed a place to start hacking on the code and being that this feature was originally intentionally called out of scope finding a home was challenging. Later we were able to move out of that kube-deploy repository to <a href="http://github.com/kubernetes-sigs/cluster-api">cluster-api</a> which is where the code lives today.</p><p>Now “Cluster API,” which is short for “Cluster Management API,” is a great example of a bad name. As Tim St. Clair (Heptio) suggested, a better name for this layer of software is probably “cluster framework”. The community is still figuring out how we plan on dealing with this conundrum.</p><p>One of the first decisions the working group made was in regard to the scope of the API itself. In other words, what would our new abstraction be responsible for representing, and what would our abstraction intentionally ignore. We landed on two primary new resources, Clusters, and Machines. The Cluster resource was intended to map cleanly to the official Kubernetes bootstrap tool kubeadm, and the Machine resource was intended to be a simple representation of some compute load in a cloud (EC2 Instances, Google Virtual Machines, etc) or a physical machine. We explicitly decided to keep the new Machine resource separate from the existing node resource, as we could munge the two together later if necessary. According to Jacob Beacham, “the biggest motivation being that this was the only way to build the functionality outside of core, since the Node API is already defined in core.” Each Cluster resource would be mapped to a set of Machine resources, and ultimately all of these combined would represent a single Kubernetes cluster.</p><p>We later looked at implementing higher level resources to manage the Machine resource, in the same way deployments and ReplicaSets manage pods in Kubernetes today. Following this logic we modeled MachineDeployment after Deployment and MachineSet after ReplicaSet. These would allow unique strategy implementations for how various controllers would manage scaling and mutating underlying Machines.</p><p>We also decided early on that “how” a controller reconciles one of these resources is unique to the controller. In other words, the API should, by design, never include bash commands or any logic that suggests “how” to bring a cluster up, only “what” the cluster should look like after it’s been stood up. Where and how the controller reasons about what it needs to do is in scope for the controller and out of scope for the API. For example, Cluster API would define what version of Kubernetes to install, but would never define how to install that version.</p><p>With ClusterAPI, we hope to solve for many of the technical concerns in managing Kubernetes clusters, by drawing upon the lessons we’ve learned from Kubeadm, Kops, Kubicorn, and Kube-up, Gardener, etc.. So we set out to build ClusterAPI with the following goals in mind:</p><p><strong>Facilitate Atomic Transactions</strong></p><p>While keeping the spirit of planning for failure and mitigating hazards in our software, we knew we wanted to build software that would make it possible to <strong>guarantee</strong> a successful infrastructure mutation or no mutation at all. We learned this from Kops, when a cluster upgrade or create would fail partially through and we would orphan costly infrastructure in a cloud account.</p><p><strong>Enabling Cluster Automation</strong></p><p>With Cluster API we find that cluster level configuration is now declared through a common API, making it easy to automate and build tooling to interface with the new API. Tools like the cluster autoscaler now are liberated from having to concern themselves with <strong>how</strong> a node is created/destroyed. This simplifies the tooling and enables new tooling to be crafted around updating the cluster definition based on arbitrary business needs. This will change how operators think about managing a cluster.</p><p><strong>Keep infrastructure resilient</strong></p><p>Kops, Kubicorn, and Kube-up all have a fatal flaw. They run only for a finite amount of time. They all have some concept of accomplishing a task, and then terminating the program. This was a good starting point, but it didn’t offer the goal seeking and resilient behavior users are used to with Kubernetes. We needed a controller, to reconcile state over time. If a machine goes down, we don’t want to have to worry about bringing it back up.</p><p><strong>Create a better user experience</strong></p><p>Standing up a Kubernetes cluster is hard. Period. We hoped to build tooling that would go from 0 to Kubernetes in a friendly way, that made sense to operators. Furthermore, we hoped the API abstractions we created would also resonate with engineers so we could encourage them to build tooling around these new abstractions. For example, if the abstraction was user-friendly, we could port the upstream autoscaler over to using the new abstraction so it no longer had to concern itself with implementation — simply updating a record in Kubernetes.</p><p><strong>Provide a solution for cluster upgrades</strong></p><p>We wanted a turnkey solution to upgrades. Upgrading a Kubernetes cluster is tedious and risky, and having a residual controller in place not only solved the implementation of <em>how</em> to upgrade a Kubernetes cluster but it also gave us visibility into the state of the current upgrade.</p><p><strong>Bring the community together</strong></p><p>As it stands every Kubernetes installer to date represents a cluster in a different way, and the user experience is fragmented. This diminishes Kubernetes adoption and frankly pisses users off. We hoped to reduce this fragmentation and provide a solution to defining “what” a cluster looks like, and provide tooling to jumpstart implementations that solve “how” to bring a cluster to life.</p><p>All of these lessons and more are starting to sing in the Cluster API repositories. We are on the verge of alpha and beta releases for clouds like AWS and GCP. We hope that the community driven API becomes a standard teams can count on, and we hope that the community can start to offer an arsenal of controller implementations that bring these various variants of clusters to life.</p><p><strong>Going Further</strong></p><p>Learn More about the Cluster API from Kris Nova (Heptio) and Loc Nguyen (VMware) <a href="https://j.hept.io/cluster-api-kubecon-2018">live at Kubecon 2018</a> during their presentation on the topic. The talk will be recorded in case you can’t make it. We will upload the video to our advocacy site as soon as we can.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=de5a1ff870a5" width="1" height="1" alt=""><hr><p><a href="https://blog.heptio.com/the-kubernetes-cluster-api-de5a1ff870a5">The Kubernetes Cluster API</a> was originally published in <a href="https://blog.heptio.com">Heptio</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
      <guid>https://blog.heptio.com/the-kubernetes-cluster-api-de5a1ff870a5?source=rss----7d24bed16a19---4</guid>
      <pubDate>Tue, 04 Dec 2018 18:38:42 +0000</pubDate>
    </item>
    <item>
      <title>Heptio Contour and Heptio Gimbal on Stage at KubeCon NA</title>
      <link>https://blog.heptio.com/heptio-contour-and-heptio-gimbal-on-stage-at-kubecon-na-ed17b1fd496?source=rss----7d24bed16a19---4</link>
      <description></description>
      <content:encoded><![CDATA[<h3>Showcasing Heptio’s OSS solutions for multi-cluster ingress at KubeCon NA</h3><p>It’s been an exciting eight months since <a href="https://blog.heptio.com/introducing-heptio-gimbal-bridging-cloud-native-and-traditional-infrastructure-9d6224bece5a">launching Heptio Gimbal</a> in partnership with Actapio and Yahoo Japan Corporation ahead of KubeCon EU 2018. We created <a href="https://github.com/heptio/contour">Heptio Contour</a> and <a href="https://github.com/heptio/gimbal">Heptio Gimbal</a> as a complementary pair of open source projects to enable organizations to unify and manage internet traffic in hybrid cloud environments.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jVn5yJ0r9A4FAbo5gfO4GA.png" /></figure><p>Actapio and Yahoo Japan Corporation were critical early design partners and we were keen to consult with other Heptio customers as well as the larger Kubernetes community on how ingress could be improved. What we consistently heard was that people are struggling to manage ingress traffic in a multi-team and multi-cluster world. Notably, several of our customers had production outages due to teams creating conflicting routing rules with other teams.</p><p>Based on that feedback, we released Heptio Contour 0.6 in September which introduced the IngressRoute CRD, <a href="https://blog.heptio.com/introducing-heptio-contour-0-6-ecaa5ee6a67d">a novel new way</a> of safely managing <a href="https://blog.heptio.com/improving-the-multi-team-kubernetes-ingress-experience-with-heptio-contour-0-6-55ae0c0cadef">multi-team ingress</a>. It’s been great to see community interest soar regarding our design and implementation that models Kubernetes Ingress similar to the delegation model of DNS. In particular, the ability to do instantaneous blue-green deployments of Ingress rules is a great feature that has come out of this work.</p><p>It’s important to recognize that the success of Heptio Contour and Heptio Gimbal wouldn’t be possible without building on <a href="http://envoyproxy.io">Envoy proxy</a>. We couldn’t be happier with Envoy’s <a href="https://www.cncf.io/announcement/2018/11/28/cncf-announces-envoy-graduation/">recent graduation</a> from the CNCF incubation process, joining Kubernetes and Prometheus as top-level CNCF projects.</p><p>At KubeCon NA next week, we’re excited to tell you more about these projects and Actapio &amp; Yahoo Japan will be presenting on their production use of Heptio Gimbal. Read on for a complete list of related talks!</p><p>If you have any questions or are interested in learning more, reach us via the <a href="https://kubernetes.slack.com/messages/C8XRH2R4J">#contour</a> and <a href="https://kubernetes.slack.com/messages/CAAKHTB0Q/">#gimbal</a> channels on the Kubernetes community Slack or follow us on <a href="https://twitter.com/heptio">Twitter</a>.</p><h3>Heptio Contour &amp; Heptio Gimbal at KubeCon NA</h3><p><a href="https://kccna18.sched.com/event/GrR3/front-end-application-deployment-patterns-ross-kukulinski-heptio">Front-end Application Deployment Patterns</a><br>Tuesday 11:40am — 12:15pm, Ballroom 6E<br><a href="https://twitter.com/rosskukulinski">Ross Kukulinski</a> — Heptio, Inc</p><p><a href="https://kccna18.sched.com/event/GrWf/multi-cloud-ingress-lb-gimbal-use-case-in-actapio-and-yahoo-japan-hirotaka-ichikawa-actapio-inc-ryutaro-inoue-yahoo-japan">Multi-Cloud Ingress LB: Gimbal Use Case in Actapio and Yahoo Japan</a><br>Thursday 10:50am — 11:25am, Ballroom 6C<br><a href="https://twitter.com/hichtakk">Hirotaka Ichikawa</a> — Actapio, Inc, Ryutaro Inoue — Yahoo Japan Corporation</p><p>Heptio Gimbal Office Hours with Actapio &amp; Yahoo Japan<br>Thursday 12:15pm — 1:45pm, Heptio Booth #P8<br>Actapio, Yahoo Japan Corporation, and Heptio teams will be holding office hours with the Heptio Gimbal community. Stop by to meet with other companies and collaborators involved with the project.</p><p><a href="https://kccna18.sched.com/event/HChv/performance-testing-ingress-for-internet-scale-workloads-alexander-brand-heptio">Performance Testing Ingress for Internet-Scale Workloads</a><br>Thursday 3:40pm — 4;15pm, 4C 3/4<br><a href="https://twitter.com/AlexBrand">Alexander Brand</a> — Heptio, Inc</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ed17b1fd496" width="1" height="1" alt=""><hr><p><a href="https://blog.heptio.com/heptio-contour-and-heptio-gimbal-on-stage-at-kubecon-na-ed17b1fd496">Heptio Contour and Heptio Gimbal on Stage at KubeCon NA</a> was originally published in <a href="https://blog.heptio.com">Heptio</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
      <guid>https://blog.heptio.com/heptio-contour-and-heptio-gimbal-on-stage-at-kubecon-na-ed17b1fd496?source=rss----7d24bed16a19---4</guid>
      <pubDate>Wed, 05 Dec 2018 17:37:45 +0000</pubDate>
    </item>
    <item>
      <title>The Results are in … The State of K8s 2018</title>
      <link>https://blog.heptio.com/the-results-are-in-the-state-of-k8s-2018-d25e54819416?source=rss----7d24bed16a19---4</link>
      <description></description>
      <content:encoded><![CDATA[<h3>The Results are in … The State of K8s 2018</h3><p>Earlier this year, Kubernetes celebrated its fourth birthday. While it’s still a very young technology, adoption has been rapid. A number of studies have charted this course, but few have dug deeper to understand what’s driving the momentum.</p><p>That’s why we commissioned a third party to poll almost 400 IT decision-makers across practically every sector. We sought to understand who is moving to Kubernetes, what obstacles they have encountered and how they have benefitted from the technology. The results confirmed some of our suspicions, defied others and unwound a few cloud native myths.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DM9-s3rH7IPdRwSB_cqxuQ.png" /></figure><p><strong>Who is adopting Kubernetes?</strong></p><p>Kubernetes has crossed the chasm. About 60% of respondents are using Kubernetes today, and 65% expect to be using the technology in the next year. That jives with similar adoption numbers from other studies.</p><p>But what caught our attention was the number of organizations running Kubernetes in production. Half of the organizations running Kubernetes are doing so in production. The bigger and more complex the organization, the more likely they’re already in production; 77% of organizations with more than 1,000 developers and 88% of organizations with more than 1,000 containers.</p><p>And they’re not just leaning on Kubernetes for stateless apps. 63% are running stateful apps, 53% have entrusted data analytics to the platform and 31% operate IoT apps on Kubernetes.</p><p>In short, as we enter 2019 it is abundantly clear that Kubernetes is ready for production workloads.</p><p><strong>What obstacles are being encountered?</strong></p><p>Now, just because Kubernetes is in wide use doesn’t mean it’s a smooth ride — there are obstacles. The respondents of our study pointed to two prominent pain points: (1) Making early design and deployment decisions with confidence (47%), and (2) Aligning internal teams around common technologies (46%)</p><p>One look at the CNCF landscape and it’s easy to see why organizations can struggle to make architectural decisions — it’s a jungle and they need a guide. Our survey showed that respondents seek out multiple sources, from peers to providers to third parties.</p><p>As organizations look to build internal cloud native expertise, they are turning to the infrastructure and operations teams to lead. But increasingly we see the emergence of Site Reliability Engineering (SRE). While Operations led architectural decisions in 54% of organizations, SRE played that role in 31% of organizations … and 48% of organizations with more than 1,000 developers. Our view is that SRE-Ops will emerge as a key role in large enterprises, functioning like DevOps at scale.</p><p><strong>How are organizations benefiting?</strong></p><p>So, is it worth it to work through these obstacles? According to our survey respondents, the answer is a resounding yes.</p><p>That’s because 63% of organizations that have deployed Kubernetes are immediately using their resources more efficiently. And 58% have shortened their software development cycles. As for the organizations that are specifically using Kubernetes in production, their realization of benefits is nearly ten points higher.</p><p><strong>What’s next?</strong></p><p>If your organization is peering over the cloud native edge, what should you ready before you make the leap? According to our survey the most wished for capabilities are greater visibility into cluster performance and far simpler backup and recovery. That’s because the number one driver for organizations that have chosen Kubernetes is the ability to orchestrate their efforts across multiple clouds; it’s imperative to have visibility and control across teams and clusters.</p><p>When you can see clearly, you can confidently take the next step forward in your cloud native journey.</p><p><a href="https://hello.heptio.com/the-state-of-kubernetes/?utm_medium=blog&amp;utm_source=blog&amp;utm_campaign=state_of_k8s">Download a full copy of the 2018 State of Kubernetes report.</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d25e54819416" width="1" height="1" alt=""><hr><p><a href="https://blog.heptio.com/the-results-are-in-the-state-of-k8s-2018-d25e54819416">The Results are in … The State of K8s 2018</a> was originally published in <a href="https://blog.heptio.com">Heptio</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
      <guid>https://blog.heptio.com/the-results-are-in-the-state-of-k8s-2018-d25e54819416?source=rss----7d24bed16a19---4</guid>
      <pubDate>Thu, 06 Dec 2018 17:46:27 +0000</pubDate>
    </item>
    <item>
      <title>Kubernetes 1.27: KMS V2 Moves to Beta</title>
      <link>https://kubernetes.io/blog/2023/05/16/kms-v2-moves-to-beta/</link>
      <description>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Anish Ramasekar, Mo Khan, and Rita Zhang (Microsoft)&lt;/p&gt;&#xA;&lt;p&gt;With Kubernetes 1.27, we (SIG Auth) are moving Key Management Service (KMS) v2 API to beta.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-kms&#34;&gt;What is KMS?&lt;/h2&gt;&#xA;&lt;p&gt;One of the first things to consider when securing a Kubernetes cluster is encrypting etcd data at&#xA;rest. KMS provides an interface for a provider to utilize a key stored in an external key service to&#xA;perform this encryption.&lt;/p&gt;&#xA;&lt;p&gt;KMS v1 has been a feature of Kubernetes since version 1.10, and is currently in beta as of version&#xA;v1.12. KMS v2 was introduced as alpha in v1.25.&lt;/p&gt;&#xA;&lt;div class=&#34;alert alert-primary&#34; role=&#34;alert&#34;&gt;&#xA;&lt;h4 class=&#34;alert-heading&#34;&gt;Note&lt;/h4&gt;&#xA;The KMS v2 API and implementation changed in incompatible&#xA;ways in-between the alpha release in v1.25 and the beta release in v1.27. The design of KMS v2 has&#xA;changed since &lt;a href=&#34;https://kubernetes.io/blog/2022/09/09/kms-v2-improvements/&#34;&gt;the previous blog post&lt;/a&gt;&#xA;was written and it is not compatible with the design in this blog post. Attempting to upgrade from&#xA;old versions with the alpha feature enabled will result in data loss.&#xA;&lt;/div&gt;&#xA;&lt;h2 id=&#34;what-s-new-in-v2beta1&#34;&gt;What’s new in &lt;code&gt;v2beta1&lt;/code&gt;?&lt;/h2&gt;&#xA;&lt;p&gt;The KMS encryption provider uses an envelope encryption scheme to encrypt data in etcd. The data is&#xA;encrypted using a data encryption key (DEK). The DEKs are encrypted with a key encryption key (KEK)&#xA;that is stored and managed in a remote KMS. With KMS v1, a new DEK is generated for each encryption.&#xA;With KMS v2, a new DEK is only generated on server startup and when the KMS plugin informs the API&#xA;server that a KEK rotation has occurred.&lt;/p&gt;&#xA;&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;&#xA;&lt;h4 class=&#34;alert-heading&#34;&gt;Caution&lt;/h4&gt;&#xA;&lt;p&gt;If you are running virtual machine (VM) based nodes&#xA;that leverage VM state store with this feature, you must not use KMS v2.&lt;/p&gt;&#xA;&lt;p&gt;With KMS v2, the API server uses AES-GCM with a 12 byte nonce (8 byte atomic counter and 4 bytes&#xA;random data) for encryption. The following issues could occur if the VM is saved and restored:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The counter value may be lost or corrupted if the VM is saved in an inconsistent state or&#xA;restored improperly. This can lead to a situation where the same counter value is used twice,&#xA;resulting in the same nonce being used for two different messages.&lt;/li&gt;&#xA;&lt;li&gt;If the VM is restored to a previous state, the counter value may be set back to its previous&#xA;value, resulting in the same nonce being used again.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Although both of these cases are partially mitigated by the 4 byte random nonce, this can compromise&#xA;the security of the encryption.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;h3 id=&#34;sequence-diagram&#34;&gt;Sequence Diagram&lt;/h3&gt;&#xA;&lt;h4 id=&#34;encrypt-request&#34;&gt;Encrypt Request&lt;/h4&gt;&#xA;&lt;!-- source&#xA;```mermaid&#xA;%%{init:{&#34;theme&#34;:&#34;neutral&#34;, &#34;sequence&#34;: {&#34;mirrorActors&#34;:true},&#xA;&#34;themeVariables&#34;: {&#xA;&#34;actorBkg&#34;:&#34;royalblue&#34;,&#xA;&#34;actorTextColor&#34;:&#34;white&#34;&#xA;}}}%%&#xA;sequenceDiagram&#xA;participant user&#xA;participant kube_api_server&#xA;participant kms_plugin&#xA;participant external_kms&#xA;alt Generate DEK at startup&#xA;Note over kube_api_server,external_kms: Refer to Generate Data Encryption Key (DEK) diagram for details&#xA;end&#xA;user-&gt;&gt;kube_api_server: create/update resource that&#39;s to be encrypted&#xA;kube_api_server-&gt;&gt;kube_api_server: encrypt resource with DEK&#xA;kube_api_server-&gt;&gt;etcd: store encrypted object&#xA;```&#xA;--&gt;&#xA;&lt;figure class=&#34;diagram-large&#34;&gt;&#xA;&lt;img src=&#34;https://kubernetes.io/images/blog/2023-05-16-kubernetes-1.27-kmsv2-beta/kubernetes-1.27-encryption.svg&#34;&#xA;alt=&#34;Sequence diagram for KMSv2 beta Encrypt&#34;/&gt;&#xA;&lt;/figure&gt;&#xA;&lt;h4 id=&#34;decrypt-request&#34;&gt;Decrypt Request&lt;/h4&gt;&#xA;&lt;!-- source&#xA;```mermaid&#xA;%%{init:{&#34;theme&#34;:&#34;neutral&#34;, &#34;sequence&#34;: {&#34;mirrorActors&#34;:true},&#xA;&#34;themeVariables&#34;: {&#xA;&#34;actorBkg&#34;:&#34;royalblue&#34;,&#xA;&#34;actorTextColor&#34;:&#34;white&#34;&#xA;}}}%%&#xA;sequenceDiagram&#xA;participant user&#xA;participant kube_api_server&#xA;participant kms_plugin&#xA;participant external_kms&#xA;participant etcd&#xA;user-&gt;&gt;kube_api_server: get/list resource that&#39;s encrypted&#xA;kube_api_server-&gt;&gt;etcd: get encrypted resource&#xA;etcd-&gt;&gt;kube_api_server: encrypted resource&#xA;alt Encrypted DEK not in cache&#xA;kube_api_server-&gt;&gt;kms_plugin: decrypt request&#xA;kms_plugin-&gt;&gt;external_kms: decrypt DEK with remote KEK&#xA;external_kms-&gt;&gt;kms_plugin: decrypted DEK&#xA;kms_plugin-&gt;&gt;kube_api_server: return decrypted DEK&#xA;kube_api_server-&gt;&gt;kube_api_server: cache decrypted DEK&#xA;end&#xA;kube_api_server-&gt;&gt;kube_api_server: decrypt resource with DEK&#xA;kube_api_server-&gt;&gt;user: return decrypted resource&#xA;```&#xA;--&gt;&#xA;&lt;figure class=&#34;diagram-large&#34;&gt;&#xA;&lt;img src=&#34;https://kubernetes.io/images/blog/2023-05-16-kubernetes-1.27-kmsv2-beta/kubernetes-1.27-decryption.svg&#34;&#xA;alt=&#34;Sequence diagram for KMSv2 beta Decrypt&#34;/&gt;&#xA;&lt;/figure&gt;&#xA;&lt;h4 id=&#34;status-request&#34;&gt;Status Request&lt;/h4&gt;&#xA;&lt;!-- source&#xA;```mermaid&#xA;%%{init:{&#34;theme&#34;:&#34;neutral&#34;, &#34;sequence&#34;: {&#34;mirrorActors&#34;:true},&#xA;&#34;themeVariables&#34;: {&#xA;&#34;actorBkg&#34;:&#34;royalblue&#34;,&#xA;&#34;actorTextColor&#34;:&#34;white&#34;&#xA;}}}%%&#xA;sequenceDiagram&#xA;participant kube_api_server&#xA;participant kms_plugin&#xA;participant external_kms&#xA;alt Generate DEK at startup&#xA;Note over kube_api_server,external_kms: Refer to Generate Data Encryption Key (DEK) diagram for details&#xA;end&#xA;loop every minute (or every 10s if error or unhealthy)&#xA;kube_api_server-&gt;&gt;kms_plugin: status request&#xA;kms_plugin-&gt;&gt;external_kms: validate remote KEK&#xA;external_kms-&gt;&gt;kms_plugin: KEK status&#xA;kms_plugin-&gt;&gt;kube_api_server: return status response &lt;br/&gt; {&#34;healthz&#34;: &#34;ok&#34;, key_id: &#34;&lt;remote KEK ID&gt;&#34;, &#34;version&#34;: &#34;v2beta1&#34;}&#xA;alt KEK rotation detected (key_id changed), rotate DEK&#xA;Note over kube_api_server,external_kms: Refer to Generate Data Encryption Key (DEK) diagram for details&#xA;end&#xA;end&#xA;```&#xA;--&gt;&#xA;&lt;figure class=&#34;diagram-large&#34;&gt;&#xA;&lt;img src=&#34;https://kubernetes.io/images/blog/2023-05-16-kubernetes-1.27-kmsv2-beta/kubernetes-1.27-status.svg&#34;&#xA;alt=&#34;Sequence diagram for KMSv2 beta Status&#34;/&gt;&#xA;&lt;/figure&gt;&#xA;&lt;h4 id=&#34;generate-data-encryption-key-dek&#34;&gt;Generate Data Encryption Key (DEK)&lt;/h4&gt;&#xA;&lt;!-- source&#xA;```mermaid&#xA;%%{init:{&#34;theme&#34;:&#34;neutral&#34;, &#34;sequence&#34;: {&#34;mirrorActors&#34;:true},&#xA;&#34;themeVariables&#34;: {&#xA;&#34;actorBkg&#34;:&#34;royalblue&#34;,&#xA;&#34;actorTextColor&#34;:&#34;white&#34;&#xA;}}}%%&#xA;sequenceDiagram&#xA;participant kube_api_server&#xA;participant kms_plugin&#xA;participant external_kms&#xA;kube_api_server-&gt;&gt;kube_api_server: generate DEK&#xA;kube_api_server-&gt;&gt;kms_plugin: encrypt request&#xA;kms_plugin-&gt;&gt;external_kms: encrypt DEK with remote KEK&#xA;external_kms-&gt;&gt;kms_plugin: encrypted DEK&#xA;kms_plugin-&gt;&gt;kube_api_server: return encrypt response &lt;br/&gt; {&#34;ciphertext&#34;: &#34;&lt;encrypted DEK&gt;&#34;, key_id: &#34;&lt;remote KEK ID&gt;&#34;, &#34;annotations&#34;: {}}&#xA;```&#xA;--&gt;&#xA;&lt;figure class=&#34;diagram-large&#34;&gt;&#xA;&lt;img src=&#34;https://kubernetes.io/images/blog/2023-05-16-kubernetes-1.27-kmsv2-beta/kubernetes-1.27-generate-dek.svg&#34;&#xA;alt=&#34;Sequence diagram for KMSv2 beta Generate DEK&#34;/&gt;&#xA;&lt;/figure&gt;&#xA;&lt;h3 id=&#34;performance-improvements&#34;&gt;Performance Improvements&lt;/h3&gt;&#xA;&lt;p&gt;With KMS v2, we have made significant improvements to the performance of the KMS encryption&#xA;provider. In case of KMS v1, a new DEK is generated for every encryption. This means that for every&#xA;write request, the API server makes a call to the KMS plugin to encrypt the DEK using the remote&#xA;KEK. The API server also has to cache the DEKs to avoid making a call to the KMS plugin for every&#xA;read request. When the API server restarts, it has to populate the cache by making a call to the KMS&#xA;plugin for every DEK in the etcd store based on the cache size. This is a significant overhead for&#xA;the API server. With KMS v2, the API server generates a DEK at startup and caches it. The API server&#xA;also makes a call to the KMS plugin to encrypt the DEK using the remote KEK. This is a one-time call&#xA;at startup and on KEK rotation. The API server then uses the cached DEK to encrypt the resources.&#xA;This reduces the number of calls to the KMS plugin and improves the overall latency of the API&#xA;server requests.&lt;/p&gt;&#xA;&lt;p&gt;We conducted a test that created 12k secrets and measured the time taken for the API server to&#xA;encrypt the resources. The metric used was&#xA;&lt;a href=&#34;https://kubernetes.io/docs/reference/instrumentation/metrics/&#34;&gt;&lt;code&gt;apiserver_storage_transformation_duration_seconds&lt;/code&gt;&lt;/a&gt;.&#xA;For KMS v1, the test was run on a managed Kubernetes v1.25 cluster with 2 nodes. There was no&#xA;additional load on the cluster during the test. For KMS v2, the test was run in the Kubernetes CI&#xA;environment with the following &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/release-1.27/test/e2e/testing-manifests/auth/encrypt/kind.yaml&#34;&gt;cluster&#xA;configuration&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;table&gt;&#xA;&lt;thead&gt;&#xA;&lt;tr&gt;&#xA;&lt;th&gt;KMS Provider&lt;/th&gt;&#xA;&lt;th&gt;Time taken by 95 percentile&lt;/th&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/thead&gt;&#xA;&lt;tbody&gt;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;KMS v1&lt;/td&gt;&#xA;&lt;td&gt;160ms&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;KMS v2&lt;/td&gt;&#xA;&lt;td&gt;80μs&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;The results show that the KMS v2 encryption provider is three orders of magnitude faster than the&#xA;KMS v1 encryption provider.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-s-next&#34;&gt;What&#39;s next?&lt;/h2&gt;&#xA;&lt;p&gt;For Kubernetes v1.28, we expect the feature to stay in beta. In the coming releases we want to&#xA;investigate:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Cryptographic changes to remove the limitation on VM state store.&lt;/li&gt;&#xA;&lt;li&gt;Kubernetes REST API changes to enable a more robust story around key rotation.&lt;/li&gt;&#xA;&lt;li&gt;Handling undecryptable resources. Refer to the&#xA;&lt;a href=&#34;https://github.com/kubernetes/enhancements/pull/3927&#34;&gt;KEP&lt;/a&gt; for details.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;You can learn more about KMS v2 by reading &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/&#34;&gt;Using a KMS provider for data&#xA;encryption&lt;/a&gt;. You can also follow along on the&#xA;&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/3299-kms-v2-improvements/#readme&#34;&gt;KEP&lt;/a&gt;&#xA;to track progress across the coming Kubernetes releases.&lt;/p&gt;&#xA;&lt;h2 id=&#34;call-to-action&#34;&gt;Call to action&lt;/h2&gt;&#xA;&lt;p&gt;In this blog post, we have covered the improvements made to the KMS encryption provider in&#xA;Kubernetes v1.27. We have also discussed the new KMS v2 API and how it works. We would love to hear&#xA;your feedback on this feature. In particular, we would like feedback from Kubernetes KMS plugin&#xA;implementors as they go through the process of building their integrations with this new API. Please&#xA;reach out to us on the &lt;a href=&#34;https://kubernetes.slack.com/archives/C03035EH4VB&#34;&gt;#sig-auth-kms-dev&lt;/a&gt;&#xA;channel on Kubernetes Slack.&lt;/p&gt;&#xA;&lt;h2 id=&#34;how-to-get-involved&#34;&gt;How to get involved&lt;/h2&gt;&#xA;&lt;p&gt;If you are interested in getting involved in the development of this feature, share feedback, or&#xA;participate in any other ongoing SIG Auth projects, please reach out on the&#xA;&lt;a href=&#34;https://kubernetes.slack.com/archives/C0EN96KUY&#34;&gt;#sig-auth&lt;/a&gt; channel on Kubernetes Slack.&lt;/p&gt;&#xA;&lt;p&gt;You are also welcome to join the bi-weekly &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-auth/README.md#meetings&#34;&gt;SIG Auth&#xA;meetings&lt;/a&gt;, held&#xA;every-other Wednesday.&lt;/p&gt;&#xA;&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;&#xA;&lt;p&gt;This feature has been an effort driven by contributors from several different companies. We would&#xA;like to extend a huge thank you to everyone that contributed their time and effort to help make this&#xA;possible.&lt;/p&gt;</description>
      <guid>https://kubernetes.io/blog/2023/05/16/kms-v2-moves-to-beta/</guid>
      <pubDate>Tue, 16 May 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Having fun with seccomp profiles on the edge</title>
      <link>https://kubernetes.io/blog/2023/05/18/seccomp-profiles-edge/</link>
      <description>&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Sascha Grunert&lt;/p&gt;&#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes-sigs/security-profiles-operator&#34;&gt;Security Profiles Operator (SPO)&lt;/a&gt; is a feature-rich&#xA;&lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/operator&#34;&gt;operator&lt;/a&gt; for Kubernetes to make managing seccomp, SELinux and&#xA;AppArmor profiles easier than ever. Recording those profiles from scratch is one&#xA;of the key features of this operator, which usually involves the integration&#xA;into large CI/CD systems. Being able to test the recording capabilities of the&#xA;operator in edge cases is one of the recent development efforts of the SPO and&#xA;makes it excitingly easy to play around with seccomp profiles.&lt;/p&gt;&#xA;&lt;h2 id=&#34;recording-seccomp-profiles-with-spoc-record&#34;&gt;Recording seccomp profiles with &lt;code&gt;spoc record&lt;/code&gt;&lt;/h2&gt;&#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes-sigs/security-profiles-operator/releases/v0.8.0&#34;&gt;v0.8.0&lt;/a&gt; release of the Security Profiles Operator shipped a new&#xA;command line interface called &lt;code&gt;spoc&lt;/code&gt;, a little helper tool for recording and&#xA;replaying seccomp profiles among various other things that are out of scope of&#xA;this blog post.&lt;/p&gt;&#xA;&lt;p&gt;Recording a seccomp profile requires a binary to be executed, which can be a&#xA;simple golang application which just calls &lt;a href=&#34;https://man7.org/linux/man-pages/man2/uname.2.html&#34;&gt;&lt;code&gt;uname(2)&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;package&lt;/span&gt; main&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; (&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;syscall&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;func&lt;/span&gt; &lt;span style=&#34;color:#00a000&#34;&gt;main&lt;/span&gt;() {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; utsname &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; syscall.Utsname{}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;if&lt;/span&gt; err &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; syscall.&lt;span style=&#34;color:#00a000&#34;&gt;Uname&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&lt;/span&gt;utsname); err &lt;span style=&#34;color:#666&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;nil&lt;/span&gt; {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#a2f&#34;&gt;panic&lt;/span&gt;(err)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Building a binary from that code can be done by:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; go build -o main main.go&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; ldd ./main&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt; not a dynamic executable&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now it&#39;s possible to download the latest binary of &lt;a href=&#34;https://github.com/kubernetes-sigs/security-profiles-operator/releases/download/v0.8.0/spoc.amd64&#34;&gt;&lt;code&gt;spoc&lt;/code&gt; from&#xA;GitHub&lt;/a&gt; and run the application on Linux with it:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; sudo ./spoc record ./main&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:08:25.591945 Loading bpf module&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:08:25.591958 Using system btf file&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;libbpf: loading object &amp;#39;recorder.bpf.o&amp;#39; from buffer&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;…&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;libbpf: prog &amp;#39;sys_enter&amp;#39;: relo #3: patched insn #22 (ALU/ALU64) imm 16 -&amp;gt; 16&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:08:25.610767 Getting bpf program sys_enter&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:08:25.610778 Attaching bpf tracepoint&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:08:25.611574 Getting syscalls map&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:08:25.611582 Getting pid_mntns map&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:08:25.613097 Module successfully loaded&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:08:25.613311 Processing events&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:08:25.613693 Running command with PID: 336007&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:08:25.613835 Received event: pid: 336007, mntns: 4026531841&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:08:25.613951 No container ID found for PID (pid=336007, mntns=4026531841, err=unable to find container ID in cgroup path)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:08:25.614856 Processing recorded data&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:08:25.614975 Found process mntns 4026531841 in bpf map&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:08:25.615110 Got syscalls: read, close, mmap, rt_sigaction, rt_sigprocmask, madvise, nanosleep, clone, uname, sigaltstack, arch_prctl, gettid, futex, sched_getaffinity, exit_group, openat&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:08:25.615195 Adding base syscalls: access, brk, capget, capset, chdir, chmod, chown, close_range, dup2, dup3, epoll_create1, epoll_ctl, epoll_pwait, execve, faccessat2, fchdir, fchmodat, fchown, fchownat, fcntl, fstat, fstatfs, getdents64, getegid, geteuid, getgid, getpid, getppid, getuid, ioctl, keyctl, lseek, mkdirat, mknodat, mount, mprotect, munmap, newfstatat, openat2, pipe2, pivot_root, prctl, pread64, pselect6, readlink, readlinkat, rt_sigreturn, sched_yield, seccomp, set_robust_list, set_tid_address, setgid, setgroups, sethostname, setns, setresgid, setresuid, setsid, setuid, statfs, statx, symlinkat, tgkill, umask, umount2, unlinkat, unshare, write&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:08:25.616293 Wrote seccomp profile to: /tmp/profile.yaml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:08:25.616298 Unloading bpf module&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I have to execute &lt;code&gt;spoc&lt;/code&gt; as root because it will internally run an &lt;a href=&#34;https://ebpf.io&#34;&gt;ebpf&lt;/a&gt;&#xA;program by reusing the same code parts from the Security Profiles Operator&#xA;itself. I can see that the bpf module got loaded successfully and &lt;code&gt;spoc&lt;/code&gt;&#xA;attached the required tracepoint to it. Then it will track the main application&#xA;by using its &lt;a href=&#34;https://man7.org/linux/man-pages/man7/mount_namespaces.7.html&#34;&gt;mount namespace&lt;/a&gt; and process the recorded syscall data. The&#xA;nature of ebpf programs is that they see the whole context of the Kernel, which&#xA;means that &lt;code&gt;spoc&lt;/code&gt; tracks all syscalls of the system, but does not interfere with&#xA;their execution.&lt;/p&gt;&#xA;&lt;p&gt;The logs indicate that &lt;code&gt;spoc&lt;/code&gt; found the syscalls &lt;code&gt;read&lt;/code&gt;, &lt;code&gt;close&lt;/code&gt;,&#xA;&lt;code&gt;mmap&lt;/code&gt; and so on, including &lt;code&gt;uname&lt;/code&gt;. All other syscalls than &lt;code&gt;uname&lt;/code&gt; are coming&#xA;from the golang runtime and its garbage collection, which already adds overhead&#xA;to a basic application like in our demo. I can also see from the log line&#xA;&lt;code&gt;Adding base syscalls: …&lt;/code&gt; that &lt;code&gt;spoc&lt;/code&gt; adds a bunch of base syscalls to the&#xA;resulting profile. Those are used by the OCI runtime (like &lt;a href=&#34;https://github.com/opencontainers/runc&#34;&gt;runc&lt;/a&gt; or&#xA;&lt;a href=&#34;https://github.com/containers/crun&#34;&gt;crun&lt;/a&gt;) in order to be able to run a container. This means that &lt;code&gt;spoc&lt;/code&gt;&#xA;can be used to record seccomp profiles which then can be containerized directly.&#xA;This behavior can be disabled in &lt;code&gt;spoc&lt;/code&gt; by using the &lt;code&gt;--no-base-syscalls&lt;/code&gt;/&lt;code&gt;-n&lt;/code&gt;&#xA;or customized via the &lt;code&gt;--base-syscalls&lt;/code&gt;/&lt;code&gt;-b&lt;/code&gt; command line flags. This can be&#xA;helpful in cases where different OCI runtimes other than crun and runc are used,&#xA;or if I just want to record the seccomp profile for the application and stack&#xA;it with another &lt;a href=&#34;https://github.com/kubernetes-sigs/security-profiles-operator/blob/35ebdda/installation-usage.md#base-syscalls-for-a-container-runtime&#34;&gt;base profile&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The resulting profile is now available in &lt;code&gt;/tmp/profile.yaml&lt;/code&gt;, but the default&#xA;location can be changed using the &lt;code&gt;--output-file value&lt;/code&gt;/&lt;code&gt;-o&lt;/code&gt; flag:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; cat /tmp/profile.yaml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;security-profiles-operator.x-k8s.io/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;SeccompProfile&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;creationTimestamp&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;null&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;main&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;architectures&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;- SCMP_ARCH_X86_64&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;defaultAction&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;SCMP_ACT_ERRNO&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;syscalls&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;action&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;SCMP_ACT_ALLOW&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;names&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;- access&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;- arch_prctl&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;- brk&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;- …&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;- uname&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;- …&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;status&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;{}&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The seccomp profile Custom Resource Definition (CRD) can be directly used&#xA;together with the Security Profiles Operator for managing it within Kubernetes.&#xA;&lt;code&gt;spoc&lt;/code&gt; is also capable of producing raw seccomp profiles (as JSON), by using the&#xA;&lt;code&gt;--type&lt;/code&gt;/&lt;code&gt;-t&lt;/code&gt; &lt;code&gt;raw-seccomp&lt;/code&gt; flag:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; sudo ./spoc record --type raw-seccomp ./main&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;…&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;52.628827 Wrote seccomp profile to: /tmp/profile.json&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; jq . /tmp/profile.json&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;defaultAction&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;SCMP_ACT_ERRNO&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;architectures&amp;#34;&lt;/span&gt;: [&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;SCMP_ARCH_X86_64&amp;#34;&lt;/span&gt;],&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;syscalls&amp;#34;&lt;/span&gt;: [&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;names&amp;#34;&lt;/span&gt;: [&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;access&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;…&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;write&amp;#34;&lt;/span&gt;],&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;action&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;SCMP_ACT_ALLOW&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; ]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The utility &lt;code&gt;spoc record&lt;/code&gt; allows us to record complex seccomp profiles directly&#xA;from binary invocations in any Linux system which is capable of running the ebpf&#xA;code within the Kernel. But it can do more: How about modifying the seccomp&#xA;profile and then testing it by using &lt;code&gt;spoc run&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;running-seccomp-profiles-with-spoc-run&#34;&gt;Running seccomp profiles with &lt;code&gt;spoc run&lt;/code&gt;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;code&gt;spoc&lt;/code&gt; is also able to run binaries with applied seccomp profiles, making it&#xA;easy to test any modification to it. To do that, just run:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; sudo ./spoc run ./main&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:29:58.153263 Reading file /tmp/profile.yaml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:29:58.153311 Assuming YAML profile&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:29:58.154138 Setting up seccomp&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:29:58.154178 Load seccomp profile&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:29:58.154189 Starting audit log enricher&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:29:58.154224 Enricher reading from file /var/log/audit/audit.log&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:29:58.155356 Running command with PID: 437880&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It looks like that the application exited successfully, which is anticipated&#xA;because I did not modify the previously recorded profile yet. I can also&#xA;specify a custom location for the profile by using the &lt;code&gt;--profile&lt;/code&gt;/&lt;code&gt;-p&lt;/code&gt; flag,&#xA;but this was not necessary because I did not modify the default output location&#xA;from the record. &lt;code&gt;spoc&lt;/code&gt; will automatically determine if it&#39;s a raw (JSON) or CRD&#xA;(YAML) based seccomp profile and then apply it to the process.&lt;/p&gt;&#xA;&lt;p&gt;The Security Profiles Operator supports a &lt;a href=&#34;https://github.com/kubernetes-sigs/security-profiles-operator/blob/35ebdda/installation-usage.md#using-the-log-enricher&#34;&gt;log enricher feature&lt;/a&gt;,&#xA;which provides additional seccomp related information by parsing the audit logs.&#xA;&lt;code&gt;spoc run&lt;/code&gt; uses the enricher in the same way to provide more data to the end&#xA;users when it comes to debugging seccomp profiles.&lt;/p&gt;&#xA;&lt;p&gt;Now I have to modify the profile to see anything valuable in the output. For&#xA;example, I could remove the allowed &lt;code&gt;uname&lt;/code&gt; syscall:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; jq &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;del(.syscalls[0].names[] | select(. == &amp;#34;uname&amp;#34;))&amp;#39;&lt;/span&gt; /tmp/profile.json &amp;gt; /tmp/no-uname-profile.json&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And then try to run it again with the new profile &lt;code&gt;/tmp/no-uname-profile.json&lt;/code&gt;:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; sudo ./spoc run -p /tmp/no-uname-profile.json ./main&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:39:12.707798 Reading file /tmp/no-uname-profile.json&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:39:12.707892 Setting up seccomp&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:39:12.707920 Load seccomp profile&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:39:12.707982 Starting audit log enricher&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:39:12.707998 Enricher reading from file /var/log/audit/audit.log&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:39:12.709164 Running command with PID: 480512&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;panic: operation not permitted&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#888&#34;&gt;goroutine 1 [running]:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;main.main()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt; /path/to/main.go:10 +0x85&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:39:12.713035 Unable to run: launch runner: wait for command: exit status 2&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Alright, that was expected! The applied seccomp profile blocks the &lt;code&gt;uname&lt;/code&gt;&#xA;syscall, which results in an &amp;quot;operation not permitted&amp;quot; error. This error is&#xA;pretty generic and does not provide any hint on what got blocked by seccomp.&#xA;It is generally extremely difficult to predict how applications behave if single&#xA;syscalls are forbidden by seccomp. It could be possible that the application&#xA;terminates like in our simple demo, but it could also lead to a strange&#xA;misbehavior and the application does not stop at all.&lt;/p&gt;&#xA;&lt;p&gt;If I now change the default seccomp action of the profile from &lt;code&gt;SCMP_ACT_ERRNO&lt;/code&gt;&#xA;to &lt;code&gt;SCMP_ACT_LOG&lt;/code&gt; like this:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; jq &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;.defaultAction = &amp;#34;SCMP_ACT_LOG&amp;#34;&amp;#39;&lt;/span&gt; /tmp/no-uname-profile.json &amp;gt; /tmp/no-uname-profile-log.json&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then the log enricher will give us a hint that the &lt;code&gt;uname&lt;/code&gt; syscall got blocked&#xA;when using &lt;code&gt;spoc run&lt;/code&gt;:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; sudo ./spoc run -p /tmp/no-uname-profile-log.json ./main&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:48:07.470126 Reading file /tmp/no-uname-profile-log.json&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:48:07.470234 Setting up seccomp&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:48:07.470245 Load seccomp profile&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:48:07.470302 Starting audit log enricher&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:48:07.470339 Enricher reading from file /var/log/audit/audit.log&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:48:07.470889 Running command with PID: 522268&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;10:48:07.472007 Seccomp: uname (63)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The application will not terminate any more, but seccomp will log the behavior&#xA;to &lt;code&gt;/var/log/audit/audit.log&lt;/code&gt; and &lt;code&gt;spoc&lt;/code&gt; will parse the data to correlate it&#xA;directly to our program. Generating the log messages to the audit subsystem&#xA;comes with a large performance overhead and should be handled with care in&#xA;production systems. It also comes with a security risk when running untrusted&#xA;apps in audit mode in production environments.&lt;/p&gt;&#xA;&lt;p&gt;This demo should give you an impression how to debug seccomp profile issues with&#xA;applications, probably by using our shiny new helper tool powered by the&#xA;features of the Security Profiles Operator. &lt;code&gt;spoc&lt;/code&gt; is a flexible and portable&#xA;binary suitable for edge cases where resources are limited and even Kubernetes&#xA;itself may not be available with its full capabilities.&lt;/p&gt;&#xA;&lt;p&gt;Thank you for reading this blog post! If you&#39;re interested in more, providing&#xA;feedback or asking for help, then feel free to get in touch with us directly via&#xA;&lt;a href=&#34;https://kubernetes.slack.com/messages/security-profiles-operator&#34;&gt;Slack (#security-profiles-operator)&lt;/a&gt; or the &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-dev&#34;&gt;mailing list&lt;/a&gt;.&lt;/p&gt;</description>
      <guid>https://kubernetes.io/blog/2023/05/18/seccomp-profiles-edge/</guid>
      <pubDate>Thu, 18 May 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Using OCI artifacts to distribute security profiles for seccomp, SELinux and AppArmor</title>
      <link>https://kubernetes.io/blog/2023/05/24/oci-security-profiles/</link>
      <description>&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Sascha Grunert&lt;/p&gt;&#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes-sigs/security-profiles-operator&#34;&gt;Security Profiles Operator (SPO)&lt;/a&gt; makes managing seccomp, SELinux and&#xA;AppArmor profiles within Kubernetes easier than ever. It allows cluster&#xA;administrators to define the profiles in a predefined custom resource YAML,&#xA;which then gets distributed by the SPO into the whole cluster. Modification and&#xA;removal of the security profiles are managed by the operator in the same way,&#xA;but that’s a small subset of its capabilities.&lt;/p&gt;&#xA;&lt;p&gt;Another core feature of the SPO is being able to stack seccomp profiles. This&#xA;means that users can define a &lt;code&gt;baseProfileName&lt;/code&gt; in the YAML specification, which&#xA;then gets automatically resolved by the operator and combines the syscall rules.&#xA;If a base profile has another &lt;code&gt;baseProfileName&lt;/code&gt;, then the operator will&#xA;recursively resolve the profiles up to a certain depth. A common use case is to&#xA;define base profiles for low level container runtimes (like &lt;a href=&#34;https://github.com/opencontainers/runc&#34;&gt;runc&lt;/a&gt; or&#xA;&lt;a href=&#34;https://github.com/containers/crun&#34;&gt;crun&lt;/a&gt;) which then contain syscalls which are required in any case to run&#xA;the container. Alternatively, application developers can define seccomp base&#xA;profiles for their standard distribution containers and stack dedicated profiles&#xA;for the application logic on top. This way developers can focus on maintaining&#xA;seccomp profiles which are way simpler and scoped to the application logic,&#xA;without having a need to take the whole infrastructure setup into account.&lt;/p&gt;&#xA;&lt;p&gt;But how to maintain those base profiles? For example, the amount of required&#xA;syscalls for a runtime can change over its release cycle in the same way it can&#xA;change for the main application. Base profiles have to be available in the same&#xA;cluster, otherwise the main seccomp profile will fail to deploy. This means that&#xA;they’re tightly coupled to the main application profiles, which acts against the&#xA;main idea of base profiles. Distributing and managing them as plain files feels&#xA;like an additional burden to solve.&lt;/p&gt;&#xA;&lt;h2 id=&#34;oci-artifacts-to-the-rescue&#34;&gt;OCI artifacts to the rescue&lt;/h2&gt;&#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes-sigs/security-profiles-operator/releases/v0.8.0&#34;&gt;v0.8.0&lt;/a&gt; release of the Security Profiles Operator supports&#xA;managing base profiles as OCI artifacts! Imagine OCI artifacts as lightweight&#xA;container images, storing files in layers in the same way images do, but without&#xA;a process to be executed. Those artifacts can be used to store security profiles&#xA;like regular container images in compatible registries. This means they can be&#xA;versioned, namespaced and annotated similar to regular container images.&lt;/p&gt;&#xA;&lt;p&gt;To see how that works in action, specify a &lt;code&gt;baseProfileName&lt;/code&gt; prefixed with&#xA;&lt;code&gt;oci://&lt;/code&gt; within a seccomp profile CRD, for example:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;security-profiles-operator.x-k8s.io/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;SeccompProfile&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;test&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;defaultAction&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;SCMP_ACT_ERRNO&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;baseProfileName&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;oci://ghcr.io/security-profiles/runc:v1.1.5&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;syscalls&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;action&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;SCMP_ACT_ALLOW&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;names&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;- uname&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The operator will take care of pulling the content by using &lt;a href=&#34;https://oras.land&#34;&gt;oras&lt;/a&gt;, as&#xA;well as verifying the &lt;a href=&#34;https://github.com/sigstore/cosign&#34;&gt;sigstore (cosign)&lt;/a&gt; signatures of the artifact. If&#xA;the artifacts are not signed, then the SPO will reject them. The resulting&#xA;profile &lt;code&gt;test&lt;/code&gt; will then contain all base syscalls from the remote &lt;code&gt;runc&lt;/code&gt;&#xA;profile plus the additional allowed &lt;code&gt;uname&lt;/code&gt; one. It is also possible to&#xA;reference the base profile by its digest (SHA256) making the artifact to be&#xA;pulled more specific, for example by referencing&#xA;&lt;code&gt;oci://ghcr.io/security-profiles/runc@sha256:380…&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The operator internally caches pulled artifacts up to 24 hours for 1000&#xA;profiles, meaning that they will be refreshed after that time period, if the&#xA;cache is full or the operator daemon gets restarted.&lt;/p&gt;&#xA;&lt;p&gt;Because the overall resulting syscalls are hidden from the user (I only have the&#xA;&lt;code&gt;baseProfileName&lt;/code&gt; listed in the SeccompProfile, and not the syscalls themselves), I&#39;ll additionally&#xA;annotate that SeccompProfile with the final &lt;code&gt;syscalls&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Here&#39;s how the SeccompProfile looks after I annotate it:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; kubectl describe seccompprofile &lt;span style=&#34;color:#a2f&#34;&gt;test&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;Name: test&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;Namespace: security-profiles-operator&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;Labels: spo.x-k8s.io/profile-id=SeccompProfile-test&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;Annotations: syscalls:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt; [{&amp;#34;names&amp;#34;:[&amp;#34;arch_prctl&amp;#34;,&amp;#34;brk&amp;#34;,&amp;#34;capget&amp;#34;,&amp;#34;capset&amp;#34;,&amp;#34;chdir&amp;#34;,&amp;#34;clone&amp;#34;,&amp;#34;close&amp;#34;,...&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;API Version: security-profiles-operator.x-k8s.io/v1beta1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The SPO maintainers provide all public base profiles as part of the &lt;a href=&#34;https://github.com/orgs/security-profiles/packages&#34;&gt;“Security&#xA;Profiles” GitHub organization&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;managing-oci-security-profiles&#34;&gt;Managing OCI security profiles&lt;/h2&gt;&#xA;&lt;p&gt;Alright, now the official SPO provides a bunch of base profiles, but how can I&#xA;define my own? Well, first of all we have to choose a working registry. There&#xA;are a bunch of registries that already supports OCI artifacts:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/distribution/distribution&#34;&gt;CNCF Distribution&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://aka.ms/acr&#34;&gt;Azure Container Registry&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/ecr&#34;&gt;Amazon Elastic Container Registry&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry&#34;&gt;Google Artifact Registry&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.github.com/en/packages/guides/about-github-container-registry&#34;&gt;GitHub Packages container registry&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://hub.docker.com&#34;&gt;Docker Hub&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zotregistry.io&#34;&gt;Zot Registry&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The Security Profiles Operator ships a new command line interface called &lt;code&gt;spoc&lt;/code&gt;,&#xA;which is a little helper tool for managing OCI profiles among doing various other&#xA;things which are out of scope of this blog post. But, the command &lt;code&gt;spoc push&lt;/code&gt;&#xA;can be used to push a security profile to a registry:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; export USERNAME=my-user&#xA;&amp;gt; export PASSWORD=my-pass&#xA;&amp;gt; spoc push -f ./examples/baseprofile-crun.yaml ghcr.io/security-profiles/crun:v1.8.3&#xA;16:35:43.899886 Pushing profile ./examples/baseprofile-crun.yaml to: ghcr.io/security-profiles/crun:v1.8.3&#xA;16:35:43.899939 Creating file store in: /tmp/push-3618165827&#xA;16:35:43.899947 Adding profile to store: ./examples/baseprofile-crun.yaml&#xA;16:35:43.900061 Packing files&#xA;16:35:43.900282 Verifying reference: ghcr.io/security-profiles/crun:v1.8.3&#xA;16:35:43.900310 Using tag: v1.8.3&#xA;16:35:43.900313 Creating repository for ghcr.io/security-profiles/crun&#xA;16:35:43.900319 Using username and password&#xA;16:35:43.900321 Copying profile to repository&#xA;16:35:46.976108 Signing container image&#xA;Generating ephemeral keys...&#xA;Retrieving signed certificate...&#xA;Note that there may be personally identifiable information associated with this signed artifact.&#xA;This may include the email address associated with the account with which you authenticate.&#xA;This information will be used for signing this artifact and will be stored in public transparency logs and cannot be removed later.&#xA;By typing &amp;#39;y&amp;#39;, you attest that you grant (or have permission to grant) and agree to have this information stored permanently in transparency logs.&#xA;Your browser will now be opened to:&#xA;https://oauth2.sigstore.dev/auth/auth?access_type=…&#xA;Successfully verified SCT...&#xA;tlog entry created with index: 16520520&#xA;Pushing signature to: ghcr.io/security-profiles/crun&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can see that the tool automatically signs the artifact and pushes the&#xA;&lt;code&gt;./examples/baseprofile-crun.yaml&lt;/code&gt; to the registry, which is then directly ready&#xA;for usage within the SPO. If username and password authentication is required,&#xA;either use the &lt;code&gt;--username&lt;/code&gt;, &lt;code&gt;-u&lt;/code&gt; flag or export the &lt;code&gt;USERNAME&lt;/code&gt; environment&#xA;variable. To set the password, export the &lt;code&gt;PASSWORD&lt;/code&gt; environment variable.&lt;/p&gt;&#xA;&lt;p&gt;It is possible to add custom annotations to the security profile by using the&#xA;&lt;code&gt;--annotations&lt;/code&gt; / &lt;code&gt;-a&lt;/code&gt; flag multiple times in &lt;code&gt;KEY:VALUE&lt;/code&gt; format. Those have no&#xA;effect for now, but at some later point additional features of the operator may&#xA;rely them.&lt;/p&gt;&#xA;&lt;p&gt;The &lt;code&gt;spoc&lt;/code&gt; client is also able to pull security profiles from OCI artifact&#xA;compatible registries. To do that, just run &lt;code&gt;spoc pull&lt;/code&gt;:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; spoc pull ghcr.io/security-profiles/runc:v1.1.5&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;16:32:29.795597 Pulling profile from: ghcr.io/security-profiles/runc:v1.1.5&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;16:32:29.795610 Verifying signature&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#888&#34;&gt;Verification for ghcr.io/security-profiles/runc:v1.1.5 --&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;The following checks were performed on each of these signatures:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt; - Existence of the claims in the transparency log was verified offline&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt; - The code-signing certificate was verified using trusted certificate authority certificates&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#888&#34;&gt;[{&amp;#34;critical&amp;#34;:{&amp;#34;identity&amp;#34;:{&amp;#34;docker-reference&amp;#34;:&amp;#34;ghcr.io/security-profiles/runc&amp;#34;},…}}]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;16:32:33.208695 Creating file store in: /tmp/pull-3199397214&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;16:32:33.208713 Verifying reference: ghcr.io/security-profiles/runc:v1.1.5&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;16:32:33.208718 Creating repository for ghcr.io/security-profiles/runc&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;16:32:33.208742 Using tag: v1.1.5&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;16:32:33.208743 Copying profile from repository&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;16:32:34.119652 Reading profile&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;16:32:34.119677 Trying to unmarshal seccomp profile&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;16:32:34.120114 Got SeccompProfile: runc-v1.1.5&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;16:32:34.120119 Saving profile in: /tmp/profile.yaml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The profile can be now found in &lt;code&gt;/tmp/profile.yaml&lt;/code&gt; or the specified output file&#xA;&lt;code&gt;--output-file&lt;/code&gt; / &lt;code&gt;-o&lt;/code&gt;. We can specify an username and password in the same way&#xA;as for &lt;code&gt;spoc push&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;spoc&lt;/code&gt; makes it easy to manage security profiles as OCI artifacts, which can be&#xA;then consumed directly by the operator itself.&lt;/p&gt;&#xA;&lt;p&gt;That was our compact journey through the latest possibilities of the Security&#xA;Profiles Operator! If you&#39;re interested in more, providing feedback or asking&#xA;for help, then feel free to get in touch with us directly via &lt;a href=&#34;https://kubernetes.slack.com/messages/security-profiles-operator&#34;&gt;Slack&#xA;(#security-profiles-operator)&lt;/a&gt; or &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-dev&#34;&gt;the mailing list&lt;/a&gt;.&lt;/p&gt;</description>
      <guid>https://kubernetes.io/blog/2023/05/24/oci-security-profiles/</guid>
      <pubDate>Wed, 24 May 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>dl.k8s.io to adopt a Content Delivery Network</title>
      <link>https://kubernetes.io/blog/2023/06/09/dl-adopt-cdn/</link>
      <description>&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Arnaud Meukam (VMware), Hannah Aubry (Fastly), Frederico&#xA;Muñoz (SAS Institute)&lt;/p&gt;&#xA;&lt;p&gt;We&#39;re happy to announce that dl.k8s.io, home of the official Kubernetes&#xA;binaries, will soon be powered by &lt;a href=&#34;https://www.fastly.com&#34;&gt;Fastly&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Fastly is known for its high-performance content delivery network (CDN) designed&#xA;to deliver content quickly and reliably around the world. With its powerful&#xA;network, Fastly will help us deliver official Kubernetes binaries to users&#xA;faster and more reliably than ever before.&lt;/p&gt;&#xA;&lt;p&gt;The decision to use Fastly was made after an extensive evaluation process in&#xA;which we carefully evaluated several potential content delivery network&#xA;providers. Ultimately, we chose Fastly because of their commitment to the open&#xA;internet and proven track record of delivering fast and secure digital&#xA;experiences to some of the most known open source projects (through their &lt;a href=&#34;https://www.fastly.com/fast-forward&#34;&gt;Fast&#xA;Forward&lt;/a&gt; program).&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-you-need-to-know-about-this-change&#34;&gt;What you need to know about this change&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;On Monday, July 24th, the IP addresses and backend storage associated with the&#xA;dl.k8s.io domain name will change.&lt;/li&gt;&#xA;&lt;li&gt;The change will not impact the vast majority of users since the domain&#xA;name will remain the same.&lt;/li&gt;&#xA;&lt;li&gt;If you restrict access to specific IP ranges, access to the dl.k8s.io domain&#xA;could stop working.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;If you think you may be impacted or want to know more about this change,&#xA;please keep reading.&lt;/p&gt;&#xA;&lt;h2 id=&#34;why-are-we-making-this-change&#34;&gt;Why are we making this change&lt;/h2&gt;&#xA;&lt;p&gt;The official Kubernetes binaries site, dl.k8s.io, is used by thousands of users&#xA;all over the world, and currently serves &lt;em&gt;more than 5 petabytes of binaries each&#xA;month&lt;/em&gt;. This change will allow us to improve access to those resources by&#xA;leveraging a world-wide CDN.&lt;/p&gt;&#xA;&lt;h2 id=&#34;does-this-affect-dl-k8s-io-only-or-are-other-domains-also-affected&#34;&gt;Does this affect dl.k8s.io only, or are other domains also affected?&lt;/h2&gt;&#xA;&lt;p&gt;Only dl.k8s.io will be affected by this change.&lt;/p&gt;&#xA;&lt;h2 id=&#34;my-company-specifies-the-domain-names-that-we-are-allowed-to-be-accessed-will-this-change-affect-the-domain-name&#34;&gt;My company specifies the domain names that we are allowed to be accessed. Will this change affect the domain name?&lt;/h2&gt;&#xA;&lt;p&gt;No, the domain name (&lt;code&gt;dl.k8s.io&lt;/code&gt;) will remain the same: no change will be&#xA;necessary, and access to the Kubernetes release binaries site should not be&#xA;affected.&lt;/p&gt;&#xA;&lt;h2 id=&#34;my-company-uses-some-form-of-ip-filtering-will-this-change-affect-access-to-the-site&#34;&gt;My company uses some form of IP filtering. Will this change affect access to the site?&lt;/h2&gt;&#xA;&lt;p&gt;If IP-based filtering is in place, it’s possible that access to the site will be&#xA;affected when the new IP addresses become active.&lt;/p&gt;&#xA;&lt;h2 id=&#34;if-my-company-doesn-t-use-ip-addresses-to-restrict-network-traffic-do-we-need-to-do-anything&#34;&gt;If my company doesn’t use IP addresses to restrict network traffic, do we need to do anything?&lt;/h2&gt;&#xA;&lt;p&gt;No, the switch to the CDN should be transparent.&lt;/p&gt;&#xA;&lt;h2 id=&#34;will-there-be-a-dual-running-period&#34;&gt;Will there be a dual running period?&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;No, it is a cutover.&lt;/strong&gt; You can, however, test your networks right now to check&#xA;if they can route to the new public IP addresses from Fastly. You should add&#xA;the new IPs to your network&#39;s &lt;code&gt;allowlist&lt;/code&gt; before July 24th. Once the transfer is&#xA;complete, ensure your networks use the new IP addresses to connect to&#xA;the &lt;code&gt;dl.k8s.io&lt;/code&gt; service.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-are-the-new-ip-addresses&#34;&gt;What are the new IP addresses?&lt;/h2&gt;&#xA;&lt;p&gt;If you need to manage an allow list for downloads, you can get the ranges to&#xA;match from the Fastly API, in JSON: &lt;a href=&#34;https://api.fastly.com/public-ip-list&#34;&gt;public IP address&#xA;ranges&lt;/a&gt;. You don&#39;t need any credentials&#xA;to download that list of ranges.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-next-steps-would-you-recommend&#34;&gt;What next steps would you recommend?&lt;/h2&gt;&#xA;&lt;p&gt;If you have IP-based filtering in place, we recommend the following course of&#xA;action &lt;strong&gt;before July, 24th&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Add the new IP addresses to your allowlist.&lt;/li&gt;&#xA;&lt;li&gt;Conduct tests with your networks/firewall to ensure your networks can route to&#xA;the new IP addresses.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;After the change is made, we recommend double-checking that HTTP calls are&#xA;accessing dl.k8s.io with the new IP addresses.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-should-i-do-if-i-detect-some-abnormality-after-the-cutover-date&#34;&gt;What should I do if I detect some abnormality after the cutover date?&lt;/h2&gt;&#xA;&lt;p&gt;If you encounter any weirdness during binaries download, please &lt;a href=&#34;https://github.com/kubernetes/k8s.io/issues/new/choose&#34;&gt;open an&#xA;issue&lt;/a&gt;.&lt;/p&gt;</description>
      <guid>https://kubernetes.io/blog/2023/06/09/dl-adopt-cdn/</guid>
      <pubDate>Fri, 09 Jun 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Verifying Container Image Signatures Within CRI Runtimes</title>
      <link>https://kubernetes.io/blog/2023/06/29/container-image-signature-verification/</link>
      <description>&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Sascha Grunert&lt;/p&gt;&#xA;&lt;p&gt;The Kubernetes community has been signing their container image-based artifacts&#xA;since release v1.24. While the graduation of the &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/3031&#34;&gt;corresponding enhancement&lt;/a&gt;&#xA;from &lt;code&gt;alpha&lt;/code&gt; to &lt;code&gt;beta&lt;/code&gt; in v1.26 introduced signatures for the binary artifacts,&#xA;other projects followed the approach by providing image signatures for their&#xA;releases, too. This means that they either create the signatures within their&#xA;own CI/CD pipelines, for example by using GitHub actions, or rely on the&#xA;Kubernetes &lt;a href=&#34;https://github.com/kubernetes-sigs/promo-tools/blob/e2b96dd/docs/image-promotion.md&#34;&gt;image promotion&lt;/a&gt; process to automatically sign the images by&#xA;proposing pull requests to the &lt;a href=&#34;https://github.com/kubernetes/k8s.io/tree/4b95cc2/k8s.gcr.io&#34;&gt;k/k8s.io&lt;/a&gt; repository. A requirement for&#xA;using this process is that the project is part of the &lt;code&gt;kubernetes&lt;/code&gt; or&#xA;&lt;code&gt;kubernetes-sigs&lt;/code&gt; GitHub organization, so that they can utilize the community&#xA;infrastructure for pushing images into staging buckets.&lt;/p&gt;&#xA;&lt;p&gt;Assuming that a project now produces signed container image artifacts, how can&#xA;one actually verify the signatures? It is possible to do it manually like&#xA;outlined in the &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-artifacts/#verifying-image-signatures&#34;&gt;official Kubernetes documentation&lt;/a&gt;. The problem with this&#xA;approach is that it involves no automation at all and should be only done for&#xA;testing purposes. In production environments, tools like the &lt;a href=&#34;https://docs.sigstore.dev/policy-controller/overview&#34;&gt;sigstore&#xA;policy-controller&lt;/a&gt; can help with the automation. These tools&#xA;provide a higher level API by using &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources&#34;&gt;Custom Resource Definitions (CRD)&lt;/a&gt; as&#xA;well as an integrated &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers&#34;&gt;admission controller and webhook&lt;/a&gt; to verify&#xA;the signatures.&lt;/p&gt;&#xA;&lt;p&gt;The general usage flow for an admission controller based verification is:&lt;/p&gt;&#xA;&lt;figure&gt;&#xA;&lt;img src=&#34;https://kubernetes.io/blog/2023/06/29/container-image-signature-verification/flow.svg&#34;&#xA;alt=&#34;Create an instance of the policy and annotate the namespace to validate the signatures. Then create the pod. The controller evaluates the policy and if it passes, then it does the image pull if necessary. If the policy evaluation fails, then it will not admit the pod.&#34;/&gt;&#xA;&lt;/figure&gt;&#xA;&lt;p&gt;A key benefit of this architecture is simplicity: A single instance within the&#xA;cluster validates the signatures before any image pull can happen in the&#xA;container runtime on the nodes, which gets initiated by the kubelet. This&#xA;benefit also brings along the issue of separation: The node which should pull&#xA;the container image is not necessarily the same node that performs the admission. This&#xA;means that if the controller is compromised, then a cluster-wide policy&#xA;enforcement can no longer be possible.&lt;/p&gt;&#xA;&lt;p&gt;One way to solve this issue is doing the policy evaluation directly within the&#xA;&lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/cri&#34;&gt;Container Runtime Interface (CRI)&lt;/a&gt; compatible container runtime. The&#xA;runtime is directly connected to the &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet&#34;&gt;kubelet&lt;/a&gt; on a node and does all&#xA;the tasks like pulling images. &lt;a href=&#34;https://github.com/cri-o/cri-o&#34;&gt;CRI-O&lt;/a&gt; is one of those available runtimes&#xA;and will feature full support for container image signature verification in v1.28.&lt;/p&gt;&#xA;&lt;p&gt;How does it work? CRI-O reads a file called &lt;a href=&#34;https://github.com/containers/image/blob/b3e0ba2/docs/containers-policy.json.5.md#sigstoresigned&#34;&gt;&lt;code&gt;policy.json&lt;/code&gt;&lt;/a&gt;, which&#xA;contains all the rules defined for container images. For example, you can define a&#xA;policy which only allows signed images &lt;code&gt;quay.io/crio/signed&lt;/code&gt; for any tag or&#xA;digest like this:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;: [{ &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;reject&amp;#34;&lt;/span&gt; }],&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;transports&amp;#34;&lt;/span&gt;: {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;docker&amp;#34;&lt;/span&gt;: {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;quay.io/crio/signed&amp;#34;&lt;/span&gt;: [&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;sigstoreSigned&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;signedIdentity&amp;#34;&lt;/span&gt;: { &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;matchRepository&amp;#34;&lt;/span&gt; },&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;fulcio&amp;#34;&lt;/span&gt;: {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;oidcIssuer&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;https://github.com/login/oauth&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;subjectEmail&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;sgrunert@redhat.com&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;caData&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUI5ekNDQVh5Z0F3SUJBZ0lVQUxaTkFQRmR4SFB3amVEbG9Ed3lZQ2hBTy80d0NnWUlLb1pJemowRUF3TXcKS2pFVk1CTUdBMVVFQ2hNTWMybG5jM1J2Y21VdVpHVjJNUkV3RHdZRFZRUURFd2h6YVdkemRHOXlaVEFlRncweQpNVEV3TURjeE16VTJOVGxhRncwek1URXdNRFV4TXpVMk5UaGFNQ294RlRBVEJnTlZCQW9UREhOcFozTjBiM0psCkxtUmxkakVSTUE4R0ExVUVBeE1JYzJsbmMzUnZjbVV3ZGpBUUJnY3Foa2pPUFFJQkJnVXJnUVFBSWdOaUFBVDcKWGVGVDRyYjNQUUd3UzRJYWp0TGszL09sbnBnYW5nYUJjbFlwc1lCcjVpKzR5bkIwN2NlYjNMUDBPSU9aZHhleApYNjljNWlWdXlKUlErSHowNXlpK1VGM3VCV0FsSHBpUzVzaDArSDJHSEU3U1hyazFFQzVtMVRyMTlMOWdnOTJqCll6QmhNQTRHQTFVZER3RUIvd1FFQXdJQkJqQVBCZ05WSFJNQkFmOEVCVEFEQVFIL01CMEdBMVVkRGdRV0JCUlkKd0I1ZmtVV2xacWw2ekpDaGt5TFFLc1hGK2pBZkJnTlZIU01FR0RBV2dCUll3QjVma1VXbFpxbDZ6SkNoa3lMUQpLc1hGK2pBS0JnZ3Foa2pPUFFRREF3TnBBREJtQWpFQWoxbkhlWFpwKzEzTldCTmErRURzRFA4RzFXV2cxdENNCldQL1dIUHFwYVZvMGpoc3dlTkZaZ1NzMGVFN3dZSTRxQWpFQTJXQjlvdDk4c0lrb0YzdlpZZGQzL1Z0V0I1YjkKVE5NZWE3SXgvc3RKNVRmY0xMZUFCTEU0Qk5KT3NRNHZuQkhKCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0=&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; },&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;rekorPublicKeyData&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;LS0tLS1CRUdJTiBQVUJMSUMgS0VZLS0tLS0KTUZrd0V3WUhLb1pJemowQ0FRWUlLb1pJemowREFRY0RRZ0FFMkcyWSsydGFiZFRWNUJjR2lCSXgwYTlmQUZ3cgprQmJtTFNHdGtzNEwzcVg2eVlZMHp1ZkJuaEM4VXIvaXk1NUdoV1AvOUEvYlkyTGhDMzBNOStSWXR3PT0KLS0tLS1FTkQgUFVCTElDIEtFWS0tLS0tCg==&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; ]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;CRI-O has to be started to use that policy as the global source of truth:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; sudo crio --log-level debug --signature-policy ./policy.json&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;CRI-O is now able to pull the image while verifying its signatures. This can be&#xA;done by using &lt;a href=&#34;https://github.com/kubernetes-sigs/cri-tools&#34;&gt;&lt;code&gt;crictl&lt;/code&gt; (cri-tools)&lt;/a&gt;, for example:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; sudo crictl -D pull quay.io/crio/signed&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;DEBU[…] get image connection&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;DEBU[…] PullImageRequest: &amp;amp;PullImageRequest{Image:&amp;amp;ImageSpec{Image:quay.io/crio/signed,Annotations:map[string]string{},},Auth:nil,SandboxConfig:nil,}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;DEBU[…] PullImageResponse: &amp;amp;PullImageResponse{ImageRef:quay.io/crio/signed@sha256:18b42e8ea347780f35d979a829affa178593a8e31d90644466396e1187a07f3a,}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;Image is up to date for quay.io/crio/signed@sha256:18b42e8ea347780f35d979a829affa178593a8e31d90644466396e1187a07f3a&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The CRI-O debug logs will also indicate that the signature got successfully&#xA;validated:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;DEBU[…] IsRunningImageAllowed for image docker:quay.io/crio/signed:latest&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;DEBU[…] Using transport &amp;#34;docker&amp;#34; specific policy section quay.io/crio/signed&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;DEBU[…] Reading /var/lib/containers/sigstore/crio/signed@sha256=18b42e8ea347780f35d979a829affa178593a8e31d90644466396e1187a07f3a/signature-1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;DEBU[…] Looking for sigstore attachments in quay.io/crio/signed:sha256-18b42e8ea347780f35d979a829affa178593a8e31d90644466396e1187a07f3a.sig&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;DEBU[…] GET https://quay.io/v2/crio/signed/manifests/sha256-18b42e8ea347780f35d979a829affa178593a8e31d90644466396e1187a07f3a.sig&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;DEBU[…] Content-Type from manifest GET is &amp;#34;application/vnd.oci.image.manifest.v1+json&amp;#34;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;DEBU[…] Found a sigstore attachment manifest with 1 layers&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;DEBU[…] Fetching sigstore attachment 1/1: sha256:8276724a208087e73ae5d9d6e8f872f67808c08b0acdfdc73019278807197c45&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;DEBU[…] Downloading /v2/crio/signed/blobs/sha256:8276724a208087e73ae5d9d6e8f872f67808c08b0acdfdc73019278807197c45&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;DEBU[…] GET https://quay.io/v2/crio/signed/blobs/sha256:8276724a208087e73ae5d9d6e8f872f67808c08b0acdfdc73019278807197c45&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;DEBU[…] Requirement 0: allowed&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;DEBU[…] Overall: allowed&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;All of the defined fields like &lt;code&gt;oidcIssuer&lt;/code&gt; and &lt;code&gt;subjectEmail&lt;/code&gt; in the policy&#xA;have to match, while &lt;code&gt;fulcio.caData&lt;/code&gt; and &lt;code&gt;rekorPublicKeyData&lt;/code&gt; are the public&#xA;keys from the upstream &lt;a href=&#34;https://github.com/sigstore/fulcio&#34;&gt;fulcio (OIDC PKI)&lt;/a&gt; and &lt;a href=&#34;https://github.com/sigstore/rekor&#34;&gt;rekor&#xA;(transparency log)&lt;/a&gt; instances.&lt;/p&gt;&#xA;&lt;p&gt;This means that if you now invalidate the &lt;code&gt;subjectEmail&lt;/code&gt; of the policy, for example to&#xA;&lt;code&gt;wrong@mail.com&lt;/code&gt;:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; jq &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;.transports.docker.&amp;#34;quay.io/crio/signed&amp;#34;[0].fulcio.subjectEmail = &amp;#34;wrong@mail.com&amp;#34;&amp;#39;&lt;/span&gt; policy.json &amp;gt; new-policy.json&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; mv new-policy.json policy.json&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then remove the image, since it already exists locally:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; sudo crictl rmi quay.io/crio/signed&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now when you pull the image, CRI-O complains that the required email is wrong:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; sudo crictl pull quay.io/crio/signed&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;FATA[…] pulling image: rpc error: code = Unknown desc = Source image rejected: Required email wrong@mail.com not found (got []string{&amp;#34;sgrunert@redhat.com&amp;#34;})&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It is also possible to test an unsigned image against the policy. For that you&#xA;have to modify the key &lt;code&gt;quay.io/crio/signed&lt;/code&gt; to something like&#xA;&lt;code&gt;quay.io/crio/unsigned&lt;/code&gt;:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; sed -i &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;s;quay.io/crio/signed;quay.io/crio/unsigned;&amp;#39;&lt;/span&gt; policy.json&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you now pull the container image, CRI-O will complain that no signature exists&#xA;for it:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; sudo crictl pull quay.io/crio/unsigned&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;FATA[…] pulling image: rpc error: code = Unknown desc = SignatureValidationFailed: Source image rejected: A signature was required, but no signature exists&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It is important to mention that CRI-O will match the&#xA;&lt;code&gt;.critical.identity.docker-reference&lt;/code&gt; field within the signature to match with&#xA;the image repository. For example, if you verify the image&#xA;&lt;code&gt;registry.k8s.io/kube-apiserver-amd64:v1.28.0-alpha.3&lt;/code&gt;, then the corresponding&#xA;&lt;code&gt;docker-reference&lt;/code&gt; should be &lt;code&gt;registry.k8s.io/kube-apiserver-amd64&lt;/code&gt;:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; cosign verify registry.k8s.io/kube-apiserver-amd64:v1.28.0-alpha.3 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#888&#34;&gt; --certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com \&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt; --certificate-oidc-issuer https://accounts.google.com \&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt; | jq -r &amp;#39;.[0].critical.identity.&amp;#34;docker-reference&amp;#34;&amp;#39;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;…&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#888&#34;&gt;registry.k8s.io/kubernetes/kube-apiserver-amd64&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The Kubernetes community introduced &lt;code&gt;registry.k8s.io&lt;/code&gt; as proxy mirror for&#xA;various registries. Before the release of &lt;a href=&#34;https://github.com/kubernetes-sigs/promo-tools/releases/tag/v4.0.2&#34;&gt;kpromo v4.0.2&lt;/a&gt;, images&#xA;had been signed with the actual mirror rather than &lt;code&gt;registry.k8s.io&lt;/code&gt;:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; cosign verify registry.k8s.io/kube-apiserver-amd64:v1.28.0-alpha.2 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#888&#34;&gt; --certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com \&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt; --certificate-oidc-issuer https://accounts.google.com \&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt; | jq -r &amp;#39;.[0].critical.identity.&amp;#34;docker-reference&amp;#34;&amp;#39;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;…&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#888&#34;&gt;asia-northeast2-docker.pkg.dev/k8s-artifacts-prod/images/kubernetes/kube-apiserver-amd64&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The change of the &lt;code&gt;docker-reference&lt;/code&gt; to &lt;code&gt;registry.k8s.io&lt;/code&gt; makes it easier for&#xA;end users to validate the signatures, because they cannot know anything about the&#xA;underlying infrastructure being used. The feature to set the identity on image&#xA;signing has been added to &lt;a href=&#34;https://github.com/sigstore/cosign/pull/2984&#34;&gt;cosign&lt;/a&gt; via the flag &lt;code&gt;sign --sign-container-identity&lt;/code&gt; as well and will be part of its upcoming release.&lt;/p&gt;&#xA;&lt;p&gt;The Kubernetes image pull error code &lt;code&gt;SignatureValidationFailed&lt;/code&gt; got &lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/117717&#34;&gt;recently added to&#xA;Kubernetes&lt;/a&gt; and will be available from v1.28. This error code allows&#xA;end-users to understand image pull failures directly from the kubectl CLI. For&#xA;example, if you run CRI-O together with Kubernetes using the policy which requires&#xA;&lt;code&gt;quay.io/crio/unsigned&lt;/code&gt; to be signed, then a pod definition like this:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;apiVersion&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;kind&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Pod&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;metadata&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;pod&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;spec&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;containers&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;- &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;name&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;container&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;image&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;quay.io/crio/unsigned&lt;span style=&#34;color:#bbb&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Will cause the &lt;code&gt;SignatureValidationFailed&lt;/code&gt; error when applying the pod manifest:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; kubectl apply -f pod.yaml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;pod/pod created&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; kubectl get pods&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;NAME READY STATUS RESTARTS AGE&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt;pod 0/1 SignatureValidationFailed 0 4s&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#000080;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; kubectl describe pod pod | tail -n8&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt; Type Reason Age From Message&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt; ---- ------ ---- ---- -------&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt; Normal Scheduled 58s default-scheduler Successfully assigned default/pod to 127.0.0.1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt; Normal BackOff 22s (x2 over 55s) kubelet Back-off pulling image &amp;#34;quay.io/crio/unsigned&amp;#34;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt; Warning Failed 22s (x2 over 55s) kubelet Error: ImagePullBackOff&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt; Normal Pulling 9s (x3 over 58s) kubelet Pulling image &amp;#34;quay.io/crio/unsigned&amp;#34;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt; Warning Failed 6s (x3 over 55s) kubelet Failed to pull image &amp;#34;quay.io/crio/unsigned&amp;#34;: SignatureValidationFailed: Source image rejected: A signature was required, but no signature exists&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#888&#34;&gt; Warning Failed 6s (x3 over 55s) kubelet Error: SignatureValidationFailed&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This overall behavior provides a more Kubernetes native experience and does not&#xA;rely on third party software to be installed in the cluster.&lt;/p&gt;&#xA;&lt;p&gt;There are still a few corner cases to consider: For example, what if you want to&#xA;allow policies per namespace in the same way the policy-controller supports it?&#xA;Well, there is an upcoming CRI-O feature in v1.28 for that! CRI-O will support&#xA;the &lt;code&gt;--signature-policy-dir&lt;/code&gt; / &lt;code&gt;signature_policy_dir&lt;/code&gt; option, which defines the&#xA;root path for pod namespace-separated signature policies. This means that CRI-O&#xA;will lookup that path and assemble a policy like &lt;code&gt;&amp;lt;SIGNATURE_POLICY_DIR&amp;gt;/&amp;lt;NAMESPACE&amp;gt;.json&lt;/code&gt;,&#xA;which will be used on image pull if existing. If no pod namespace is&#xA;provided on image pull (&lt;a href=&#34;https://github.com/kubernetes/cri-api/blob/e5515a5/pkg/apis/runtime/v1/api.proto#L1448&#34;&gt;via the sandbox config&lt;/a&gt;), or the&#xA;concatenated path is non-existent, then CRI-O&#39;s global policy will be used as&#xA;fallback.&lt;/p&gt;&#xA;&lt;p&gt;Another corner case to consider is critical for the correct signature&#xA;verification within container runtimes: The kubelet only invokes container image&#xA;pulls if the image does not already exist on disk. This means that an&#xA;unrestricted policy from Kubernetes namespace A can allow pulling an image,&#xA;while namespace B is not able to enforce the policy because it already exits on&#xA;the node. Finally, CRI-O has to verify the policy not only on image pull, but&#xA;also on container creation. This fact makes things even a bit more complicated,&#xA;because the CRI does not really pass down the user specified image reference on&#xA;container creation, but an already resolved image ID, or digest. A &lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/118652&#34;&gt;small&#xA;change to the CRI&lt;/a&gt; can help with that.&lt;/p&gt;&#xA;&lt;p&gt;Now that everything happens within the container runtime, someone has to&#xA;maintain and define the policies to provide a good user experience around that&#xA;feature. The CRDs of the policy-controller are great, while we could imagine that&#xA;a daemon within the cluster can write the policies for CRI-O per namespace. This&#xA;would make any additional hook obsolete and moves the responsibility of&#xA;verifying the image signature to the actual instance which pulls the image. &lt;a href=&#34;https://groups.google.com/g/kubernetes-sig-node/c/kgpxqcsJ7Vc/m/7X7t_ElsAgAJ&#34;&gt;I&#xA;evaluated&lt;/a&gt; other possible paths toward a better container image&#xA;signature verification within plain Kubernetes, but I could not find a great fit&#xA;for a native API. This means that I believe that a CRD is the way to go, but&#xA;users still need an instance which actually serves it.&lt;/p&gt;&#xA;&lt;p&gt;Thank you for reading this blog post! If you&#39;re interested in more, providing&#xA;feedback or asking for help, then feel free to get in touch with me directly via&#xA;&lt;a href=&#34;https://kubernetes.slack.com/messages/crio&#34;&gt;Slack (#crio)&lt;/a&gt; or the &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-sig-node&#34;&gt;SIG Node mailing list&lt;/a&gt;.&lt;/p&gt;</description>
      <guid>https://kubernetes.io/blog/2023/06/29/container-image-signature-verification/</guid>
      <pubDate>Thu, 29 Jun 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Confidential Kubernetes: Use Confidential Virtual Machines and Enclaves to improve your cluster security</title>
      <link>https://kubernetes.io/blog/2023/07/06/confidential-kubernetes/</link>
      <description>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fabian Kammel (Edgeless Systems), Mikko Ylinen (Intel), Tobin Feldman-Fitzthum (IBM)&lt;/p&gt;&#xA;&lt;p&gt;In this blog post, we will introduce the concept of Confidential Computing (CC) to improve any computing environment&#39;s security and privacy properties. Further, we will show how&#xA;the Cloud-Native ecosystem, particularly Kubernetes, can benefit from the new compute paradigm.&lt;/p&gt;&#xA;&lt;p&gt;Confidential Computing is a concept that has been introduced previously in the cloud-native world. The&#xA;&lt;a href=&#34;https://confidentialcomputing.io/&#34;&gt;Confidential Computing Consortium&lt;/a&gt; (CCC) is a project community in the Linux Foundation&#xA;that already worked on&#xA;&lt;a href=&#34;https://confidentialcomputing.io/wp-content/uploads/sites/85/2019/12/CCC_Overview.pdf&#34;&gt;Defining and Enabling Confidential Computing&lt;/a&gt;.&#xA;In the &lt;a href=&#34;https://confidentialcomputing.io/wp-content/uploads/sites/85/2023/01/CCC-A-Technical-Analysis-of-Confidential-Computing-v1.3_Updated_November_2022.pdf&#34;&gt;Whitepaper&lt;/a&gt;,&#xA;they provide a great motivation for the use of Confidential Computing:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Data exists in three states: in transit, at rest, and in use. …Protecting sensitive data&#xA;in all of its states is more critical than ever. Cryptography is now commonly deployed&#xA;to provide both data confidentiality (stopping unauthorized viewing) and data integrity&#xA;(preventing or detecting unauthorized changes). While techniques to protect data in transit&#xA;and at rest are now commonly deployed, the third state - protecting data in use - is the new frontier.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;Confidential Computing aims to primarily solve the problem of &lt;strong&gt;protecting data in use&lt;/strong&gt;&#xA;by introducing a hardware-enforced Trusted Execution Environment (TEE).&lt;/p&gt;&#xA;&lt;h2 id=&#34;trusted-execution-environments&#34;&gt;Trusted Execution Environments&lt;/h2&gt;&#xA;&lt;p&gt;For more than a decade, Trusted Execution Environments (TEEs) have been available in commercial&#xA;computing hardware in the form of &lt;a href=&#34;https://en.wikipedia.org/wiki/Hardware_security_module&#34;&gt;Hardware Security Modules&lt;/a&gt;&#xA;(HSMs) and &lt;a href=&#34;https://www.iso.org/standard/50970.html&#34;&gt;Trusted Platform Modules&lt;/a&gt; (TPMs). These&#xA;technologies provide trusted environments for shielded computations. They can&#xA;store highly sensitive cryptographic keys and carry out critical cryptographic operations&#xA;such as signing or encrypting data.&lt;/p&gt;&#xA;&lt;p&gt;TPMs are optimized for low cost, allowing them to be integrated into mainboards and act as a&#xA;system&#39;s physical root of trust. To keep the cost low, TPMs are limited in scope, i.e., they&#xA;provide storage for only a few keys and are capable of just a small subset of cryptographic operations.&lt;/p&gt;&#xA;&lt;p&gt;In contrast, HSMs are optimized for high performance, providing secure storage for far&#xA;more keys and offering advanced physical attack detection mechanisms. Additionally, high-end HSMs&#xA;can be programmed so that arbitrary code can be compiled and executed. The downside&#xA;is that they are very costly. A managed CloudHSM from AWS costs&#xA;&lt;a href=&#34;https://aws.amazon.com/cloudhsm/pricing/&#34;&gt;around $1.50 / hour&lt;/a&gt; or ~$13,500 / year.&lt;/p&gt;&#xA;&lt;p&gt;In recent years, a new kind of TEE has gained popularity. Technologies like&#xA;&lt;a href=&#34;https://developer.amd.com/sev/&#34;&gt;AMD SEV&lt;/a&gt;,&#xA;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/tools/software-guard-extensions/overview.html&#34;&gt;Intel SGX&lt;/a&gt;,&#xA;and &lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/articles/technical/intel-trust-domain-extensions.html&#34;&gt;Intel TDX&lt;/a&gt;&#xA;provide TEEs that are closely integrated with userspace. Rather than low-power or high-performance&#xA;devices that support specific use cases, these TEEs shield normal processes or virtual machines&#xA;and can do so with relatively low overhead. These technologies each have different design goals,&#xA;advantages, and limitations, and they are available in different environments, including consumer&#xA;laptops, servers, and mobile devices.&lt;/p&gt;&#xA;&lt;p&gt;Additionally, we should mention&#xA;&lt;a href=&#34;https://www.arm.com/technologies/trustzone-for-cortex-a&#34;&gt;ARM TrustZone&lt;/a&gt;, which is optimized&#xA;for embedded devices such as smartphones, tablets, and smart TVs, as well as&#xA;&lt;a href=&#34;https://aws.amazon.com/ec2/nitro/nitro-enclaves/&#34;&gt;AWS Nitro Enclaves&lt;/a&gt;, which are only available&#xA;on &lt;a href=&#34;https://aws.amazon.com/&#34;&gt;Amazon Web Services&lt;/a&gt; and have a different threat model compared&#xA;to the CPU-based solutions by Intel and AMD.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.ibm.com/docs/en/linux-on-systems?topic=virtualization-secure-execution&#34;&gt;IBM Secure Execution for Linux&lt;/a&gt;&#xA;lets you run your Kubernetes cluster&#39;s nodes as KVM guests within a trusted execution environment on&#xA;IBM Z series hardware. You can use this hardware-enhanced virtual machine isolation to&#xA;provide strong isolation between tenants in a cluster, with hardware attestation about the (virtual) node&#39;s integrity.&lt;/p&gt;&#xA;&lt;h3 id=&#34;security-properties-and-feature-set&#34;&gt;Security properties and feature set&lt;/h3&gt;&#xA;&lt;p&gt;In the following sections, we will review the security properties and additional features&#xA;these new technologies bring to the table. Only some solutions will provide all properties;&#xA;we will discuss each technology in further detail in their respective section.&lt;/p&gt;&#xA;&lt;p&gt;The &lt;strong&gt;Confidentiality&lt;/strong&gt; property ensures that information cannot be viewed while it is&#xA;in use in the TEE. This provides us with the highly desired feature to secure&#xA;&lt;strong&gt;data in use&lt;/strong&gt;. Depending on the specific TEE used, both code and data may be protected&#xA;from outside viewers. The differences in TEE architectures and how their use&#xA;in a cloud native context are important considerations when designing end-to-end security&#xA;for sensitive workloads with a minimal &lt;strong&gt;Trusted Computing Base&lt;/strong&gt; (TCB) in mind. CCC has recently&#xA;worked on a &lt;a href=&#34;https://confidentialcomputing.io/wp-content/uploads/sites/85/2023/01/Common-Terminology-for-Confidential-Computing.pdf&#34;&gt;common vocabulary and supporting material&lt;/a&gt;&#xA;that helps to explain where confidentiality boundaries are drawn with the different TEE&#xA;architectures and how that impacts the TCB size.&lt;/p&gt;&#xA;&lt;p&gt;Confidentiality is a great feature, but an attacker can still manipulate&#xA;or inject arbitrary code and data for the TEE to execute and, therefore, easily leak critical&#xA;information. &lt;strong&gt;Integrity&lt;/strong&gt; guarantees a TEE owner that neither code nor data can be&#xA;tampered with while running critical computations.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Availability&lt;/strong&gt; is a basic property often discussed in the context of information&#xA;security. However, this property is outside the scope of most TEEs. Usually, they can be controlled&#xA;(shut down, restarted, …) by some higher level abstraction. This could be the CPU itself, the&#xA;hypervisor, or the kernel. This is to preserve the overall system&#39;s availability,&#xA;not the TEE itself. When running in the cloud, availability is usually guaranteed by&#xA;the cloud provider in terms of Service Level Agreements (SLAs) and is not cryptographically enforceable.&lt;/p&gt;&#xA;&lt;p&gt;Confidentiality and Integrity by themselves are only helpful in some cases. For example,&#xA;consider a TEE running in a remote cloud. How would you know the TEE is genuine and running&#xA;your intended software? It could be an imposter stealing your data as soon as you send it over.&#xA;This fundamental problem is addressed by &lt;strong&gt;Attestability&lt;/strong&gt;. Attestation allows us to verify&#xA;the identity, confidentiality, and integrity of TEEs based on cryptographic certificates issued&#xA;from the hardware itself. This feature can also be made available to clients outside of the&#xA;confidential computing hardware in the form of remote attestation.&lt;/p&gt;&#xA;&lt;p&gt;TEEs can hold and process information that predates or outlives the trusted environment. That&#xA;could mean across restarts, different versions, or platform migrations. Therefore &lt;strong&gt;Recoverability&lt;/strong&gt;&#xA;is an important feature. Data and the state of a TEE need to be sealed before they are written&#xA;to persistent storage to maintain confidentiality and integrity guarantees. The access to such&#xA;sealed data needs to be well-defined. In most cases, the unsealing is bound to a TEE&#39;s identity.&#xA;Hence, making sure the recovery can only happen in the same confidential context.&lt;/p&gt;&#xA;&lt;p&gt;This does not have to limit the flexibility of the overall system.&#xA;&lt;a href=&#34;https://www.amd.com/system/files/TechDocs/SEV-SNP-strengthening-vm-isolation-with-integrity-protection-and-more.pdf&#34;&gt;AMD SEV-SNP&#39;s migration agent (MA)&lt;/a&gt;&#xA;allows users to migrate a confidential virtual machine to a different host system&#xA;while keeping the security properties of the TEE intact.&lt;/p&gt;&#xA;&lt;h2 id=&#34;feature-comparison&#34;&gt;Feature comparison&lt;/h2&gt;&#xA;&lt;p&gt;These sections of the article will dive a little bit deeper into the specific implementations,&#xA;compare supported features and analyze their security properties.&lt;/p&gt;&#xA;&lt;h3 id=&#34;amd-sev&#34;&gt;AMD SEV&lt;/h3&gt;&#xA;&lt;p&gt;AMD&#39;s &lt;a href=&#34;https://developer.amd.com/sev/&#34;&gt;Secure Encrypted Virtualization (SEV)&lt;/a&gt; technologies&#xA;are a set of features to enhance the security of virtual machines on AMD&#39;s server CPUs. SEV&#xA;transparently encrypts the memory of each VM with a unique key. SEV can also calculate a&#xA;signature of the memory contents, which can be sent to the VM&#39;s owner as an attestation that&#xA;the initial guest memory was not manipulated.&lt;/p&gt;&#xA;&lt;p&gt;The second generation of SEV, known as&#xA;&lt;a href=&#34;https://www.amd.com/content/dam/amd/en/documents/epyc-business-docs/white-papers/Protecting-VM-Register-State-with-SEV-ES.pdf&#34;&gt;Encrypted State&lt;/a&gt;&#xA;or SEV-ES, provides additional protection from the hypervisor by encrypting all&#xA;CPU register contents when a context switch occurs.&lt;/p&gt;&#xA;&lt;p&gt;The third generation of SEV,&#xA;&lt;a href=&#34;https://www.amd.com/system/files/TechDocs/SEV-SNP-strengthening-vm-isolation-with-integrity-protection-and-more.pdf&#34;&gt;Secure Nested Paging&lt;/a&gt;&#xA;or SEV-SNP, is designed to prevent software-based integrity attacks and reduce the risk associated with&#xA;compromised memory integrity. The basic principle of SEV-SNP integrity is that if a VM can read&#xA;a private (encrypted) memory page, it must always read the value it last wrote.&lt;/p&gt;&#xA;&lt;p&gt;Additionally, by allowing the guest to obtain remote attestation statements dynamically,&#xA;SNP enhances the remote attestation capabilities of SEV.&lt;/p&gt;&#xA;&lt;p&gt;AMD SEV has been implemented incrementally. New features and improvements have been added with&#xA;each new CPU generation. The Linux community makes these features available as part of the KVM hypervisor&#xA;and for host and guest kernels. The first SEV features were discussed and implemented in 2016 - see&#xA;&lt;a href=&#34;https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/kaplan&#34;&gt;AMD x86 Memory Encryption Technologies&lt;/a&gt;&#xA;from the 2016 Usenix Security Symposium. The latest big addition was&#xA;&lt;a href=&#34;https://www.phoronix.com/news/AMD-SEV-SNP-Arrives-Linux-5.19&#34;&gt;SEV-SNP guest support in Linux 5.19&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/updates/azureconfidentialvm/&#34;&gt;Confidential VMs based on AMD SEV-SNP&lt;/a&gt;&#xA;are available in Microsoft Azure since July 2022. Similarly, Google Cloud Platform (GCP) offers&#xA;&lt;a href=&#34;https://cloud.google.com/compute/confidential-vm/docs/about-cvm&#34;&gt;confidential VMs based on AMD SEV-ES&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h3 id=&#34;intel-sgx&#34;&gt;Intel SGX&lt;/h3&gt;&#xA;&lt;p&gt;Intel&#39;s&#xA;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/tools/software-guard-extensions/overview.html&#34;&gt;Software Guard Extensions&lt;/a&gt;&#xA;has been available since 2015 and were introduced with the Skylake architecture.&lt;/p&gt;&#xA;&lt;p&gt;SGX is an instruction set that enables users to create a protected and isolated process called&#xA;an &lt;em&gt;enclave&lt;/em&gt;. It provides a reverse sandbox that protects enclaves from the operating system,&#xA;firmware, and any other privileged execution context.&lt;/p&gt;&#xA;&lt;p&gt;The enclave memory cannot be read or written from outside the enclave, regardless of&#xA;the current privilege level and CPU mode. The only way to call an enclave function is&#xA;through a new instruction that performs several protection checks. Its memory is encrypted.&#xA;Tapping the memory or connecting the DRAM modules to another system will yield only encrypted&#xA;data. The memory encryption key randomly changes every power cycle. The key is stored&#xA;within the CPU and is not accessible.&lt;/p&gt;&#xA;&lt;p&gt;Since the enclaves are process isolated, the operating system&#39;s libraries are not usable as is;&#xA;therefore, SGX enclave SDKs are required to compile programs for SGX. This also implies applications&#xA;need to be designed and implemented to consider the trusted/untrusted isolation boundaries.&#xA;On the other hand, applications get built with very minimal TCB.&lt;/p&gt;&#xA;&lt;p&gt;An emerging approach to easily transition to process-based confidential computing&#xA;and avoid the need to build custom applications is to utilize library OSes. These OSes&#xA;facilitate running native, unmodified Linux applications inside SGX enclaves.&#xA;A library OS intercepts all application requests to the host OS and processes them securely&#xA;without the application knowing it&#39;s running a TEE.&lt;/p&gt;&#xA;&lt;p&gt;The 3rd generation Xeon CPUs (aka Ice Lake Server - &amp;quot;ICX&amp;quot;) and later generations did switch to using a technology called&#xA;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/articles/news/runtime-encryption-of-memory-with-intel-tme-mk.html&#34;&gt;Total Memory Encryption - Multi-Key&lt;/a&gt;&#xA;(TME-MK) that uses AES-XTS, moving away from the&#xA;&lt;a href=&#34;https://eprint.iacr.org/2016/204.pdf&#34;&gt;Memory Encryption Engine&lt;/a&gt;&#xA;that the consumer and Xeon E CPUs used. This increased the possible&#xA;&lt;a href=&#34;https://sgx101.gitbook.io/sgx101/sgx-bootstrap/enclave#enclave-page-cache-epc&#34;&gt;enclave page cache&lt;/a&gt;&#xA;(EPC) size (up to 512GB/CPU) and improved performance. More info&#xA;about SGX on multi-socket platforms can be found in the&#xA;&lt;a href=&#34;https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/supporting-intel-sgx-on-mulit-socket-platforms.pdf&#34;&gt;Whitepaper&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;A &lt;a href=&#34;https://ark.intel.com/content/www/us/en/ark/search/featurefilter.html?productType=873&#34;&gt;list of supported platforms&lt;/a&gt;&#xA;is available from Intel.&lt;/p&gt;&#xA;&lt;p&gt;SGX is available on&#xA;&lt;a href=&#34;https://azure.microsoft.com/de-de/updates/intel-sgx-based-confidential-computing-vms-now-available-on-azure-dedicated-hosts/&#34;&gt;Azure&lt;/a&gt;,&#xA;&lt;a href=&#34;https://www.alibabacloud.com/help/en/elastic-compute-service/latest/build-an-sgx-encrypted-computing-environment&#34;&gt;Alibaba Cloud&lt;/a&gt;,&#xA;&lt;a href=&#34;https://cloud.ibm.com/docs/bare-metal?topic=bare-metal-bm-server-provision-sgx&#34;&gt;IBM&lt;/a&gt;, and many more.&lt;/p&gt;&#xA;&lt;h3 id=&#34;intel-tdx&#34;&gt;Intel TDX&lt;/h3&gt;&#xA;&lt;p&gt;Where Intel SGX aims to protect the context of a single process,&#xA;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/articles/technical/intel-trust-domain-extensions.html&#34;&gt;Intel&#39;s Trusted Domain Extensions&lt;/a&gt;&#xA;protect a full virtual machine and are, therefore, most closely comparable to AMD SEV.&lt;/p&gt;&#xA;&lt;p&gt;As with SEV-SNP, guest support for TDX was &lt;a href=&#34;https://www.phoronix.com/news/Intel-TDX-For-Linux-5.19&#34;&gt;merged in Linux Kernel 5.19&lt;/a&gt;.&#xA;However, hardware support will land with &lt;a href=&#34;https://en.wikipedia.org/wiki/Sapphire_Rapids&#34;&gt;Sapphire Rapids&lt;/a&gt; during 2023:&#xA;&lt;a href=&#34;https://www.alibabacloud.com/help/en/elastic-compute-service/latest/build-a-tdx-confidential-computing-environment&#34;&gt;Alibaba Cloud provides&lt;/a&gt;&#xA;invitational preview instances, and&#xA;&lt;a href=&#34;https://techcommunity.microsoft.com/t5/azure-confidential-computing/preview-introducing-dcesv5-and-ecesv5-series-confidential-vms/ba-p/3800718&#34;&gt;Azure has announced&lt;/a&gt;&#xA;its TDX preview opportunity.&lt;/p&gt;&#xA;&lt;h2 id=&#34;overhead-analysis&#34;&gt;Overhead analysis&lt;/h2&gt;&#xA;&lt;p&gt;The benefits that Confidential Computing technologies provide via strong isolation and enhanced&#xA;security to customer data and workloads are not for free. Quantifying this impact is challenging and&#xA;depends on many factors: The TEE technology, the benchmark, the metrics, and the type of workload&#xA;all have a huge impact on the expected performance overhead.&lt;/p&gt;&#xA;&lt;p&gt;Intel SGX-based TEEs are hard to benchmark, as &lt;a href=&#34;https://arxiv.org/pdf/2205.06415.pdf&#34;&gt;shown&lt;/a&gt;&#xA;&lt;a href=&#34;https://www.ibr.cs.tu-bs.de/users/mahhouk/papers/eurosec2021.pdf&#34;&gt;by&lt;/a&gt;&#xA;&lt;a href=&#34;https://dl.acm.org/doi/fullHtml/10.1145/3533737.3535098&#34;&gt;different papers&lt;/a&gt;. The chosen SDK/library&#xA;OS, the application itself, as well as the resource requirements (especially large memory requirements)&#xA;have a huge impact on performance. A single-digit percentage overhead can be expected if an application&#xA;is well suited to run inside an enclave.&lt;/p&gt;&#xA;&lt;p&gt;Confidential virtual machines based on AMD SEV-SNP require no changes to the executed program&#xA;and operating system and are a lot easier to benchmark. A&#xA;&lt;a href=&#34;https://community.amd.com/t5/business/microsoft-azure-confidential-computing-powered-by-3rd-gen-epyc/ba-p/497796&#34;&gt;benchmark from Azure and AMD&lt;/a&gt;&#xA;shows that SEV-SNP VM overhead is &amp;lt;10%, sometimes as low as 2%.&lt;/p&gt;&#xA;&lt;p&gt;Although there is a performance overhead, it should be low enough to enable real-world workloads&#xA;to run in these protected environments and improve the security and privacy of our data.&lt;/p&gt;&#xA;&lt;h2 id=&#34;confidential-computing-compared-to-fhe-zkp-and-mpc&#34;&gt;Confidential Computing compared to FHE, ZKP, and MPC&lt;/h2&gt;&#xA;&lt;p&gt;Fully Homomorphic Encryption (FHE), Zero Knowledge Proof/Protocol (ZKP), and Multi-Party&#xA;Computations (MPC) are all a form of encryption or cryptographic protocols that offer&#xA;similar security guarantees to Confidential Computing but do not require hardware support.&lt;/p&gt;&#xA;&lt;p&gt;Fully (also partially and somewhat) homomorphic encryption allows one to perform&#xA;computations, such as addition or multiplication, on encrypted data. This provides&#xA;the property of encryption in use but does not provide integrity protection or attestation&#xA;like confidential computing does. Therefore, these two technologies can &lt;a href=&#34;https://confidentialcomputing.io/2023/03/29/confidential-computing-and-homomorphic-encryption/&#34;&gt;complement to each other&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Zero Knowledge Proofs or Protocols are a privacy-preserving technique (PPT) that&#xA;allows one party to prove facts about their data without revealing anything else about&#xA;the data. ZKP can be used instead of or in addition to Confidential Computing to protect&#xA;the privacy of the involved parties and their data. Similarly, Multi-Party Computation&#xA;enables multiple parties to work together on a computation, i.e., each party provides&#xA;their data to the result without leaking it to any other parties.&lt;/p&gt;&#xA;&lt;h2 id=&#34;use-cases-of-confidential-computing&#34;&gt;Use cases of Confidential Computing&lt;/h2&gt;&#xA;&lt;p&gt;The presented Confidential Computing platforms show that both the isolation of a single container&#xA;process and, therefore, minimization of the trusted computing base and the isolation of a&#xA;``&#xA;full virtual machine are possible. This has already enabled a lot of interesting and secure&#xA;projects to emerge:&lt;/p&gt;&#xA;&lt;h3 id=&#34;confidential-containers&#34;&gt;Confidential Containers&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/confidential-containers&#34;&gt;Confidential Containers&lt;/a&gt; (CoCo) is a&#xA;CNCF sandbox project that isolates Kubernetes pods inside of confidential virtual machines.&lt;/p&gt;&#xA;&lt;p&gt;CoCo can be installed on a Kubernetes cluster with an operator.&#xA;The operator will create a set of runtime classes that can be used to deploy&#xA;pods inside an enclave on several different platforms, including&#xA;AMD SEV, Intel TDX, Secure Execution for IBM Z, and Intel SGX.&lt;/p&gt;&#xA;&lt;p&gt;CoCo is typically used with signed and/or encrypted container images&#xA;which are pulled, verified, and decrypted inside the enclave.&#xA;Secrets, such as image decryption keys, are conditionally provisioned&#xA;to the enclave by a trusted Key Broker Service that validates the&#xA;hardware evidence of the TEE prior to releasing any sensitive information.&lt;/p&gt;&#xA;&lt;p&gt;CoCo has several deployment models. Since the Kubernetes control plane&#xA;is outside the TCB, CoCo is suitable for managed environments. CoCo can&#xA;be run in virtual environments that don&#39;t support nesting with the help of an&#xA;API adaptor that starts pod VMs in the cloud. CoCo can also be run on&#xA;bare metal, providing strong isolation even in multi-tenant environments.&lt;/p&gt;&#xA;&lt;h3 id=&#34;managed-confidential-kubernetes&#34;&gt;Managed confidential Kubernetes&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/confidential-computing/confidential-node-pool-aks&#34;&gt;Azure&lt;/a&gt; and&#xA;&lt;a href=&#34;https://cloud.google.com/blog/products/identity-security/announcing-general-availability-of-confidential-gke-nodes&#34;&gt;GCP&lt;/a&gt;&#xA;both support the use of confidential virtual machines as worker nodes for their managed Kubernetes offerings.&lt;/p&gt;&#xA;&lt;p&gt;Both services aim for better workload protection and security guarantees by enabling memory encryption&#xA;for container workloads. However, they don&#39;t seek to fully isolate the cluster or workloads against&#xA;the service provider or infrastructure. Specifically, they don&#39;t offer a dedicated confidential control&#xA;plane or expose attestation capabilities for the confidential cluster/nodes.&lt;/p&gt;&#xA;&lt;p&gt;Azure also enables&#xA;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/confidential-computing/confidential-nodes-aks-overview&#34;&gt;Confidential Containers&lt;/a&gt;&#xA;in their managed Kubernetes offering. They support the creation based on&#xA;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/confidential-computing/confidential-containers-enclaves&#34;&gt;Intel SGX enclaves&lt;/a&gt;&#xA;and &lt;a href=&#34;https://techcommunity.microsoft.com/t5/azure-confidential-computing/microsoft-introduces-preview-of-confidential-containers-on-azure/ba-p/3410394&#34;&gt;AMD SEV-based VMs&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h3 id=&#34;constellation&#34;&gt;Constellation&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/edgelesssys/constellation&#34;&gt;Constellation&lt;/a&gt; is a Kubernetes engine that aims to&#xA;provide the best possible data security. Constellation wraps your entire Kubernetes cluster into&#xA;a single confidential context that is shielded from the underlying cloud infrastructure. Everything&#xA;inside is always encrypted, including at runtime in memory. It shields both the worker and control&#xA;plane nodes. In addition, it already integrates with popular CNCF software such as Cilium for&#xA;secure networking and provides extended CSI drivers to write data securely.&lt;/p&gt;&#xA;&lt;h3 id=&#34;occlum-and-gramine&#34;&gt;Occlum and Gramine&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://occlum.io/&#34;&gt;Occlum&lt;/a&gt; and &lt;a href=&#34;https://gramineproject.io/&#34;&gt;Gramine&lt;/a&gt; are examples of open source&#xA;library OS projects that can be used to run unmodified applications in SGX enclaves. They&#xA;are member projects under the CCC, but similar projects and products maintained by companies&#xA;also exist. With these libOS projects, existing containerized applications can be&#xA;easily converted into confidential computing enabled containers. Many curated prebuilt&#xA;containers are also available.&lt;/p&gt;&#xA;&lt;h2 id=&#34;where-are-we-today-vendors-limitations-and-foss-landscape&#34;&gt;Where are we today? Vendors, limitations, and FOSS landscape&lt;/h2&gt;&#xA;&lt;p&gt;As we hope you have seen from the previous sections, Confidential Computing is a powerful new concept&#xA;to improve security, but we are still in the (early) adoption phase. New products are&#xA;starting to emerge to take advantage of the unique properties.&lt;/p&gt;&#xA;&lt;p&gt;Google and Microsoft are the first major cloud providers to have confidential offerings that&#xA;can run unmodified applications inside a protected boundary.&#xA;Still, these offerings are limited to compute, while end-to-end solutions for confidential&#xA;databases, cluster networking, and load balancers have to be self-managed.&lt;/p&gt;&#xA;&lt;p&gt;These technologies provide opportunities to bring even the most&#xA;sensitive workloads into the cloud and enables them to leverage all the&#xA;tools in the CNCF landscape.&lt;/p&gt;&#xA;&lt;h2 id=&#34;call-to-action&#34;&gt;Call to action&lt;/h2&gt;&#xA;&lt;p&gt;If you are currently working on a high-security product that struggles to run in the&#xA;public cloud due to legal requirements or are looking to bring the privacy and security&#xA;of your cloud-native project to the next level: Reach out to all the great projects&#xA;we have highlighted! Everyone is keen to improve the security of our ecosystem, and you can&#xA;play a vital role in that journey.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/confidential-containers&#34;&gt;Confidential Containers&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/edgelesssys/constellation&#34;&gt;Constellation: Always Encrypted Kubernetes&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://occlum.io/&#34;&gt;Occlum&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://gramineproject.io/&#34;&gt;Gramine&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;CCC also maintains a &lt;a href=&#34;https://confidentialcomputing.io/projects/&#34;&gt;list of projects&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
      <guid>https://kubernetes.io/blog/2023/07/06/confidential-kubernetes/</guid>
      <pubDate>Thu, 06 Jul 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Spotlight on SIG CLI</title>
      <link>https://kubernetes.io/blog/2023/07/20/sig-cli-spotlight-2023/</link>
      <description>&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Arpit Agrawal&lt;/p&gt;&#xA;&lt;p&gt;In the world of Kubernetes, managing containerized applications at&#xA;scale requires powerful and efficient tools. The command-line&#xA;interface (CLI) is an integral part of any developer or operator’s&#xA;toolkit, offering a convenient and flexible way to interact with a&#xA;Kubernetes cluster.&lt;/p&gt;&#xA;&lt;p&gt;SIG CLI plays a crucial role in improving the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-cli&#34;&gt;Kubernetes&#xA;CLI&lt;/a&gt;&#xA;experience by focusing on the development and enhancement of&#xA;&lt;code&gt;kubectl&lt;/code&gt;, the primary command-line tool for Kubernetes.&lt;/p&gt;&#xA;&lt;p&gt;In this SIG CLI Spotlight, Arpit Agrawal, SIG ContribEx-Comms team&#xA;member, talked with &lt;a href=&#34;https://github.com/KnVerey&#34;&gt;Katrina Verey&lt;/a&gt;, Tech&#xA;Lead &amp;amp; Chair of SIG CLI,and &lt;a href=&#34;https://github.com/soltysh&#34;&gt;Maciej&#xA;Szulik&lt;/a&gt;, SIG CLI Batch Lead, about SIG&#xA;CLI, current projects, challenges and how anyone can get involved.&lt;/p&gt;&#xA;&lt;p&gt;So, whether you are a seasoned Kubernetes enthusiast or just getting&#xA;started, understanding the significance of SIG CLI will undoubtedly&#xA;enhance your Kubernetes journey.&lt;/p&gt;&#xA;&lt;h2 id=&#34;introductions&#34;&gt;Introductions&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Arpit&lt;/strong&gt;: Could you tell us a bit about yourself, your role, and how&#xA;you got involved in SIG CLI?&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Maciej&lt;/strong&gt;: I’m one of the technical leads for SIG-CLI. I was working&#xA;on Kubernetes in multiple areas since 2014, and in 2018 I got&#xA;appointed a lead.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Katrina&lt;/strong&gt;: I’ve been working with Kubernetes as an end-user since&#xA;2016, but it was only in late 2019 that I discovered how well SIG CLI&#xA;aligned with my experience from internal projects. I started regularly&#xA;attending meetings and made a few small PRs, and by 2021 I was working&#xA;more deeply with the&#xA;&lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize&#34;&gt;Kustomize&lt;/a&gt; team&#xA;specifically. Later that year, I was appointed to my current roles as&#xA;subproject owner for Kustomize and KRM Functions, and as SIG CLI Tech&#xA;Lead and Chair.&lt;/p&gt;&#xA;&lt;h2 id=&#34;about-sig-cli&#34;&gt;About SIG CLI&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Arpit&lt;/strong&gt;: Thank you! Could you share with us the purpose and goals of SIG CLI?&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Maciej&lt;/strong&gt;: Our&#xA;&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-cli/&#34;&gt;charter&lt;/a&gt;&#xA;has the most detailed description, but in few words, we handle all CLI&#xA;tooling that helps you manage your Kubernetes manifests and interact&#xA;with your Kubernetes clusters.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Arpit&lt;/strong&gt;: I see. And how does SIG CLI work to promote best-practices&#xA;for CLI development and usage in the cloud native ecosystem?&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Maciej&lt;/strong&gt;: Within &lt;code&gt;kubectl&lt;/code&gt;, we have several on-going efforts that&#xA;try to encourage new contributors to align existing commands to new&#xA;standards. We publish several libraries which hopefully make it easier&#xA;to write CLIs that interact with Kubernetes APIs, such as cli-runtime&#xA;and&#xA;&lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/tree/master/kyaml&#34;&gt;kyaml&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Katrina&lt;/strong&gt;: We also maintain some interoperability specifications for&#xA;CLI tooling, such as the &lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/blob/master/cmd/config/docs/api-conventions/functions-spec.md&#34;&gt;KRM Functions&#xA;Specification&lt;/a&gt;&#xA;(GA) and the new ApplySet&#xA;Specification&#xA;(alpha).&lt;/p&gt;&#xA;&lt;h2 id=&#34;current-projects-and-challenges&#34;&gt;Current projects and challenges&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Arpit&lt;/strong&gt;: Going through the README file, it’s clear SIG CLI has a&#xA;number of subprojects, could you highlight some important ones?&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Maciej&lt;/strong&gt;: The four most active subprojects that are, in my opinion,&#xA;worthy of your time investment would be:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/kubectl&#34;&gt;&lt;code&gt;kubectl&lt;/code&gt;&lt;/a&gt;: the canonical Kubernetes CLI.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize&#34;&gt;Kustomize&lt;/a&gt;: a&#xA;template-free customization tool for Kubernetes yaml manifest files.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kui.tools&#34;&gt;KUI&lt;/a&gt; - a GUI interface to Kubernetes, think&#xA;&lt;code&gt;kubectl&lt;/code&gt; on steroids.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/krew&#34;&gt;&lt;code&gt;krew&lt;/code&gt;&lt;/a&gt;: a plugin manager for &lt;code&gt;kubectl&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;Arpit&lt;/strong&gt;: Are there any upcoming initiatives or developments that SIG&#xA;CLI is working on?&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Maciej&lt;/strong&gt;: There are always several initiatives we’re working on at&#xA;any given point in time. It’s best to join &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-cli/#meetings&#34;&gt;one of our&#xA;calls&lt;/a&gt;&#xA;to learn about the current ones.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Katrina&lt;/strong&gt;: For major features, you can check out &lt;a href=&#34;https://www.kubernetes.dev/resources/keps/&#34;&gt;our open&#xA;KEPs&lt;/a&gt;. For instance, in&#xA;1.27 we introduced alphas for &lt;a href=&#34;https://kubernetes.io/blog/2023/05/09/introducing-kubectl-applyset-pruning/&#34;&gt;a new pruning mode in kubectl&#xA;apply&lt;/a&gt;,&#xA;and for kubectl create plugins. Exciting ideas that are currently&#xA;under discussion include an interactive mode for &lt;code&gt;kubectl&lt;/code&gt; delete&#xA;(&lt;a href=&#34;https://kubernetes.io/blog/2023/05/09/introducing-kubectl-applyset-pruning&#34;&gt;KEP&#xA;3895&lt;/a&gt;)&#xA;and the &lt;code&gt;kuberc&lt;/code&gt; user preferences file (&lt;a href=&#34;https://kubernetes.io/blog/2023/05/09/introducing-kubectl-applyset-pruning&#34;&gt;KEP&#xA;3104&lt;/a&gt;).&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Arpit&lt;/strong&gt;: Could you discuss any challenges that SIG CLI faces in its&#xA;efforts to improve CLIs for cloud-native technologies? What are the&#xA;future efforts to solve them?&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Katrina&lt;/strong&gt;: The biggest challenge we’re facing with every decision is&#xA;backwards compatibility and ensuring we don’t break existing users. It&#xA;frequently happens that fixing what&#39;s on the surface may seem&#xA;straightforward, but even fixing a bug could constitute a breaking&#xA;change for some users, which means we need to go through an extended&#xA;deprecation process to change it, or in some cases we can’t change it&#xA;at all. Another challenge is the need to balance customization with&#xA;usability in the flag sets we expose on our tools. For example, we get&#xA;many proposals for new flags that would certainly be useful to some&#xA;users, but not a large enough subset to justify the increased&#xA;complexity having them in the tool entails for everyone. The &lt;code&gt;kuberc&lt;/code&gt;&#xA;proposal may help with some of these problems by giving individual&#xA;users the ability to set or override default values we can’t change,&#xA;and even create custom subcommands via aliases&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Arpit&lt;/strong&gt;: With every new version release of Kubernetes, maintaining&#xA;consistency and integrity is surely challenging: how does the SIG CLI&#xA;team tackle it?&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Maciej&lt;/strong&gt;: This is mostly similar to the topic mentioned in the&#xA;previous question: every new change, especially to existing commands&#xA;goes through a lot of scrutiny to ensure we don’t break existing&#xA;users. At any point in time we have to keep a reasonable balance&#xA;between features and not breaking users.&lt;/p&gt;&#xA;&lt;h2 id=&#34;future-plans-and-contribution&#34;&gt;Future plans and contribution&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Arpit&lt;/strong&gt;: How do you see the role of CLI tools in the cloud-native&#xA;ecosystem evolving in the future?&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Maciej&lt;/strong&gt;: I think that CLI tools were and will always be an&#xA;important piece of the ecosystem. Whether used by administrators on&#xA;remote machines that don’t have GUI or in every CI/CD pipeline, they&#xA;are irreplaceable.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Arpit&lt;/strong&gt;: Kubernetes is a community-driven project. Any&#xA;recommendation for anyone looking into getting involved in SIG CLI&#xA;work? Where should they start? Are there any prerequisites?&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Maciej&lt;/strong&gt;: There are no prerequisites other than a little bit of free&#xA;time on your hands and willingness to learn something new :-)&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Katrina&lt;/strong&gt;: A working knowledge of &lt;a href=&#34;https://go.dev/&#34;&gt;Go&lt;/a&gt; often helps,&#xA;but we also have areas in need of non-code contributions, such as the&#xA;&lt;a href=&#34;https://github.com/kubernetes-sigs/kustomize/issues/4338&#34;&gt;Kustomize docs consolidation&#xA;project&lt;/a&gt;.&lt;/p&gt;</description>
      <guid>https://kubernetes.io/blog/2023/07/20/sig-cli-spotlight-2023/</guid>
      <pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Spotlight on SIG ContribEx</title>
      <link>https://kubernetes.io/blog/2023/08/14/sig-contribex-spotlight-2023/</link>
      <description>&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Fyka Ansari&lt;/p&gt;&#xA;&lt;p&gt;Welcome to the world of Kubernetes and its vibrant contributor&#xA;community! In this blog post, we&#39;ll be shining a spotlight on the&#xA;&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-contributor-experience/README.md&#34;&gt;Special Interest Group for Contributor&#xA;Experience&lt;/a&gt;&#xA;(SIG ContribEx), an essential component of the Kubernetes project.&lt;/p&gt;&#xA;&lt;p&gt;SIG ContribEx in Kubernetes is responsible for developing and&#xA;maintaining a healthy and productive community of contributors to the&#xA;project. This involves identifying and addressing bottlenecks that may&#xA;hinder the project&#39;s growth and feature velocity, such as pull request&#xA;latency and the number of open pull requests and issues.&lt;/p&gt;&#xA;&lt;p&gt;SIG ContribEx works to improve the overall contributor experience by&#xA;creating and maintaining guidelines, tools, and processes that&#xA;facilitate collaboration and communication among contributors. They&#xA;also focus on community building and support, including outreach&#xA;programs and mentorship initiatives to onboard and retain new&#xA;contributors.&lt;/p&gt;&#xA;&lt;p&gt;Ultimately, the role of SIG ContribEx is to foster a welcoming and&#xA;inclusive environment that encourages contribution and supports the&#xA;long-term sustainability of the Kubernetes project.&lt;/p&gt;&#xA;&lt;p&gt;In this blog post, &lt;a href=&#34;https://twitter.com/1fyka&#34;&gt;Fyka Ansari&lt;/a&gt; interviews&#xA;&lt;a href=&#34;https://twitter.com/kaslinfields&#34;&gt;Kaslin Fields&lt;/a&gt;, a DevRel Engineer&#xA;at Google, who is a chair of SIG ContribEx, and &lt;a href=&#34;https://twitter.com/MadhavJivrajani&#34;&gt;Madhav&#xA;Jivrajani&lt;/a&gt;, a Software Engineer&#xA;at VMWare who serves as a SIG ContribEx Tech Lead. This interview&#xA;covers various aspects of SIG ContribEx, including current&#xA;initiatives, exciting developments, and how interested individuals can&#xA;get involved and contribute to the group. It provides valuable&#xA;insights into the workings of SIG ContribEx and highlights the&#xA;importance of its role in the Kubernetes ecosystem.&lt;/p&gt;&#xA;&lt;h3 id=&#34;introductions&#34;&gt;Introductions&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Fyka:&lt;/strong&gt; Let&#39;s start by diving into your background and how you got&#xA;involved in the Kubernetes ecosystem. Can you tell us more about that&#xA;journey?&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Kaslin:&lt;/strong&gt; I first got involved in the Kubernetes ecosystem through&#xA;my mentor, Jonathan Rippy, who introduced me to containers during my&#xA;early days in tech. Eventually, I transitioned to a team working with&#xA;containers, which sparked my interest in Kubernetes when it was&#xA;announced. While researching Kubernetes in that role, I eagerly sought&#xA;opportunities to engage with the containers/Kubernetes community. It&#xA;was not until my subsequent job that I found a suitable role to&#xA;contribute consistently. I joined SIG ContribEx, specifically in the&#xA;Contributor Comms subproject, to both deepen my knowledge of&#xA;Kubernetes and support the community better.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Madhav:&lt;/strong&gt; My journey with Kubernetes began when I was a student,&#xA;searching for interesting and exciting projects to work on. With my&#xA;peers, I discovered open source and attended The New Contributor&#xA;Workshop organized by the Kubernetes community. The workshop not only&#xA;provided valuable insights into the community structure but also gave&#xA;me a sense of warmth and welcome, which motivated me to join and&#xA;remain involved. I realized that collaboration is at the heart of&#xA;open-source communities, and to get answers and support, I needed to&#xA;contribute and do my part. I started working on issues in ContribEx,&#xA;particularly focusing on GitHub automation, despite not fully&#xA;understanding the task at first. I continued to contribute for various&#xA;technical and non-technical aspects of the project, finding it to be&#xA;one of the most professionally rewarding experiences in my life.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Fyka:&lt;/strong&gt; That&#39;s such an inspiration in itself! I&#39;m sure beginners who&#xA;are reading this got the ultimate motivation to take their first&#xA;steps. Embracing the Learning journey, seeking mentorship, and&#xA;engaging with the Kubernetes community can pave the way for exciting&#xA;opportunities in the tech industry. Your stories proved the importance&#xA;of starting small and being proactive, just like Madhav said Don&#39;t be&#xA;afraid to take on tasks, even if you&#39;re uncertain at first.&lt;/p&gt;&#xA;&lt;h3 id=&#34;primary-goals-and-scope&#34;&gt;Primary goals and scope&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Fyka:&lt;/strong&gt; Given your experience as a member of SIG ContribEx, could&#xA;you tell us a bit about the group&#39;s primary goals and initiatives? Its&#xA;current focus areas? What do you see as the scope of SIG ContribEx and&#xA;the impact it has on the Kubernetes community?&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Kaslin:&lt;/strong&gt; SIG ContribEx&#39;s primary goals are to simplify the&#xA;contributions of Kubernetes contributors and foster a welcoming&#xA;community. It collaborates with other Kubernetes SIGs, such as&#xA;planning the Contributor Summit at KubeCon, ensuring it meets the&#xA;needs of various groups. The group&#39;s impact is evident in projects&#xA;like updating org membership policies and managing critical platforms&#xA;like Zoom, YouTube, and Slack. Its scope encompasses making the&#xA;contributor experience smoother and supporting the overall Kubernetes&#xA;community.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Madhav:&lt;/strong&gt; The Kubernetes project has vertical SIGs and cross-cutting&#xA;SIGs, ContribEx is a deeply cross-cutting SIG, impacting virtually&#xA;every area of the Kubernetes community. Adding to Kaslin,&#xA;sustainability in the Kubernetes project and community is critical now&#xA;more than ever, it plays a central role in addressing critical issues,&#xA;such as maintainer succession, by facilitating cohorts for SIGs to&#xA;train experienced community members to take on leadership&#xA;roles. Excellent examples include SIG CLI and SIG Apps, leading to the&#xA;onboarding of new reviewers. Additionally, SIG ContribEx is essential&#xA;in managing GitHub automation tools, including bots and commands used&#xA;by contributors for interacting with &lt;a href=&#34;https://docs.prow.k8s.io/&#34;&gt;Prow&lt;/a&gt;&#xA;and other automation (label syncing, group and GitHub team management,&#xA;etc).&lt;/p&gt;&#xA;&lt;h3 id=&#34;beginner-s-guide&#34;&gt;Beginner&#39;s guide!&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Fyka:&lt;/strong&gt; I&#39;ll never forget talking to Kaslin when I joined the&#xA;community and needed help with contributing. Kaslin, your quick and&#xA;clear answers were a huge help in getting me started. Can you both&#xA;give some tips for people new to contributing to Kubernetes? What&#xA;makes SIG ContribEx a great starting point? Why should beginners and&#xA;current contributors consider it? And what cool opportunities are&#xA;there for newbies to jump in?&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Kaslin:&lt;/strong&gt; If you want to contribute to Kubernetes for the first&#xA;time, it can be overwhelming to know where to start. A good option is&#xA;to join SIG ContribEx as it offers great opportunities to know and&#xA;serve the community. Within SIG ContribEx, various subprojects allow&#xA;you to explore different parts of the Kubernetes project while you&#xA;learn how contributions work. Once you know a bit more, it’s common&#xA;for you to move to other SIGs within the project, and we think that’s&#xA;wonderful. While many newcomers look for &amp;quot;good first issues&amp;quot; to start&#xA;with, these opportunities can be scarce and get claimed&#xA;quickly. Instead, the real benefit lies in attending meetings and&#xA;getting to know the community. As you learn more about the project and&#xA;the people involved, you&#39;ll be better equipped to offer your help, and&#xA;the community will be more inclined to seek your assistance when&#xA;needed. As a co-lead for the Contributor Comms subproject, I can&#xA;confidently say that it&#39;s an excellent place for beginners to get&#xA;involved. We have supportive leads and particularly beginner-friendly&#xA;projects too.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Madhav:&lt;/strong&gt; To begin, read the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master#readme&#34;&gt;SIG&#xA;README&lt;/a&gt; on&#xA;GitHub, which provides an overview of the projects the SIG&#xA;manages. While attending meetings is beneficial for all SIGs, it&#39;s&#xA;especially recommended for SIG ContribEx, as each subproject gets&#xA;dedicated slots for updates and areas that need help. If you can&#39;t&#xA;attend in real-time due to time zone differences, you can catch the&#xA;meeting recordings or&#xA;&lt;a href=&#34;https://docs.google.com/document/d/1K3vjCZ9C3LwYrOJOhztQtFuDQCe-urv-ewx1bI8IPVQ/edit?usp=sharing&#34;&gt;Notes&lt;/a&gt;&#xA;later.&lt;/p&gt;&#xA;&lt;h3 id=&#34;skills-you-learn&#34;&gt;Skills you learn!&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Fyka:&lt;/strong&gt; What skills do you look for when bringing in new&#xA;contributors to SIG ContribEx, from passion to expertise?&#xA;Additionally, what skills can contributors expect to develop while&#xA;working with SIG ContribEx?&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Kaslin:&lt;/strong&gt; Skills folks need to have or will acquire vary depending&#xA;on what area of ContribEx they work upon. Even within a subproject, a&#xA;range of skills can be useful and/or developed. For example, the tech&#xA;lead role involves technical tasks and overseeing automation, while&#xA;the social media lead role requires excellent communication&#xA;skills. Working with SIG ContribEx allows contributors to acquire&#xA;various skills based on their chosen subproject. By participating in&#xA;meetings, listening, learning, and taking on tasks related to their&#xA;interests, they can develop and hone these skills. Some subprojects&#xA;may require more specialized skills, like program management for the&#xA;mentoring project, but all contributors can benefit from offering&#xA;their talents to help teach others and contribute to the community.&lt;/p&gt;&#xA;&lt;h3 id=&#34;sub-projects-under-sig-contribex&#34;&gt;Sub-projects under SIG ContribEx&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Fyka:&lt;/strong&gt; SIG ContribEx has several smaller projects. Can you tell me&#xA;about the aims of these projects and how they&#39;ve impacted the&#xA;Kubernetes community?&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Kaslin:&lt;/strong&gt; Some SIGs have one or two subprojects and some have none&#xA;at all, but in SIG ContribEx, we have ELEVEN!&lt;/p&gt;&#xA;&lt;p&gt;Here’s a list of them and their respective mission statements&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Community&lt;/strong&gt;: Manages the community repository, documentation,&#xA;and operations.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Community management&lt;/strong&gt;: Handles communication platforms and&#xA;policies for the community.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Contributor-comms&lt;/strong&gt;: Focuses on promoting the success of&#xA;Kubernetes contributors through marketing.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Contributors-documentation&lt;/strong&gt;: Writes and maintains documentation&#xA;for contributing to Kubernetes.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Devstats&lt;/strong&gt;: Maintains and updates the &lt;a href=&#34;https://k8s.devstats.cncf.io&#34;&gt;Kubernetes&#xA;statistics&lt;/a&gt; website.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Elections&lt;/strong&gt;: Oversees community elections and maintains related&#xA;documentation and software.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Events&lt;/strong&gt;: Organizes contributor-focused events like the&#xA;Contributor Summit.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Github management&lt;/strong&gt;: Manages permissions, repositories, and&#xA;groups on GitHub.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Mentoring&lt;/strong&gt;: Develop programs to help contributors progress in&#xA;their contributions.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Sigs-GitHub-actions&lt;/strong&gt;: Repository for GitHub actions related to&#xA;all SIGs in Kubernetes.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Slack-infra&lt;/strong&gt;: Creates and maintains tools and automation for&#xA;Kubernetes Slack.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;Madhav:&lt;/strong&gt; Also, Devstats is critical from a sustainability&#xA;standpoint!&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;(If you are willing to learn more and get involved with any of these&#xA;sub-projects, check out the&lt;/em&gt; &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-contributor-experience/README.md#subprojects&#34;&gt;SIG ContribEx&#xA;README&lt;/a&gt;)._&lt;/p&gt;&#xA;&lt;h3 id=&#34;accomplishments&#34;&gt;Accomplishments&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Fyka:&lt;/strong&gt; With that said, any SIG-related accomplishment that you’re&#xA;proud of?&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Kaslin:&lt;/strong&gt; I&#39;m proud of the accomplishments made by SIG ContribEx and&#xA;its contributors in supporting the community. Some of the recent&#xA;achievements include:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;em&gt;Establishment of the elections subproject&lt;/em&gt;: Kubernetes is a massive&#xA;project, and ensuring smooth leadership transitions is&#xA;crucial. The contributors in this subproject organize fair and&#xA;consistent elections, which helps keep the project running&#xA;effectively.&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;New issue triage proces&lt;/em&gt;: With such a large open-source project&#xA;like Kubernetes, there&#39;s always a lot of work to be done. To&#xA;ensure things progress safely, we implemented new labels and&#xA;updated functionality for issue triage using our PROW tool. This&#xA;reduces bottlenecks in the workflow and allows leaders to&#xA;accomplish more.&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;New org membership requirements&lt;/em&gt;: Becoming an org member in&#xA;Kubernetes can be overwhelming for newcomers. We view org&#xA;membership as a significant milestone for contributors aiming to&#xA;take on leadership roles. We recently updated the rules to&#xA;automatically remove privileges from inactive members, making sure&#xA;that the right people have access to the necessary tools and&#xA;responsibilities.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Overall, these accomplishments have greatly benefited our fellow&#xA;contributors and strengthened the Kubernetes community.&lt;/p&gt;&#xA;&lt;h3 id=&#34;upcoming-initiatives&#34;&gt;Upcoming initiatives&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Fyka:&lt;/strong&gt; Could you give us a sneak peek into what&#39;s next for the&#xA;group? We&#39;re excited to hear about upcoming projects and initiatives&#xA;from this dynamic team.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Madhav:&lt;/strong&gt; We’d love for more groups to sign up for mentoring&#xA;cohorts! We’re probably going to have to spend some time polishing the&#xA;process around that.&lt;/p&gt;&#xA;&lt;h3 id=&#34;final-thoughts&#34;&gt;Final thoughts&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Fyka:&lt;/strong&gt; As we wrap up our conversation, would you like to share some&#xA;final thoughts for those interested in contributing to SIG ContribEx&#xA;or getting involved with Kubernetes?&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Madhav&lt;/strong&gt;: Kubernetes is meant to be overwhelming and difficult&#xA;initially! You’re coming into something that’s taken multiple people,&#xA;from multiple countries, multiple years to build. Embrace that&#xA;diversity! Use the high entropy initially to collide around and gain&#xA;as much knowledge about the project and community as possible before&#xA;you decide to settle in your niche.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Fyka:&lt;/strong&gt; Thank You Madhav and Kaslin, it was an absolute pleasure&#xA;chatting about SIG ContribEx and your experiences as a member. It&#39;s&#xA;clear that the role of SIG ContribEx in Kubernetes is significant and&#xA;essential, ensuring scalability, growth and productivity, and I hope&#xA;this interview inspires more people to get involved and contribute to&#xA;Kubernetes. I wish SIG ContribEx all the best, and can&#39;t wait to see&#xA;what exciting things lie ahead!&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-next&#34;&gt;What next?&lt;/h2&gt;&#xA;&lt;p&gt;We love meeting new contributors and helping them in investigating&#xA;different Kubernetes project spaces. If you are interested in getting&#xA;more involved with SIG ContribEx, here are some resources for you to&#xA;get started:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-contributor-experience#contributor-experience-special-interest-group&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://groups.google.com/g/kubernetes-sig-contribex&#34;&gt;Mailing list&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/community/labels/sig%2Fcontributor-experience&#34;&gt;Open Community&#xA;Issues/PRs&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://slack.k8s.io/&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kubernetes.slack.com/messages/sig-contribex&#34;&gt;Slack channel&#xA;#sig-contribex&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;SIG Contribex also hosted a &lt;a href=&#34;https://youtu.be/5Bs1bs6iFmY&#34;&gt;KubeCon&#xA;talk&lt;/a&gt; about studying Kubernetes&#xA;Contributor experiences.&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
      <guid>https://kubernetes.io/blog/2023/08/14/sig-contribex-spotlight-2023/</guid>
      <pubDate>Mon, 14 Aug 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Kubernetes v1.28: Planternetes</title>
      <link>https://kubernetes.io/blog/2023/08/15/kubernetes-v1-28-release/</link>
      <description>&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.28/release-team.md&#34;&gt;Kubernetes v1.28 Release Team&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Announcing the release of Kubernetes v1.28 Planternetes, the second release of 2023!&lt;/p&gt;&#xA;&lt;p&gt;This release consists of 45 enhancements. Of those enhancements, 19 are entering Alpha, 14 have graduated to Beta, and 12 have graduated to Stable.&lt;/p&gt;&#xA;&lt;h2 id=&#34;release-theme-and-logo&#34;&gt;Release Theme And Logo&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Kubernetes v1.28: &lt;em&gt;Planternetes&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;The theme for Kubernetes v1.28 is &lt;em&gt;Planternetes&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;figure class=&#34;release-logo&#34;&gt;&#xA;&lt;img src=&#34;https://kubernetes.io/images/blog/2023-08-15-kubernetes-1.28-blog/kubernetes-1.28.png&#34;&#xA;alt=&#34;Kubernetes 1.28 Planternetes logo&#34;/&gt;&#xA;&lt;/figure&gt;&#xA;&lt;p&gt;Each Kubernetes release is the culmination of the hard work of thousands of individuals from our community. The people behind this release come from a wide range of backgrounds, some of us industry veterans, parents, others students and newcomers to open-source. We combine our unique experience to create a collective artifact with global impact.&lt;/p&gt;&#xA;&lt;p&gt;Much like a garden, our release has ever-changing growth, challenges and opportunities. This theme celebrates the meticulous care, intention and efforts to get the release to where we are today. Harmoniously together, we grow better.&lt;/p&gt;&#xA;&lt;h1 id=&#34;what-s-new-major-themes&#34;&gt;What&#39;s New (Major Themes)&lt;/h1&gt;&#xA;&lt;h2 id=&#34;changes-to-supported-skew-between-control-plane-and-node-versions&#34;&gt;Changes to supported skew between control plane and node versions&lt;/h2&gt;&#xA;&lt;p&gt;Kubernetes v1.28 expands the supported skew between core node and control plane&#xA;components by one minor version, from &lt;em&gt;n-2&lt;/em&gt; to &lt;em&gt;n-3&lt;/em&gt;, so that node components&#xA;(kubelet and kube-proxy) for the oldest supported minor version work with&#xA;control plane components (kube-apiserver, kube-scheduler, kube-controller-manager,&#xA;cloud-controller-manager) for the newest supported minor version.&lt;/p&gt;&#xA;&lt;p&gt;Some cluster operators avoid node maintenance and especially changes to node&#xA;behavior, because nodes are where the workloads run. For minor version upgrades&#xA;to a kubelet, the supported process includes draining that node, and hence&#xA;disruption to any Pods that had been executing there. For Kubernetes end users&#xA;with very long running workloads, and where Pods should stay running wherever&#xA;possible, reducing the time lost to node maintenance is a benefit.&lt;/p&gt;&#xA;&lt;p&gt;The Kubernetes yearly support period already made annual upgrades possible. Users can&#xA;upgrade to the latest patch versions to pick up security fixes and do 3 sequential&#xA;minor version upgrades once a year to &amp;quot;catch up&amp;quot; to the latest supported minor version.&lt;/p&gt;&#xA;&lt;p&gt;Previously, to stay within the supported skew, a cluster operator planning an annual&#xA;upgrade would have needed to upgrade their nodes twice (perhaps only hours apart). Now,&#xA;with Kubernetes v1.28, you have the option of making a minor version upgrade to&#xA;nodes just once in each calendar year and still staying within upstream support.&lt;/p&gt;&#xA;&lt;p&gt;If you&#39;d like to stay current and upgrade your clusters more often, that&#39;s&#xA;fine and is still completely supported.&lt;/p&gt;&#xA;&lt;h2 id=&#34;generally-available-recovery-from-non-graceful-node-shutdown&#34;&gt;Generally available: recovery from non-graceful node shutdown&lt;/h2&gt;&#xA;&lt;p&gt;If a node shuts down unexpectedly or ends up in a non-recoverable state (perhaps due to hardware failure or unresponsive OS), Kubernetes allows you to clean up afterward and allow stateful workloads to restart on a different node. For Kubernetes v1.28, that&#39;s now a stable feature.&lt;/p&gt;&#xA;&lt;p&gt;This allows stateful workloads to fail over to a different node successfully after the original node is shut down or in a non-recoverable state, such as the hardware failure or broken OS.&lt;/p&gt;&#xA;&lt;p&gt;Versions of Kubernetes earlier than v1.20 lacked handling for node shutdown on Linux, the kubelet integrates with systemd&#xA;and implements graceful node shutdown (beta, and enabled by default). However, even an intentional&#xA;shutdown might not get handled well that could be because:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the node runs Windows&lt;/li&gt;&#xA;&lt;li&gt;the node runs Linux, but uses a different &lt;code&gt;init&lt;/code&gt; (not &lt;code&gt;systemd&lt;/code&gt;)&lt;/li&gt;&#xA;&lt;li&gt;the shutdown does not trigger the system inhibitor locks mechanism&lt;/li&gt;&#xA;&lt;li&gt;because of a node-level configuration error&#xA;(such as not setting appropriate values for &lt;code&gt;shutdownGracePeriod&lt;/code&gt; and &lt;code&gt;shutdownGracePeriodCriticalPods&lt;/code&gt;).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;When a node shutdowns or fails, and that shutdown was not detected by the kubelet, the pods that are part&#xA;of a StatefulSet will be stuck in terminating status on the shutdown node. If the stopped node restarts, the&#xA;kubelet on that node can clean up (&lt;code&gt;DELETE&lt;/code&gt;) the Pods that the Kubernetes API still sees as bound to that node.&#xA;However, if the node stays stopped - or if the kubelet isn&#39;t able to start after a reboot - then Kubernetes may&#xA;not be able to create replacement Pods. When the kubelet on the shut-down node is not available to delete&#xA;the old pods, an associated StatefulSet cannot create a new pod (which would have the same name).&lt;/p&gt;&#xA;&lt;p&gt;There&#39;s also a problem with storage. If there are volumes used by the pods, existing VolumeAttachments will&#xA;not be disassociated from the original - and now shut down - node so the PersistentVolumes used by these&#xA;pods cannot be attached to a different, healthy node. As a result, an application running on an&#xA;affected StatefulSet may not be able to function properly. If the original, shut down node does come up, then&#xA;their pods will be deleted by its kubelet and new pods can be created on a different running node.&#xA;If the original node does not come up (common with an &lt;a href=&#34;https://glossary.cncf.io/immutable-infrastructure/&#34;&gt;immutable infrastructure&lt;/a&gt; design), those pods would be stuck in a &lt;code&gt;Terminating&lt;/code&gt; status on the shut-down node forever.&lt;/p&gt;&#xA;&lt;p&gt;For more information on how to trigger cleanup after a non-graceful node shutdown,&#xA;read &lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/nodes/#non-graceful-node-shutdown&#34;&gt;non-graceful node shutdown&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;improvements-to-customresourcedefinition-validation-rules&#34;&gt;Improvements to CustomResourceDefinition validation rules&lt;/h2&gt;&#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/google/cel-go&#34;&gt;Common Expression Language (CEL)&lt;/a&gt; can be used to validate&#xA;&lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&#34;&gt;custom resources&lt;/a&gt;. The primary goal is to allow the majority of the validation use cases that might once have needed you, as a CustomResourceDefinition (CRD) author, to design and implement a webhook. Instead, and as a beta feature, you can add &lt;em&gt;validation expressions&lt;/em&gt; directly into the schema of a CRD.&lt;/p&gt;&#xA;&lt;p&gt;CRDs need direct support for non-trivial validation. While admission webhooks do support CRDs validation, they significantly complicate the development and operability of CRDs.&lt;/p&gt;&#xA;&lt;p&gt;In 1.28, two optional fields &lt;code&gt;reason&lt;/code&gt; and &lt;code&gt;fieldPath&lt;/code&gt; were added to allow user to specify the failure reason and fieldPath when validation failed.&lt;/p&gt;&#xA;&lt;p&gt;For more information, read &lt;a href=&#34;https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules&#34;&gt;validation rules&lt;/a&gt; in the CRD documentation.&lt;/p&gt;&#xA;&lt;h2 id=&#34;validatingadmissionpolicies-graduate-to-beta&#34;&gt;ValidatingAdmissionPolicies graduate to beta&lt;/h2&gt;&#xA;&lt;p&gt;Common Expression language for admission control is customizable, in-process validation of requests to the Kubernetes API server as an alternative to validating admission webhooks.&lt;/p&gt;&#xA;&lt;p&gt;This builds on the capabilities of the CRD Validation Rules feature that graduated to beta in 1.25 but with a focus on the policy enforcement capabilities of validating admission control.&lt;/p&gt;&#xA;&lt;p&gt;This will lower the infrastructure barrier to enforcing customizable policies as well as providing primitives that help the community establish and adhere to the best practices of both K8s and its extensions.&lt;/p&gt;&#xA;&lt;p&gt;To use &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/validating-admission-policy/&#34;&gt;ValidatingAdmissionPolicies&lt;/a&gt;, you need to enable both the &lt;code&gt;admissionregistration.k8s.io/v1beta1&lt;/code&gt; API group and the &lt;code&gt;ValidatingAdmissionPolicy&lt;/code&gt; feature gate in your cluster&#39;s control plane.&lt;/p&gt;&#xA;&lt;h2 id=&#34;match-conditions-for-admission-webhooks&#34;&gt;Match conditions for admission webhooks&lt;/h2&gt;&#xA;&lt;p&gt;Kubernetes v1.27 lets you specify &lt;em&gt;match conditions&lt;/em&gt; for admission webhooks,&#xA;which lets you narrow the scope of when Kubernetes makes a remote HTTP call at admission time.&#xA;The &lt;code&gt;matchCondition&lt;/code&gt; field for ValidatingWebhookConfiguration and MutatingWebhookConfiguration&#xA;is a CEL expression that must evaluate to true for the admission request to be sent to the webhook.&lt;/p&gt;&#xA;&lt;p&gt;In Kubernetes v1.28, that field moved to beta, and it&#39;s enabled by default.&lt;/p&gt;&#xA;&lt;p&gt;To learn more, see &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-matchconditions&#34;&gt;&lt;code&gt;matchConditions&lt;/code&gt;&lt;/a&gt; in the Kubernetes documentation.&lt;/p&gt;&#xA;&lt;h2 id=&#34;beta-support-for-enabling-swap-space-on-linux&#34;&gt;Beta support for enabling swap space on Linux&lt;/h2&gt;&#xA;&lt;p&gt;This adds swap support to nodes in a controlled, predictable manner so that Kubernetes users can perform testing and provide data to continue building cluster capabilities on top of swap.&lt;/p&gt;&#xA;&lt;p&gt;There are two distinct types of users for swap, who may overlap:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Node administrators, who may want swap available for node-level performance tuning and stability/reducing noisy neighbor issues.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Application developers, who have written applications that would benefit from using swap memory.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;mixed-version-proxy&#34;&gt;Mixed version proxy (alpha)&lt;/h2&gt;&#xA;&lt;p&gt;When a cluster has multiple API servers at mixed versions (such as during an upgrade/downgrade or when runtime-config changes and a rollout happens), not every apiserver can serve every resource at every version.&lt;/p&gt;&#xA;&lt;p&gt;For Kubernetes v1.28, you can enable the &lt;em&gt;mixed version proxy&lt;/em&gt; within the API server&#39;s aggregation layer.&#xA;The mixed version proxy finds requests that the local API server doesn&#39;t recognize but another API server&#xA;inside the control plan is able to support. Having found a suitable peer, the aggregation layer proxies&#xA;the request to a compatible API server; this is transparent from the client&#39;s perspective.&lt;/p&gt;&#xA;&lt;p&gt;When an upgrade or downgrade is performed on a cluster, for some period of time the API servers&#xA;within the control plane may be at differing versions; when that happens, different subsets of the&#xA;API servers are able to serve different sets of built-in resources (different groups, versions, and resources&#xA;are all possible). This new alpha mechanism lets you hide that skew from clients.&lt;/p&gt;&#xA;&lt;h2 id=&#34;source-code-reorganization-for-control-plane-components&#34;&gt;Source code reorganization for control plane components&lt;/h2&gt;&#xA;&lt;p&gt;Kubernetes contributors have begun to reorganize the code for the kube-apiserver to build on a new staging repository that consumes &lt;a href=&#34;https://github.com/kubernetes/apiserver&#34;&gt;k/apiserver&lt;/a&gt; but has a bigger, carefully chosen subset of the functionality of kube-apiserver such that it is reusable.&lt;/p&gt;&#xA;&lt;p&gt;This is a gradual reorganization; eventually there will be a new git repository with generic functionality abstracted from Kubernetes&#39; API server.&lt;/p&gt;&#xA;&lt;h2 id=&#34;cdi-device-plugin&#34;&gt;Support for CDI injection into containers (alpha)&lt;/h2&gt;&#xA;&lt;p&gt;CDI provides a standardized way of injecting complex devices into a container (i.e. devices that logically require more than just a single /dev node to be injected for them to work). This new feature enables plugin developers to utilize the CDIDevices field added to the CRI in 1.27 to pass CDI devices directly to CDI enabled runtimes (of which containerd and crio-o are in recent releases).&lt;/p&gt;&#xA;&lt;h2 id=&#34;sidecar-init-containers&#34;&gt;API awareness of sidecar containers (alpha)&lt;/h2&gt;&#xA;&lt;p&gt;Kubernetes 1.28 introduces an alpha &lt;code&gt;restartPolicy&lt;/code&gt; field for &lt;a href=&#34;https://github.com/kubernetes/website/blob/main/content/en/docs/concepts/workloads/pods/init-containers.md&#34;&gt;init containers&lt;/a&gt;,&#xA;and uses that to indicate when an init container is also a &lt;em&gt;sidecar container&lt;/em&gt;.&#xA;The kubelet will start init containers with &lt;code&gt;restartPolicy: Always&lt;/code&gt; in the order&#xA;they are defined, along with other init containers.&#xA;Instead of waiting for that sidecar container to complete before starting the main&#xA;container(s) for the Pod, the kubelet only waits for the sidecar init container to have started.&lt;/p&gt;&#xA;&lt;p&gt;The kubelet will consider the startup for the sidecar container as being completed&#xA;if the startup probe succeeds and the postStart handler is completed.&#xA;This condition is represented with the field Started of ContainerStatus type.&#xA;If you do not define a startup probe, the kubelet will consider the container&#xA;startup to be completed immediately after the postStart handler completion.&lt;/p&gt;&#xA;&lt;p&gt;For init containers, you can either omit the &lt;code&gt;restartPolicy&lt;/code&gt; field, or set it to &lt;code&gt;Always&lt;/code&gt;. Omitting the field&#xA;means that you want a true init container that runs to completion before application startup.&lt;/p&gt;&#xA;&lt;p&gt;Sidecar containers do not block Pod completion: if all regular containers are complete, sidecar&#xA;containers in that Pod will be terminated.&lt;/p&gt;&#xA;&lt;p&gt;Once the sidecar container has started (process running, &lt;code&gt;postStart&lt;/code&gt; was successful, and&#xA;any configured startup probe is passing), and then there&#39;s a failure, that sidecar container will be&#xA;restarted even when the Pod&#39;s overall &lt;code&gt;restartPolicy&lt;/code&gt; is &lt;code&gt;Never&lt;/code&gt; or &lt;code&gt;OnFailure&lt;/code&gt;.&#xA;Furthermore, sidecar containers will be restarted (on failure or on normal exit)&#xA;&lt;em&gt;even during Pod termination&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;p&gt;To learn more, read &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#api-for-sidecar-containers&#34;&gt;API for sidecar containers&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;automatic-retroactive-assignment-of-a-default-storageclass-graduates-to-stable&#34;&gt;Automatic, retroactive assignment of a default StorageClass graduates to stable&lt;/h2&gt;&#xA;&lt;p&gt;Kubernetes automatically sets a &lt;code&gt;storageClassName&lt;/code&gt; for a PersistentVolumeClaim (PVC) if you don&#39;t provide&#xA;a value. The control plane also sets a StorageClass for any existing PVC that doesn&#39;t have a &lt;code&gt;storageClassName&lt;/code&gt;&#xA;defined.&#xA;Previous versions of Kubernetes also had this behavior; for Kubernetes v1.28 it is automatic and always&#xA;active; the feature has graduated to stable (general availability).&lt;/p&gt;&#xA;&lt;p&gt;To learn more, read about &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/storage-classes/&#34;&gt;StorageClass&lt;/a&gt; in the Kubernetes&#xA;documentation.&lt;/p&gt;&#xA;&lt;h2 id=&#34;pod-replacement-policy&#34;&gt;Pod replacement policy for Jobs (alpha)&lt;/h2&gt;&#xA;&lt;p&gt;Kubernetes 1.28 adds a new field for the Job API that allows you to specify if you want the control&#xA;plane to make new Pods as soon as the previous Pods begin termination (existing behavior),&#xA;or only once the existing pods are fully terminated (new, optional behavior).&lt;/p&gt;&#xA;&lt;p&gt;Many common machine learning frameworks, such as Tensorflow and JAX, require unique pods per index.&#xA;With the older behaviour, if a pod that belongs to an &lt;code&gt;Indexed&lt;/code&gt; Job enters a terminating state (due to preemption, eviction or other external factors), a replacement pod is created but then immediately fails to start due&#xA;to the clash with the old pod that has not yet shut down.&lt;/p&gt;&#xA;&lt;p&gt;Having a replacement Pod appear before the previous one fully terminates can also cause problems&#xA;in clusters with scarce resources or with tight budgets. These resources can be difficult to obtain so pods may only be able to find nodes once the existing pods have been terminated. If cluster autoscaler is enabled, early creation of replacement Pods might produce undesired scale-ups.&lt;/p&gt;&#xA;&lt;p&gt;To learn more, read &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/job/#delayed-creation-of-replacement-pods&#34;&gt;Delayed creation of replacement pods&lt;/a&gt;&#xA;in the Job documentation.&lt;/p&gt;&#xA;&lt;h2 id=&#34;job-per-index-retry-backoff&#34;&gt;Job retry backoff limit, per index (alpha)&lt;/h2&gt;&#xA;&lt;p&gt;This extends the Job API to support indexed jobs where the backoff limit is per index, and the Job can continue execution despite some of its indexes failing.&lt;/p&gt;&#xA;&lt;p&gt;Currently, the indexes of an indexed job share a single backoff limit. When the job reaches this shared backoff limit, the job controller marks the entire job as failed, and the resources are cleaned up, including indexes that have yet to run to completion.&lt;/p&gt;&#xA;&lt;p&gt;As a result, the existing implementation did not cover the situation where the workload is truly&#xA;&lt;a href=&#34;https://en.wikipedia.org/wiki/Embarrassingly_parallel&#34;&gt;embarrassingly parallel&lt;/a&gt;: each index is&#xA;fully independent of other indexes.&lt;/p&gt;&#xA;&lt;p&gt;For instance, if indexed jobs were used as the basis for a suite of long-running integration tests, then each test run would only be able to find a single test failure.&lt;/p&gt;&#xA;&lt;p&gt;For more information, read &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/job/#handling-pod-and-container-failures&#34;&gt;Handling Pod and container failures&lt;/a&gt; in the Kubernetes documentation.&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;a id=&#34;cri-container-and-pod-statistics-without-cadvisor&#34; /&gt;&#xA;&lt;p&gt;&lt;strong&gt;Correction&lt;/strong&gt;: the feature CRI container and pod statistics without cAdvisor has been removed as it did not make the release.&#xA;The original release announcement stated that Kubernetes 1.28 included the new feature.&lt;/p&gt;&#xA;&lt;h2 id=&#34;feature-graduations-and-deprecations-in-kubernetes-v1-28&#34;&gt;Feature graduations and deprecations in Kubernetes v1.28&lt;/h2&gt;&#xA;&lt;h3 id=&#34;graduations-to-stable&#34;&gt;Graduations to stable&lt;/h3&gt;&#xA;&lt;p&gt;This release includes a total of 12 enhancements promoted to Stable:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1440&#34;&gt;&lt;code&gt;kubectl events&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/3333&#34;&gt;Retroactive default StorageClass assignment&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/2268&#34;&gt;Non-graceful node shutdown&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/606&#34;&gt;Support 3rd party device monitoring plugins&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/3325&#34;&gt;Auth API to get self-user attributes&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1669&#34;&gt;Proxy Terminating Endpoints&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/2595&#34;&gt;Expanded DNS Configuration&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/3178&#34;&gt;Cleaning up IPTables Chain Ownership&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/3453&#34;&gt;Minimizing iptables-restore input size&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/3743&#34;&gt;Graduate the kubelet pod resources endpoint to GA&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/2403&#34;&gt;Extend podresources API to report allocatable resources&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/3685&#34;&gt;Move EndpointSlice Reconciler into Staging&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;deprecations-and-removals&#34;&gt;Deprecations and removals&lt;/h3&gt;&#xA;&lt;p&gt;Removals:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1488&#34;&gt;Removal of CSI Migration for GCE PD&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Deprecations:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/118303&#34;&gt;Ceph RBD in-tree plugin&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/118143&#34;&gt;Ceph FS in-tree plugin&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;release-notes&#34;&gt;Release Notes&lt;/h2&gt;&#xA;&lt;p&gt;The complete details of the Kubernetes v1.28 release are available in our &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.28.md&#34;&gt;release notes&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;availability&#34;&gt;Availability&lt;/h2&gt;&#xA;&lt;p&gt;Kubernetes v1.28 is available for download on &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.28.0&#34;&gt;GitHub&lt;/a&gt;. To get started with Kubernetes, you can run local Kubernetes clusters using &lt;a href=&#34;https://minikube.sigs.k8s.io/docs/&#34;&gt;minikube&lt;/a&gt;, &lt;a href=&#34;https://kind.sigs.k8s.io/&#34;&gt;kind&lt;/a&gt;, etc. You can also easily install v1.28 using &lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/&#34;&gt;kubeadm&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;release-team&#34;&gt;Release Team&lt;/h2&gt;&#xA;&lt;p&gt;Kubernetes is only possible with the support, commitment, and hard work of its community. Each release team is comprised of dedicated community volunteers who work together to build the many pieces that make up the Kubernetes releases you rely on. This requires the specialized skills of people from all corners of our community, from the code itself to its documentation and project management.&lt;/p&gt;&#xA;&lt;p&gt;We would like to thank the entire release team for the hours spent hard at work to ensure we deliver a solid Kubernetes v1.28 release for our community.&lt;/p&gt;&#xA;&lt;p&gt;Special thanks to our release lead, Grace Nguyen, for guiding us through a smooth and successful release cycle.&lt;/p&gt;&#xA;&lt;h2 id=&#34;ecosystem-updates&#34;&gt;Ecosystem Updates&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;KubeCon + CloudNativeCon China 2023 will take place in Shanghai, China, from 26 – 28 September 2023! You can find more information about the conference and registration on the &lt;a href=&#34;https://www.lfasiallc.com/kubecon-cloudnativecon-open-source-summit-china/&#34;&gt;event site&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;KubeCon + CloudNativeCon North America 2023 will take place in Chicago, Illinois, The United States of America, from 6 – 9 November 2023! You can find more information about the conference and registration on the &lt;a href=&#34;https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/&#34;&gt;event site&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;project-velocity&#34;&gt;Project Velocity&lt;/h2&gt;&#xA;&lt;p&gt;The &lt;a href=&#34;https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1&amp;amp;refresh=15m&#34;&gt;CNCF K8s DevStats&lt;/a&gt; project aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.&lt;/p&gt;&#xA;&lt;p&gt;In the v1.28 release cycle, which &lt;a href=&#34;https://github.com/kubernetes/sig-release/tree/master/releases/release-1.28&#34;&gt;ran for 14 weeks&lt;/a&gt; (May 15 to August 15), we saw contributions from &lt;a href=&#34;https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.27.0%20-%20now&amp;amp;var-metric=contributions&#34;&gt;911 companies&lt;/a&gt; and &lt;a href=&#34;https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.27.0%20-%20now&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-repo_name=kubernetes%2Fkubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All&#34;&gt;1440 individuals&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;upcoming-release-webinar&#34;&gt;Upcoming Release Webinar&lt;/h2&gt;&#xA;&lt;p&gt;Join members of the Kubernetes v1.28 release team on Wednesday, September 6th, 2023, at 9 A.M. PDT to learn about the major features of this release, as well as deprecations and removals to help plan for upgrades. For more information and registration, visit the &lt;a href=&#34;https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-128-release/&#34;&gt;event page&lt;/a&gt; on the CNCF Online Programs site.&lt;/p&gt;&#xA;&lt;h2 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h2&gt;&#xA;&lt;p&gt;The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34;&gt;Special Interest Groups&lt;/a&gt; (SIGs) that align with your interests.&lt;/p&gt;&#xA;&lt;p&gt;Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/communication&#34;&gt;community meeting&lt;/a&gt;, and through the channels below:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Find out more about contributing to Kubernetes at the &lt;a href=&#34;https://www.kubernetes.dev/&#34;&gt;Kubernetes Contributors website&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34;&gt;@Kubernetesio&lt;/a&gt; for the latest updates.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Join the community discussion on &lt;a href=&#34;https://discuss.kubernetes.io/&#34;&gt;Discuss&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Join the community on &lt;a href=&#34;https://communityinviter.com/apps/kubernetes/community&#34;&gt;Slack&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Post questions (or answer questions) on &lt;a href=&#34;https://serverfault.com/questions/tagged/kubernetes&#34;&gt;Server Fault&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34;&gt;Share&lt;/a&gt; your Kubernetes story.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Read more about what’s happening with Kubernetes on the &lt;a href=&#34;https://kubernetes.io/blog/&#34;&gt;blog&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Learn more about the &lt;a href=&#34;https://github.com/kubernetes/sig-release/tree/master/release-team&#34;&gt;Kubernetes Release Team&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
      <guid>https://kubernetes.io/blog/2023/08/15/kubernetes-v1-28-release/</guid>
      <pubDate>Tue, 15 Aug 2023 12:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>